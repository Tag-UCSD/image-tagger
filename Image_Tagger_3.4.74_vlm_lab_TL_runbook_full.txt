#!/usr/bin/env python
"""Deconcatenate this file into the original Image Tagger repository.

Usage:
    python this_file.py [output_dir]

If output_dir is omitted, it will create ./reconstructed_repo
in the current working directory.
"""
import os
import sys

MARK_PATH = "----- FILE PATH: "
MARK_START = "----- CONTENT START -----"
MARK_END = "----- CONTENT END -----"

def main():
    src_path = os.path.abspath(sys.argv[0])
    out_dir = os.path.abspath(sys.argv[1]) if len(sys.argv) > 1 else os.path.abspath("reconstructed_repo")
    os.makedirs(out_dir, exist_ok=True)

    with open(src_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    i = 0
    n = len(lines)
    while i < n:
        line = lines[i]
        if line.startswith(MARK_PATH):
            rel_path = line[len(MARK_PATH):].rstrip("\n")
            i += 1
            while i < n and lines[i].strip() == "":
                i += 1
            if i >= n or not lines[i].strip().startswith(MARK_START):
                raise RuntimeError(f"Malformed block for {rel_path}: missing CONTENT START")
            i += 1
            content_lines = []
            while i < n and not lines[i].strip().startswith(MARK_END):
                content_lines.append(lines[i])
                i += 1
            if i >= n:
                raise RuntimeError(f"Malformed block for {rel_path}: missing CONTENT END")
            out_path = os.path.join(out_dir, rel_path)
            os.makedirs(os.path.dirname(out_path), exist_ok=True)
            with open(out_path, "w", encoding="utf-8") as outf:
                outf.writelines(content_lines)
        else:
            i += 1

if __name__ == "__main__":
    main()

# ----- REPO CONTENT BELOW -----
----- FILE PATH: CANONICAL_ANCHOR.json
----- CONTENT START -----
{
  "base_artifact": "Image_Tagger_v3.4.17_stub_safe_vlm_seed_full.zip",
  "base_sha256": "096f717914206160de9b9510c6a668bf63122b41c6a580974750aac4bb0fcedc",
  "base_version": "3.4.17_stub_safe_vlm_seed",
  "notes": "Canonical anchor for 3.4.18_vlm_patterns_materials_seed_full; built monotonically from 3.4.17_stub_safe_vlm_seed."
}----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.23_tag_inspector.md
----- CONTENT START -----
# Image Tagger v3.4.23 ‚Äî Tag Inspector wiring

This version builds on v3.4.22_canon_guard_fixed and adds:

- A new supervisor endpoint: GET /v1/monitor/image/{image_id}/inspector
  which aggregates:
    * basic image metadata + /static URL,
    * science_pipeline_* Validation rows,
    * composite indices + bins from the BN export helpers,
    * a BN-style node summary for each candidate index,
    * per-user validations for the image.
- An upgraded Tag Inspector drawer in the Supervisor UI that:
    * fetches the inspector payload,
    * shows an image preview and science snapshot,
    * lists composite indices (BN inputs) with bins/values,
    * lists raw science attributes,
    * lists human validations with dwell time and timestamps.

All existing files from v3.4.22 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.24_tag_inspector_help.md
----- CONTENT START -----
# Image Tagger v3.4.24 ‚Äî Tag Inspector inline help

This version builds on v3.4.23_tag_inspector_full and adds:

- Inline help to the Tag Inspector drawer in the Supervisor GUI:
  * A HelpCircle icon in the drawer header toggles a help panel.
  * The help panel explains what Tag Inspector shows:
    - science attributes,
    - high-level indices (BN inputs),
    - human validations,
    - how to interpret the snapshot counts and IRR.
- No backend changes; this is a pure UI help enhancement on top of the existing inspector endpoint.

All existing files from v3.4.23 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.25_role_help_strips.md
----- CONTENT START -----
# Image Tagger v3.4.25 ‚Äî Role help strips

This version builds on v3.4.24_tag_inspector_help and focuses on inline, role-specific help:

- Workbench (Tagger Station):
  * Adds a small help strip directly under the existing quick keyboard help.
  * Explains the role of the Workbench (fast YES/NO tagging for images) and how decisions are logged as validations.
- Explorer (Research Discovery):
  * Already included a rich "How to use Explorer" strip; no code changes in this version, but Explorer is now part of the documented role-help pattern.

All existing files from v3.4.24 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.27_priority_help.md
----- CONTENT START -----
# Image Tagger v3.4.27 ‚Äî Priority GUI help (Monitor + Admin)

This version builds on v3.4.26_governance_todo and implements the highest-priority role-based help strips:

- Supervisor / Monitor app:
  * Added an inline "How to use the Supervisor dashboard" help box beneath the controls row,
    explaining how supervisors should interpret throughput, IRR, and error metrics,
    and when to drill into Tag Inspector vs escalate to engineering.

- Admin app:
  * VLMConfigPanel: Added a "VLM configuration: handle with care" help strip under the VLM Engine header,
    clarifying that provider/API settings are global and should be smoke-tested with a known image before use.
  * Training Export card: Added a "What this export is for" help strip explaining how the JSON export maps
    to image/case rows, how it is used for BN/regression/ML training, and why canon/schema versioning matters.

- Governance:
  * Updated `governance/TODO_running_list.md` with a status section for v3.4.27,
    marking which GUI help tasks are now implemented and which remain pending (schema/index catalog UI, pipeline health panel).

All existing files from v3.4.26 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.28_pipeline_health_view.md
----- CONTENT START -----
# Image Tagger v3.4.28 ‚Äî Science pipeline health view

This version builds on v3.4.27_priority_help and adds a dedicated pipeline health view for supervisors:

- Monitor app:
  * Added a "Science pipeline health" section beneath the top metrics grid.
  * The panel calls `/api/v1/debug/pipeline_health` (via a `debugApi` client) during the main `loadData` call
    and on demand via a "Re-check" button.
  * It displays:
    - Import status (OK/FAILED).
    - OpenCV availability.
    - Analyzers by tier with their `requires`/`provides` contracts.
    - Any warnings or analyzer instantiation errors returned by the endpoint.
  * Includes inline explanatory text clarifying that this is a contracts/instantiation check, not a full
    image-processing run, and that persistent failures should be escalated to engineering.

- Governance:
  * Updated `governance/TODO_running_list.md` with a v3.4.28 status entry marking the pipeline health
    view as implemented.

All existing files from v3.4.27 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.30_pipeline_health_tests.md
----- CONTENT START -----
# Image Tagger v3.4.30 ‚Äî Pipeline health endpoint tests

This version builds on v3.4.29_tag_inspector_tests and adds a smoketest for the
`/api/v1/debug/pipeline_health` endpoint.

- Tests:
  * New `tests/test_pipeline_health_debug.py`:
    - Uses FastAPI's `TestClient` to hit `/api/v1/debug/pipeline_health`.
    - Asserts HTTP 200.
    - Validates that the response is a dict with:
      - `import_ok` (bool),
      - `cv2_available` (bool),
      - `analyzers_by_tier` (dict),
      - optional `warnings` and `analyzer_errors` (lists, if present).
    - Performs a light shape check over `analyzers_by_tier`, ensuring each analyzer entry
      includes `name`, `tier`, `requires`, and `provides`.

- Governance:
  * `governance/TODO_running_list.md` updated with a v3.4.30 status note marking
    TODO item 4.3 (pipeline health endpoint tests) as implemented.

All existing files from v3.4.29 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.31_bn_canon_sanity.md
----- CONTENT START -----
# Image Tagger v3.4.31 ‚Äî BN canon sanity tests

This version builds on v3.4.30_pipeline_health_tests and adds higher-level
"canon sanity" tests for the BN export and index catalog.

- Tests:
  * New `tests/test_bn_canon_sanity.py`:
    - `test_index_catalog_candidate_entries_are_well_formed` verifies that
      every candidate BN index in `backend/science/index_catalog.py` has:
        - a non-empty `label` and `description`,
        - a valid `type` in {"float", "int", "str"},
        - a `bins` spec with a non-empty `field` and list of `values`,
          and, when 3 values are present, that they are exactly {low, mid, high}.
    - `test_bn_export_respects_index_catalog_canon` seeds synthetic Validation
      rows for all candidate indices and their bin fields, calls
      `export_bn_snapshot`, and asserts:
        - `BNRow.indices.keys()` equals the candidate index key set and all
          values are non-None.
        - All expected bin fields appear in `BNRow.bins`, with labels from
          {low, mid, high} where configured.

- Governance:
  * `governance/TODO_running_list.md` updated with a v3.4.31 status entry
    describing the new BN canon sanity tests as part of the science/BN
    hardening track.

All existing files from v3.4.30 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.32_restorativeness_H1.md
----- CONTENT START -----
# Image Tagger v3.4.32 ‚Äî Restorativeness H1 heuristic in Tag Inspector

This version builds on v3.4.31_bn_canon_sanity and introduces the first explicit
"restorativeness" rule family (H1) in the Tag Inspector.

- Tag Inspector:
  * Added `_build_restorativeness_heuristic_node` to `backend/api/v1_supervision.py`.
  * The helper:
    - reads CNfA fluency + biophilia features from the `features` list returned by
      `/v1/monitor/image/{image_id}/inspector` (if present), focusing on:
        - `cnfa.biophilic.natural_material_ratio`
        - `cnfa.fluency.visual_entropy_spatial`
        - `cnfa.fluency.clutter_density_count`
        - `cnfa.fluency.processing_load_proxy`
    - computes a simple, explicitly heuristic restorativeness score in [0, 1], combining:
        - higher natural material ratio -> more restorative,
        - mid-level visual entropy -> more restorative,
        - lower clutter and processing load -> more restorative,
      with clearly documented weights.
    - maps this score into a 3-level label {low, mid, high}.
    - appends:
        - a BN-like node to `bn.nodes` with `name="affect.restorative_h1"` and a one-hot
          posterior on the chosen bin, and
        - a derived tag to `tags` with `status="derived"`, `raw_value` equal to the numeric
          score, and `bin` equal to the label.
    - fails safely (returns no node/tag) if fewer than two of the required features are
      available for a given image.

- Governance:
  * `governance/TODO_running_list.md` updated with a v3.4.32 status entry describing this
    H1 restorativeness heuristic as a scaffold for future calibration and rule-family work.

All existing files from v3.4.31 are preserved; this version is strictly additive.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.36_upload_guard_and_bn_guard.md
----- CONTENT START -----
# v3.4.36 ‚Äì Admin upload hardening + BN naming guard

This version is strictly additive relative to v3.4.35 and focuses on
closing the specific concerns raised by the five-panel Ruthless
review.

## Admin bulk upload

- Replaced the truncated `/api/v1/admin/upload` implementation with a
  hardened endpoint that:
  - restricts uploads to `.jpg`, `.jpeg`, `.png`, `.webp`;
  - enforces a 10 MiB per-file size limit;
  - validates the entire batch before writing anything;
  - stores files under `IMAGE_STORAGE_ROOT` with UUID filenames; and
  - records `upload_batch_id` for all new `Image` rows.

- Extended `tests/test_admin_upload.py` with:
  - rejection of unsupported extensions;
  - rejection of oversized uploads; and
  - an atomicity test for mixed valid/invalid batches.

## BN naming and glossary

- Added `backend/science/bn_naming_guard.py` as an advisory naming
  guard for `candidate_bn_input` keys.
- Added `scripts/export_bn_glossary.py` to export a JSON glossary to
  `docs/BN_GLOSSARY_AUTO.json`.
- Added `docs/BN_NAMING_GUIDE.md` and
  `docs/INTERPRETING_RUTHLESS_REPORTS.md`.
- Added `reports/Ruthless_3.4.35_five_panel_summary.md` to capture
  the five-panel verdict and how v3.4.36 responds.

----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.60_semantic_tags_vlm.md
----- CONTENT START -----
# v3.4.60 ‚Äì Semantic style/room tags + cost-aware VLM

This version is strictly additive relative to v3.4.51 and focuses on
turning a subset of semantic tags from *stub* to live, cost-accounted
VLM-backed features.

## Semantic VLM analyzer

- Added `backend/science/semantics/semantic_tags_vlm.py` with a new
  `SemanticTagAnalyzer` that:
  - encodes the in-memory RGB frame as JPEG;
  - calls the configured VLM with an explicit JSON-only prompt; and
  - emits:
    - `style.*` scores (modern, traditional, minimalist, scandinavian,
      industrial, rustic, bohemian, farmhouse, japandi);
    - `spatial.room_function.*` scores (living_room, kitchen, bedroom,
      home_office, bathroom).

- The analyzer:
  - records a `semantics` metadata block with primary style and room
    function guesses;
  - runs in *metadata-only* mode when `StubEngine` is active; and
  - fails soft, recording an error flag without breaking the pipeline.

## Cost-aware semantics

- Mirrored the `CognitiveStateAnalyzer` pattern by logging a single
  cost entry per semantic VLM call via `backend.services.costs.log_vlm_usage`,
  tagged with `source="science_pipeline_semantic_tags"`.

- Kept L2 analyzers as explicit opt-ins:
  - `SciencePipelineConfig.enable_cognitive` stays `False` by default.
  - New `SciencePipelineConfig.enable_semantic` is also `False` by default.

## Registry + stub hygiene

- Removed `style.*` and `spatial.room_function.*` keys from
  `backend/science/feature_stubs.py` now that they have a concrete
  compute implementation.
- This keeps the feature coverage test strict: semantic keys are now
  treated as live features backed by the science pipeline rather than
  permanent stubs.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.62_science_tag_coverage_enforced.md
----- CONTENT START -----
# Image Tagger v3.4.62 ‚Äî Science Tag Coverage Enforced

This release builds directly on v3.4.60 (semantic_tags_vlm base) and
includes two governance-focused changes:

## 1. Science Tag Coverage Map (Sprint A recap)

- New script: `scripts/generate_tag_coverage.py`.
- Produces a machine-readable snapshot at `science_tag_coverage_v1.json` that
  summarises:
    - total known feature keys
    - which keys have at least one compute implementation
    - which keys are marked as stubs
    - source_type classification:
      - `math_or_deterministic`
      - `vlm_cognitive`
      - `vlm_semantic`
      - `stub_only`
      - `unassigned` (should remain zero).
- Writes a human-readable summary to `docs/SCIENCE_TAG_MAP.md`.

## 2. Governance Enforcement for Tag Coverage (Sprint B)

- `v3_governance.yml` gains a new constraint flag:
    - `constraints.enforce_science_tag_coverage: true`
- `scripts/guardian.py` now includes `_check_science_tag_coverage(...)` and
  calls it from `verify(...)`.
- When the flag is enabled, `guardian verify` will fail if:
    - `science_tag_coverage_v1.json` is missing or unreadable; or
    - any feature keys are reported with `source_type == "unassigned"`.
- This ensures that every feature key in the union of registry/stub/computed
  keys is either:
    - wired via a math/VLM analyzer, or
    - explicitly tracked as a stub in `backend/science/feature_stubs.py`.

## Notes

- In this snapshot, the meta section of `science_tag_coverage_v1.json`
  reports zero `unassigned` keys; all tracked features are either wired or
  explicitly stubbed.
- Legacy discrepancies between `governance.lock` and the current working
  tree (e.g. missing __pycache__ files or changed hashes in protected
  modules) will still cause `guardian verify` to fail until the baseline is
  consciously refreshed via `guardian freeze` in the target environment.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.63_bn_db_tightening_and_health.md
----- CONTENT START -----
# Image Tagger v3.4.63 ‚Äî BN / DB Tightening + Optional Health Guard

Scope of this sprint
--------------------
- Tie BN-facing Validation rows more tightly to the canonical Attribute
  registry used throughout the system.
- Provide an optional, database-aware health check that can be run
  manually or via the governance guardian to detect drift between:
    * `validations.attribute_key`
    * `attributes.key`
    * BN candidate keys from the science index catalog.

1. Validation.attribute_key ‚Üí ForeignKey(attributes.key)
--------------------------------------------------------

The `Validation` model in `backend/models/annotation.py` now declares
`attribute_key` as:

    ForeignKey("attributes.key")

This means every validation row (including those emitted by the science
pipeline and BN export) must reference a key present in the `attributes`
table in fresh schemas.

The model also includes an explicit relationship to `Attribute`:

    attribute = relationship("Attribute", back_populates="validations")

2. Attribute ‚Üî Validation relationship
--------------------------------------

The `Attribute` model in `backend/models/attribute.py` now exposes:

    validations = relationship("Validation", back_populates="attribute")

This gives a clean ORM join path both ways for BN snapshots, training
exports, and any downstream analytics that need to reason over the
attribute registry and its associated validation records.

3. Schema sanity test for the FK
--------------------------------

Added `tests/test_validation_attribute_fk_schema.py` to assert that the
`Validation` table includes a foreign key whose target is `attributes.key`.

This test is database-agnostic (it inspects SQLAlchemy table metadata)
and will fail fast if future refactors accidentally drop or rename the FK.

4. BN / DB health checker module
--------------------------------

New module: `backend/scripts/bn_db_health.py`

- Provides a function:

      run_health_check(exit_on_failure: bool = True) -> Dict[str, Any]

  that:

    * collects active `attributes.key` values;
    * collects distinct `Validation.attribute_key` values;
    * obtains BN candidate keys via `get_candidate_bn_keys()`; and
    * reports:

          - orphan Validation.attribute_key values (not in attributes.key)
          - BN candidate keys that are missing from attributes.key

- CLI usage (from repo root):

      python -m backend.scripts.bn_db_health

  or equivalently:

      python backend/scripts/bn_db_health.py

The CLI exits with status 1 if any violations are detected.

5. Optional integration into the governance guardian
----------------------------------------------------

- `scripts/guardian.py` now exposes `_check_bn_db_health`, which can be
  enabled via a new constraint:

      constraints:
        ...
        check_bn_db_health: false

- When `check_bn_db_health` is set to `true` in `v3_governance.yml` and
  `guardian.py verify` is run in an environment with a live database,
  the BN / DB health check will run in-process and report any violations
  as governance failures.

Operational notes
-----------------

- Existing databases created before v3.4.63 will not automatically gain
  the new FK constraint on `validations.attribute_key`. For production
  deployments you should either:

    * run an ALTER TABLE to add the FK constraint manually, or
    * recreate the database schema from scratch using the updated models.

- The existing `backend/scripts/seed_attributes.py` remains the
  recommended way to populate the `attributes` table before running
  science or BN export.

- The BN / DB health check requires a running database and an environment
  where `SessionLocal` is correctly configured (e.g., inside Docker via
  `docker-compose exec api ...`). By default the guardian constraint
  `check_bn_db_health` is `false` to avoid breaking installs that do not
  yet have a live DB.
----- CONTENT END -----
----- FILE PATH: CHANGELOG_v3.4.64_bn_db_migration_helper.md
----- CONTENT START -----
# Changelog v3.4.64 ‚Äì BN/DB Migration Helper and Docs

- Added ``backend/scripts/migrate_3_4_63_add_validation_fk.py``, an idempotent
  helper for adding the missing ``Validation.attribute_key ‚Üí attributes.key``
  foreign key on legacy PostgreSQL databases created before v3.4.63.
- Extended ``docs/devops_quickstart.md`` with a new section on BN / DB health
  checks and the legacy FK migration workflow, including example Docker
  commands for TAs and developers.
- No changes to the science pipeline, API surface, or frontend behaviour.
  Fresh installs of v3.4.63+ remain the canonical baseline; v3.4.64 is a
  documentation and operations refinement for teams with existing databases.
----- CONTENT END -----
----- FILE PATH: Makefile
----- CONTENT START -----


# VLM health check convenience targets
RUN_ID ?= $(shell date +%F)_$(shell cat VERSION)_main_vlm

vlm-health-init:
	mkdir -p reports/vlm_health/$(RUN_ID)/raw reports/vlm_health/$(RUN_ID)/derived
	cp reports/bn_validations_flat.csv reports/vlm_health/$(RUN_ID)/raw/ || true
	cp reports/vlm_validations.csv    reports/vlm_health/$(RUN_ID)/raw/ || true
	cp reports/human_validations.csv  reports/vlm_health/$(RUN_ID)/raw/ || true
	@echo "Initialised vlm_health run at reports/vlm_health/$(RUN_ID)"

vlm-health-audit:
	python scripts/audit_vlm_variance.py \
	  reports/vlm_health/$(RUN_ID)/raw/bn_validations_flat.csv \
	  --out reports/vlm_health/$(RUN_ID)/derived/vlm_variance_audit.csv \
	  --source-column source \
	  --attribute-column attribute_key \
	  --value-column value \
	  --source-prefix science_pipeline.vlm

vlm-health-panel:
	python scripts/vlm_turing_test_prep.py \
	  --vlm   reports/vlm_health/$(RUN_ID)/raw/vlm_validations.csv \
	  --human reports/vlm_health/$(RUN_ID)/raw/human_validations.csv \
	  --out   reports/vlm_health/$(RUN_ID)/raw/vlm_turing_panel.csv \
	  --max-trials 400 \
	  --seed 42

vlm-health-score:
	python scripts/vlm_turing_test_score.py \
	  --panel reports/vlm_health/$(RUN_ID)/raw/vlm_turing_panel_completed.csv \
	  > reports/vlm_health/$(RUN_ID)/derived/vlm_turing_summary.txt
----- CONTENT END -----
----- FILE PATH: PROJECT_CONSTITUTION.md
----- CONTENT START -----
# Project Constitution: Image Tagger v3 (v3.4.12)

This document encodes the core rules for how the Image Tagger v3 repository should evolve.
It is meant to be read by humans (students, collaborators, AI agents) before making changes.

## 1. Versioning and releases

- The v3.x line is versioned as `3.M.N` where `N` increments for each release.
- Each release must be shippable as:
  - A ZIP of the full directory tree.
  - A single concatenated TXT file with a `deconcat.py` header that can reconstruct the tree.
- A later version should be a **superset** of earlier releases in terms of file paths.
  If code is removed from active use, it should be archived rather than deleted.

## 2. No-deletion rule (with archives)

- Do not delete files that have shipped in a prior release.
- When replacing or majorly refactoring a file, move the old version into an `archive/`
  subtree such as:
  - `archive/v3_2_38_old_science/‚Ä¶`
- New files can be added freely, as long as they pass syntax checks and do not break imports.

## 3. Governance and Guardian

- The file `v3_governance.yml` and `scripts/guardian.py` define a ‚Äúdrift shield‚Äù over
  critical parts of the system (science modules, API, deploy scripts, governance config).
- Contributors should run `python scripts/guardian.py verify` before proposing a new release.
- If intentional changes are made to protected scopes, the baseline can be updated with
  `python scripts/guardian.py freeze` after review.

## 4. Code quality guarantees

For active (non-archived) code:

- No `...` placeholders in Python modules.
- No syntax errors in `.py` files under `backend/`, `scripts/` or `tests/`.
- No obvious stubs that would cause runtime failures when a documented feature is used.
- Every API router and endpoint imported in `backend/main.py` must exist and be importable.

## 5. Science and UX honesty

- Science modules are heuristic and deterministic; they should be implemented in a way that
  allows students to:
  - Read the code.
  - Reproduce results.
  - Modify thresholds and see predictable changes.
- UX should be honest about system state:
  - When there is no data yet, screens should say so explicitly.
  - When the backend is unreachable or access is denied, error messages should be clear.

## 6. Documentation guarantees

Each release should keep student-facing documentation reasonably aligned with reality, e.g.:

- `README_v3.md` for high-level overview and quickstart.
- `docs/science_overview.md` for the science pipeline.
- `docs/devops_quickstart.md` for setup and basic operations.
- `docs/governance_guide.md` for governance and Guardian.
- `docs/AI_COLLAB_WORKFLOW.md` for the multi-agent AI collaboration rules specific to this project.

If the behaviour of a core subsystem changes (science pipeline, routers, governance), the
corresponding document should be updated in the same release.

## 7. Contributions

- Prefer small, well-structured changes with clear commit messages or release notes.
- When making structural changes (for example new routers, new science analyzers), update:
  - Relevant docs.
  - Tests where applicable.
  - The index catalog if new indices are introduced.
- When working with AI tools, follow `docs/AI_COLLAB_WORKFLOW.md` so that all AIs respect
  the same governance and packaging rules.

By following this constitution, the Image Tagger v3 line can evolve rapidly while remaining
readable, teachable and robust enough for real experiments.
----- CONTENT END -----
----- FILE PATH: README_v3.md
----- CONTENT START -----
# Image Tagger v3.4.12 - Enterprise Edition

This is the production-ready, micro-frontend architecture for the Image Tagger system.

## üèóÔ∏è Architecture
* **Frontend:** Monorepo with 4 distinct React Apps (Workbench, Monitor, Admin, Explorer).
* **Backend:** Unified FastAPI service with PostgreSQL.
* **Infrastructure:** Docker Compose orchestration with Nginx Gateway.

## üöÄ Quick Start (The "Enterprise Go")

1.  **Ensure Docker is installed.**
2.  **Run:**
    ```bash
    cd deploy
    docker-compose up --build
    ```
3.  **Access the GUIs:**
    * **Research Explorer:** [http://localhost:8080/explorer](http://localhost:8080/explorer)
    * **Tagger Workbench:** [http://localhost:8080/workbench](http://localhost:8080/workbench)
    * **Supervisor Monitor:** [http://localhost:8080/monitor](http://localhost:8080/monitor)
    * **Admin Cockpit:** [http://localhost:8080/admin](http://localhost:8080/admin)

## üß™ Running Tests

To verify the API logic without Docker:
1.  `pip install pytest httpx`
2.  `pytest tests/test_v3_api.py`

## ü§ñ AI Collaboration Workflow

For guidelines on how to use LLMs (ChatGPT, Claude, Gemini, etc.) with this
repository ‚Äî including ZIP + concatenated TXT expectations and Guardian
governance rules ‚Äî see:

- `docs/AI_COLLAB_WORKFLOW.md`


## Quickstart & Seeding (v3.4.12)

From the repository root:

```bash
# Build containers, run seeds, and execute smoketests
./install.sh
```

The install script is intentionally small and opinionated. In the default configuration it will:

1. Build the containers (API, DB, frontend) using the `deploy/` Dockerfiles.
2. Run seeding scripts to populate core configuration:
   - `backend/scripts/seed_tool_configs.py`
   - `backend/scripts/seed_attributes.py`
3. Run cheap smoketests:
   - `scripts/smoke_api.py` (basic API shape)
   - `scripts/smoke_science.py` (science pipeline + composite indices and bins)

If any step fails, the script prints a clear message and returns a non-zero exit code.

Once `install.sh` completes successfully, you can point a browser at the frontend portal (typically `http://localhost:8000/index.html` or your configured frontend host) and choose the appropriate GUI:

- Tagger Workbench
- Supervisor Monitor
- Admin Cockpit
- Research Explorer

## Known Limitations (v3.4.12)

This v3 line is designed as a teachable, inspectable system rather than a fully productized SaaS. In particular:

- **Science modules** implement a deterministic, heuristic-based pipeline. Hooks for VLM / external models exist but are intentionally stubbed out by default to keep costs predictable.
- **Composite indices and bins** (for example, `science.visual_richness[_bin]`, `science.organized_complexity[_bin]`) are deliberately simple and intended as a starting point for BN and downstream modeling, not final scientific truth.
- **CI workflow** is included as a template. It shows how to wire Guardian and basic tests but may require adaptation to your specific infrastructure (Python versions, DB configuration, secrets).
- **Empty dashboards** in Monitor / Explorer generally mean you have not yet:
  - Run the seeding scripts, and
  - Processed any images through the science pipeline, or
  - Collected enough validation data for IRR / Tag Inspector to be informative.

## CI Skeleton

The repository includes a minimal GitHub Actions workflow under `.github/workflows/ci_v3.yml`. It is meant as a starting point for teams who want to:

1. Guard against repository drift via `scripts/guardian.py verify`.
2. Exercise core API contracts via `pytest tests/test_v3_api.py`.
3. Optionally, run `scripts/smoke_science.py` as a lightweight end-to-end science check.

The CI recipe assumes a standard Python environment and may need adjustments to match your Docker / DB setup or your preferred dependency-management strategy.

## Minimal test suite

Before cutting a new release or making substantial changes, run:

- `pytest tests/test_v3_api.py` ‚Äì API and RBAC sanity checks.
- `pytest tests/test_guardian.py` ‚Äì governance and Guardian behaviour.
- `pytest tests/test_bn_export_smoke.py` ‚Äì BN export tied to Validation.
- `pytest tests/test_workbench_smoke.py` ‚Äì Tagger Workbench basic flow.
- `pytest tests/test_explorer_smoke.py` ‚Äì Explorer attributes and search.

For deeper science verification, you can also run `pytest -m slow` to include
`tests/test_science_pipeline_smoke.py`, which exercises the full science
pipeline on a synthetic image.

## Science Debug Layers

For an explanation of the edge-map and overlay debug views in the Explorer,
see `docs/SCIENCE_DEBUG_LAYERS.md`. This document is intended as a teaching
aid to help students understand how Canny edge detection and parameter
settings relate to visual complexity and the science pipeline.

## Production Deployment

For guidance on running Image Tagger beyond a single development laptop
(e.g. on a lab server or departmental host), see
`docs/PRODUCTION_DEPLOYMENT.md`. It covers environment variables,
volumes, HTTPS / reverse proxy structure, and a minimal security
checklist.



## Optional: AutoInstaller + AI Copilot (v1.3.0)

You can also use the shared **AutoInstaller + AI Copilot** kit:

```bash
bash infra/turnkey_installer_v1.3/installer/install.sh
```

This will:
- Write a detailed log to `logs/install.log`
- Run the native `install.sh` for Image Tagger via `infra/turnkey_installer_v1.3/installer_config.json`
- Optionally allow an AI copilot to propose remediation steps from the logs:

```bash
python ai/installer_copilot.py --logfile logs/install.log --out logs/ai_plan.json --dry-run 1 --provider none
```
----- CONTENT END -----
----- FILE PATH: STUDENT_START_HERE.md
----- CONTENT START -----
# STUDENT START HERE ‚Äì Image Tagger 3.4.74_vlm_lab_notebook_TL_runbook

If you are a **student** using Image Tagger for a course, this is your entry point.

## 1. What you should do first

1. Read: `docs/ops/Student_Quickstart_v3.4.73.md`.  
   This explains, in one page:
   - what Image Tagger is,
   - the two ways you might use it in the course,
   - and exactly what you are expected to do.
2. Check with your TA which **track** your course is using:
   - **Track A ‚Äì Full App (persistent, Docker-based)**  
     You will normally be given a URL to a running instance, or in some cases asked to run Docker locally.
   - **Track B ‚Äì Colab Science Notebook (ephemeral)**  
     You will use the `notebooks/VLM_Health_Lab.ipynb` notebook in Google Colab to run a small ‚ÄúVLM Health Lab.‚Äù

Once you know your track, follow the corresponding section in the Student Quickstart.

## 2. Quick mental model

- Image Tagger is a research tool for **tagging architectural images** and analysing how different spaces are perceived.
- In the course, you will **not** be rewriting the internals.
  You will:
  - run tagging jobs (Track A), or
  - run a small, scripted lab (Track B),
  - and then interpret or analyse the outputs.

## 3. If you are on Track A (Full App)

- Your TA will either:
  - give you a URL (recommended), or
  - ask you to run the app via Docker (only if you are comfortable with that).
- Follow the ‚ÄúTrack A ‚Äì Full App‚Äù section in `docs/ops/Student_Quickstart_v3.4.73.md`.

## 4. If you are on Track B (Colab VLM Health Lab)

- You will open a Colab notebook (`VLM_Health_Lab.ipynb`),
  upload a copy of this repository zip when prompted,
  and step through a small, **self-contained** experiment that:
  - creates a tiny dataset of synthetic images,
  - runs the Image Tagger science pipeline on them,
  - runs a VLM variance audit.
- Follow the ‚ÄúTrack B ‚Äì Colab Notebook‚Äù section in `docs/ops/Student_Quickstart_v3.4.73.md`.

**Remember:** in Colab, nothing in `/content` persists after the runtime restarts.
Download or save your outputs (CSVs, text summaries) if you will need them later.

## 5. When you need help

When you contact your TA for help, always include:

- which track you are using (**Track A** or **Track B**),
- what you were trying to do in one sentence,
- a screenshot of the error, and
- either:
  - the URL/path you were on (for Track A), or
  - the notebook cell you just ran (for Track B).

With that information, your TA can usually diagnose the problem quickly.
----- CONTENT END -----
----- FILE PATH: VERSION
----- CONTENT START -----
3.4.74_vlm_lab_notebook_TL_runbook
----- CONTENT END -----
----- FILE PATH: auto_install.sh
----- CONTENT START -----
#!/usr/bin/env bash
#
# auto_install.sh
#
# Safer, logged bootstrap wrapper for install.sh.
# - strict mode
# - robust Python discovery
# - dual logging (console + logs/)
# - optional virtualenv creation
# - Docker preflight check
#
set -euo pipefail
echo "[Info] If you are a STUDENT running this script, please make sure you have read:"
echo "       STUDENT_START_HERE.md  (at the repo root)"
echo "       docs/ops/Student_Quickstart_v3.4.73.md"

# Resolve project root to this script's directory and cd into it
ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$ROOT_DIR"

# --- Python discovery ----------------------------------------------------
PYTHON_CMD="$(command -v python3 || true)"
if [ -z "$PYTHON_CMD" ]; then
  PYTHON_CMD="$(command -v python || true)"
fi

if [ -z "$PYTHON_CMD" ]; then
  echo "‚ùå Critical: No Python interpreter found (python3 or python)."
  exit 1
fi

# --- Dual logging: stdout/stderr to console and file --------------------
mkdir -p logs
LOG_FILE="logs/install_$(date +%Y%m%d_%H%M%S).log"
echo "üìì Logging install to ${LOG_FILE}"
exec > >(tee -i "${LOG_FILE}") 2>&1

# --- Virtualenv enforcement ---------------------------------------------
if [ -z "${VIRTUAL_ENV:-}" ]; then
  echo "‚ö†Ô∏è  No virtualenv detected. Creating .venv..."
  "${PYTHON_CMD}" -m venv .venv
  # shellcheck disable=SC1091
  source .venv/bin/activate
  echo "‚úÖ Activated virtualenv at .venv"
else
  echo "‚úÖ Using existing virtualenv at ${VIRTUAL_ENV}"
fi

# --- Docker preflight ----------------------------------------------------
if ! command -v docker >/dev/null 2>&1; then
  echo "‚ùå Fatal: Docker is required but was not found in PATH."
  echo "    Please install Docker Desktop or a compatible Docker engine."
  exit 1
fi

# Optional: warn if docker-compose is missing (install.sh expects it)
if ! command -v docker-compose >/dev/null 2>&1; then
  echo "‚ö†Ô∏è  docker-compose not found on host. install.sh will call 'docker-compose';"
  echo "    ensure it is installed or aliased appropriately."
fi

# --- Delegate to main installer -----------------------------------------
echo "üöÄ Delegating to install.sh..."
bash install.sh
----- CONTENT END -----
----- FILE PATH: deconcat.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
Deconcatenation helper for Image Tagger multi-file bundles.

This script understands files produced with the following marker format:

    ----- FILE PATH: <relative/path>
    ----- CONTENT START -----
    ... file contents ...
    ----- CONTENT END -----

Usage
-----

    python deconcat.py Image_Tagger_v3.4.63_bn_db_tightening_health_full.txt ./output_dir

The output directory will be created if it does not exist. Existing files
with the same paths will be overwritten.
"""
from __future__ import annotations

import sys
from pathlib import Path
from typing import TextIO


FILE_PATH_PREFIX = "----- FILE PATH: "
CONTENT_START = "----- CONTENT START -----"
CONTENT_END = "----- CONTENT END -----"


def _iter_lines(fp: TextIO):
    """Yield lines from a file-like object, normalizing newlines."""
    for line in fp:
        # Ensure we work with plain str and strip trailing newlines only
        yield line.rstrip("\n")


def deconcat(concat_path: Path, out_dir: Path) -> None:
    if not concat_path.exists():
        raise FileNotFoundError(concat_path)

    out_dir.mkdir(parents=True, exist_ok=True)

    current_rel: Path | None = None
    buffer: list[str] = []
    in_content = False

    with concat_path.open("r", encoding="utf-8", errors="ignore") as fp:
        for raw_line in _iter_lines(fp):
            if raw_line.startswith(FILE_PATH_PREFIX) and not in_content:
                # Start a new file section
                current_rel = Path(raw_line[len(FILE_PATH_PREFIX) :].strip())
                buffer = []
                continue

            if raw_line == CONTENT_START and current_rel is not None and not in_content:
                in_content = True
                continue

            if raw_line == CONTENT_END and in_content and current_rel is not None:
                # Flush the current buffer to disk
                target_path = out_dir / current_rel
                target_path.parent.mkdir(parents=True, exist_ok=True)
                target_path.write_text("\n".join(buffer) + "\n", encoding="utf-8")
                # Reset for the next file
                current_rel = None
                buffer = []
                in_content = False
                continue

            if in_content and current_rel is not None:
                buffer.append(raw_line)

    if in_content and current_rel is not None and buffer:
        # Gracefully handle a missing CONTENT_END at EOF by writing
        target_path = out_dir / current_rel
        target_path.parent.mkdir(parents=True, exist_ok=True)
        target_path.write_text("\n".join(buffer) + "\n", encoding="utf-8")


def main(argv: list[str] | None = None) -> None:
    args = list(argv) if argv is not None else sys.argv[1:]
    if len(args) != 2:
        print("Usage: python deconcat.py <concatenated.txt> <output-dir>")
        raise SystemExit(1)

    concat_path = Path(args[0]).expanduser().resolve()
    out_dir = Path(args[1]).expanduser().resolve()

    deconcat(concat_path, out_dir)
    print(f"[deconcat] Wrote files into {out_dir}")


if __name__ == "__main__":  # pragma: no cover - thin CLI wrapper
    main()
----- CONTENT END -----
----- FILE PATH: deconcat_v3_3.py
----- CONTENT START -----
import os
from pathlib import Path

def deconcat(filename="Image_Tagger_v3.3.0_Grand_Jury_Fixes.txt"):
    with open(filename, "r", encoding="utf-8") as f:
        lines = f.readlines()

    current_file = None
    buffer = []
    
    print("üöÄ Applying v3.3.0 Grand Jury Fixes...")

    for line in lines:
        if line.startswith("----- FILE PATH:"):
            if current_file and buffer:
                write_file(current_file, buffer)
                buffer = []
            rel_path = line.split("----- FILE PATH:")[1].strip()
            current_file = Path(rel_path)
        
        elif line.startswith("----- CONTENT START -----"):
            continue
        elif line.startswith("----- CONTENT END -----"):
            continue
        else:
            if current_file:
                buffer.append(line)

    if current_file and buffer:
        write_file(current_file, buffer)

def write_file(path: Path, content: list):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.writelines(content)
    print(f"‚úÖ Updated: {path}")

if __name__ == "__main__":
    deconcat()
----- CONTENT END -----
----- FILE PATH: governance.lock
----- CONTENT START -----
{
  "constraints": {
    "min_file_size_bytes": 20,
    "prevent_new_root_files": true
  },
  "critical_files": [
    "backend/main.py",
    "backend/models/config.py",
    "backend/database/core.py",
    "backend/models/annotation.py",
    "backend/api/v1_annotation.py",
    "backend/science/pipeline.py",
    "deploy/docker-compose.yml",
    "deploy/nginx.conf",
    "deploy/Dockerfile.backend",
    "scripts/guardian.py",
    "v3_governance.yml",
    "README_v3.md",
    "install.sh"
  ],
  "policy_version": "3.0.0",
  "protected_files": {
    "backend/database/__init__.py": {
      "hash": "3877da7309c607610b45642fe229888c07a5eba568084c9bf96fbb387605a2b0",
      "size": 26
    },
    "backend/database/__pycache__/__init__.cpython-311.pyc": {
      "hash": "c9873490ab525233d11e966dbbab3a894193756a796a7c2c232de69e76ebe63b",
      "size": 171
    },
    "backend/database/__pycache__/core.cpython-311.pyc": {
      "hash": "fb6ca34f65221977a2c9ec898261bebc7ad57ecd68ab90b46d8f46b067991e8b",
      "size": 1547
    },
    "backend/database/__pycache__/mixins.cpython-311.pyc": {
      "hash": "bf06dff97df4ab5f3ce64b22982b1c838101d25f7a1d9733728a0e2f72daba3e",
      "size": 1304
    },
    "backend/database/core.py": {
      "hash": "70b3943d0046c47831652d243e62c5dd71b9b15ae89384598648ee52f6bc11eb",
      "size": 1061
    },
    "backend/database/mixins.py": {
      "hash": "8a64be69c949be93a7464568c0608efda52842b92164c1d9658d0aea8ee9eac3",
      "size": 616
    },
    "backend/models/__init__.py": {
      "hash": "e943fb632e5ef6e3f992cdc71fecf7a63b9eda28cd17b6def53c9f861082b703",
      "size": 544
    },
    "backend/models/__pycache__/__init__.cpython-311.pyc": {
      "hash": "3a0cc72cac1f90c6e04851334142e9454ad7dcb9e2de2c98f3072c0d818d63c7",
      "size": 732
    },
    "backend/models/__pycache__/annotation.cpython-311.pyc": {
      "hash": "b00ef6870d2ec70f19770c08afb3a8ecddc7789acee9316468cddfbd8d5f3dc8",
      "size": 2010
    },
    "backend/models/__pycache__/assets.cpython-311.pyc": {
      "hash": "6c7556b0b24809e35d7293697c609fa2112a4914c7a096b49d5b77d03ce1f53e",
      "size": 2889
    },
    "backend/models/__pycache__/attribute.cpython-311.pyc": {
      "hash": "2aeb34946a0a6409ff32792568caeadc04a6bb0af4ea7f42b997510f958b2fdd",
      "size": 2397
    },
    "backend/models/__pycache__/config.cpython-311.pyc": {
      "hash": "84ede22b2657099f8089f0ffff8544f0bedafd8a59e37ca6cd8345d9de5697e3",
      "size": 1826
    },
    "backend/models/__pycache__/users.cpython-311.pyc": {
      "hash": "bb181770c60f0c9df4afbaf91b289ab449f32a3588b9f374d3ac03572af51235",
      "size": 1967
    },
    "backend/models/annotation.py": {
      "hash": "10b4efd808db8e933a70580723f0ee11ed3bd9c347f119745fd8b26a737bb848",
      "size": 1356
    },
    "backend/models/assets.py": {
      "hash": "6a71e24713bf9143ae4030ea91ffe93b8d00802a41f31142e8bc958b50c5ddfd",
      "size": 1964
    },
    "backend/models/attribute.py": {
      "hash": "b05450a0c4770374fc0d08bdfa8b733296f9200dd34eee1f4a63f20437ea100f",
      "size": 1188
    },
    "backend/models/config.py": {
      "hash": "9855095a98fc4c733ed27acfd4401dcf545f3bd19ea1523f0da8b710ec6b4160",
      "size": 1252
    },
    "backend/models/users.py": {
      "hash": "a0e1d5911597b2a18d5711d397c58253d26ed582f169e1516b34be26f6a813a4",
      "size": 1192
    },
    "backend/science/__init__.py": {
      "hash": "f0be820d4df5b438267d41177779831df166bc8bb5581297b6eaaceb36c4835d",
      "size": 25
    },
    "backend/science/__pycache__/__init__.cpython-311.pyc": {
      "hash": "d3ae8a952cfd71e6206fd78240aa15bbaf221bae4910e5f5b4f927d084e48423",
      "size": 170
    },
    "backend/science/__pycache__/core.cpython-311.pyc": {
      "hash": "a342f41ab2180d805899459867ff698ab81ad2d38de22d343418fd3766b82199",
      "size": 2157
    },
    "backend/science/__pycache__/perception.cpython-311.pyc": {
      "hash": "2b3239eb42a72c2c69d89d1d05ccb2f8d2202d80514cf6bb7aa3c9616709b506",
      "size": 2203
    },
    "backend/science/__pycache__/pipeline.cpython-311.pyc": {
      "hash": "4d0118ba280407cb1505219bbf9cd1e90a543a387c3c61b9f7e3cad029b8fd20",
      "size": 6646
    },
    "backend/science/__pycache__/vision.cpython-311.pyc": {
      "hash": "9e2904e6161bcbcc4ac73decc3a6bd0d473f6e0a0d615860763a65a7a0c86901",
      "size": 3841
    },
    "backend/science/context/__pycache__/cognitive.cpython-311.pyc": {
      "hash": "a821c9ee30ca69f80d1ed91a26f6b2ef0af37ee45b495f792aa5fc07c0edece1",
      "size": 1617
    },
    "backend/science/context/__pycache__/social.cpython-311.pyc": {
      "hash": "739f996ffb7e2b9026966507bf481da7b4f1bfd1b0ca28b6b8f75619b506e251",
      "size": 1748
    },
    "backend/science/context/cognitive.py": {
      "hash": "eeb24d823331dd0725b8a2160fc0a3b559314a5b9d33033f483c19de525f977c",
      "size": 878
    },
    "backend/science/context/social.py": {
      "hash": "6061f94865c7b9f36309edee4f3670efc932f3995b91a0634668bb49029d5c95",
      "size": 1257
    },
    "backend/science/core.py": {
      "hash": "2ffdb976505a30d50b2a259fd11228f2806a9ed89207af134a6835881cd15d27",
      "size": 1057
    },
    "backend/science/math/__pycache__/color.cpython-311.pyc": {
      "hash": "5b31522507f56283283ac3565c0c2b267b7eec21b38bfc91a7f429133b849662",
      "size": 6484
    },
    "backend/science/math/__pycache__/complexity.cpython-311.pyc": {
      "hash": "68d3e860e1c4996f8ccc398db800b09f37d7c0e0fc51c1a1715c3d833aec3d65",
      "size": 2925
    },
    "backend/science/math/__pycache__/fractals.cpython-311.pyc": {
      "hash": "b2c6ecd09913cd1d0052c205347486ab7515591530690664e4ca23be27f72d8e",
      "size": 3986
    },
    "backend/science/math/__pycache__/glcm.cpython-311.pyc": {
      "hash": "a168352e94de203ed463988ddc0272ad4b306876fa1acfdea05d50df2f383fac",
      "size": 5041
    },
    "backend/science/math/color.py": {
      "hash": "eb97e4ac9017a7578d433978a7e3e48c0edb01c66fec7db6b376995ce9d5f183",
      "size": 4369
    },
    "backend/science/math/complexity.py": {
      "hash": "12aba68bf741e674e21e07fb81c9d65c2f08e810011f1e09d21d277edb1e068b",
      "size": 1813
    },
    "backend/science/math/fractals.py": {
      "hash": "5efd701b6dc64d6c0904dfdd7a2f9cfd1706b0d474aa301e92c2202de3a551d0",
      "size": 2142
    },
    "backend/science/math/glcm.py": {
      "hash": "f13205cb86ed9c22e195de1a0c8cb9c1095a00c7e0e113ad1d7fa8afcb62074c",
      "size": 3281
    },
    "backend/science/perception.py": {
      "hash": "b1c282f7b610218885c3286299f6a3c6f14a80107d042724a558b4e5d06d047e",
      "size": 1408
    },
    "backend/science/pipeline.py": {
      "hash": "129b9503225457bf75e0d51180ed7c9683d79b33058102fa7e3a3ed3fa7cce95",
      "size": 4259
    },
    "backend/science/spatial/__pycache__/isovist.cpython-311.pyc": {
      "hash": "6c0c7486c9b99286d315aed703dd77176a2a2801968663f293ceeb37f902d0c6",
      "size": 2569
    },
    "backend/science/spatial/isovist.py": {
      "hash": "e6c5680ddf2ebe51ee36380d83fe3a6b7041cac669b448b49aa1ecee4141b189",
      "size": 1871
    },
    "backend/science/vision.py": {
      "hash": "6c855f208c009e29861b282144314ffe57da754dc4c1aa8d0bf849a1f43d2d48",
      "size": 2253
    },
    "backend/science/vision/__pycache__/materials.cpython-311.pyc": {
      "hash": "ef4ba327bbc00616c134025645edb9d90fe88f4c03669881d10eb9264db3a186",
      "size": 3665
    },
    "backend/science/vision/__pycache__/objects.cpython-311.pyc": {
      "hash": "356889cdc8d950af94f408e6a44d757eeedd666200d66c5f9faed49fc33eb7a2",
      "size": 2687
    },
    "backend/science/vision/materials.py": {
      "hash": "b626eb15c1eabc7b93fe4a74af429b5bc19a6b9b973a947b1dc90b8194b5f9ce",
      "size": 2586
    },
    "backend/science/vision/objects.py": {
      "hash": "53f85a85b4322de922764a02ce5947b75a4ee6d120955c8481d68fc9f4452fe4",
      "size": 1765
    },
    "frontend/apps/admin/index.html": {
      "hash": "37620e963b95668174bc0af8fa959f0ce906c329b1a20a7d33bbf931f4ccb8ff",
      "size": 337
    },
    "frontend/apps/admin/package.json": {
      "hash": "13db837fbb95386f00424858c8c020d3f35b6fa091cfe6fd2ce0e99e52f902cf",
      "size": 147
    },
    "frontend/apps/admin/public/feature_onboarding.html": {
      "hash": "2531b0e0427affe6ebdc5d5d5ebad87ada036492c9d90b96e20f8032f580ab72",
      "size": 2717
    },
    "frontend/apps/admin/src/App.jsx": {
      "hash": "ca4f718e7f054b14baa0f634d54c12356f0a9580188c498eb266b3ad77080027",
      "size": 16251
    },
    "frontend/apps/admin/src/main.jsx": {
      "hash": "301d7d0b864874bc303a81e652b30b32460eaa0b8dfdb8f08338dd89963f6e21",
      "size": 235
    },
    "frontend/apps/admin/vite.config.js": {
      "hash": "a6389be299f08218b06ab249c9b9c3ebd8d022214503f963890fb36ea6e17ac7",
      "size": 269
    },
    "frontend/apps/explorer/index.html": {
      "hash": "12db1e86c2b229a0e0ca1f3a39351867e624f738b444d02ab5a9211de041cc4c",
      "size": 354
    },
    "frontend/apps/explorer/package.json": {
      "hash": "2f3a0b177e481b7f203fdaf9649fac27e84dda76c5de0fcb0e4fbc5a6e89bcc3",
      "size": 150
    },
    "frontend/apps/explorer/src/App.jsx": {
      "hash": "17b12403cf1abcf1fcc9672bb2878424769d557edcd32edc894ec6f6c5055ef4",
      "size": 15205
    },
    "frontend/apps/explorer/src/main.jsx": {
      "hash": "301d7d0b864874bc303a81e652b30b32460eaa0b8dfdb8f08338dd89963f6e21",
      "size": 235
    },
    "frontend/apps/explorer/vite.config.js": {
      "hash": "a6389be299f08218b06ab249c9b9c3ebd8d022214503f963890fb36ea6e17ac7",
      "size": 269
    },
    "frontend/apps/monitor/index.html": {
      "hash": "32395ae9780efe876b0c1fd7bcf583bbe8b6a98e3ab3f30c30c23edf135b3c10",
      "size": 343
    },
    "frontend/apps/monitor/package.json": {
      "hash": "79ccdfecb5f3b574f4a46faf9edfb5a0c69f81b9a09aa08460d4b4e5ad45af85",
      "size": 149
    },
    "frontend/apps/monitor/src/App.jsx": {
      "hash": "e9f6af305f36bb73643eff29ffaf2a404a0015f0123db70d258abd1c0b064dbc",
      "size": 18333
    },
    "frontend/apps/monitor/src/main.jsx": {
      "hash": "301d7d0b864874bc303a81e652b30b32460eaa0b8dfdb8f08338dd89963f6e21",
      "size": 235
    },
    "frontend/apps/monitor/vite.config.js": {
      "hash": "a6389be299f08218b06ab249c9b9c3ebd8d022214503f963890fb36ea6e17ac7",
      "size": 269
    },
    "frontend/apps/workbench/index.html": {
      "hash": "fe20d9404f85762c1cfe70d0fac4f317a6f6e5ddcf0283700ab7b5b534f721c7",
      "size": 358
    },
    "frontend/apps/workbench/package.json": {
      "hash": "ead96376fd9cc3511266c0d7b2bf9435a4f011fff2c79a05236e37f13d0871a5",
      "size": 151
    },
    "frontend/apps/workbench/src/App.jsx": {
      "hash": "ff4fc0932e214f4d43999e08511d4584f3224c3bb6e261fb8ee38dab46528fe4",
      "size": 7476
    },
    "frontend/apps/workbench/src/main.jsx": {
      "hash": "301d7d0b864874bc303a81e652b30b32460eaa0b8dfdb8f08338dd89963f6e21",
      "size": 235
    },
    "frontend/apps/workbench/vite.config.js": {
      "hash": "a6389be299f08218b06ab249c9b9c3ebd8d022214503f963890fb36ea6e17ac7",
      "size": 269
    },
    "frontend/shared/package.json": {
      "hash": "1abc19e430f91e7349eec034e8034f237466befa44bfb53b345049ed24d7f142",
      "size": 104
    },
    "frontend/shared/src/api-client.js": {
      "hash": "144359afdfc6612011585887009b129fc31fb87cbd2d072dad4cf1cd582febb9",
      "size": 1647
    },
    "frontend/shared/src/components/Button.jsx": {
      "hash": "f1e9d6a82030b5d9d37bd76c9c69ff0b3bccd84f63feb2f7dea6b0174043a9f0",
      "size": 836
    },
    "frontend/shared/src/components/Header.jsx": {
      "hash": "63cc0a782cbbb773496002c1fe1bc3533cff032997e8c3519cee90ee48a481c2",
      "size": 1098
    },
    "frontend/shared/src/components/Toggle.jsx": {
      "hash": "e17f14b2459845cefe4344f7e8504ee3b33f710453b4ab6850390c1f13cadc32",
      "size": 1034
    },
    "frontend/shared/src/index.js": {
      "hash": "a3c3ee23129d7f32606dc326c3b75d9416d7c14482346406fb6953ca5842e8a1",
      "size": 179
    }
  },
  "root_files": [
    "README_v3.md",
    "governance.lock",
    "install.sh",
    "v3_governance.yml"
  ]
}----- CONTENT END -----
----- FILE PATH: install.sh
----- CONTENT START -----
#!/bin/bash
VERSION="dev"
if [ -f VERSION ]; then
  VERSION=$(cat VERSION)
fi
# Discover a suitable Python interpreter for host-side checks
if command -v python3 >/dev/null 2>&1; then
  PYTHON_CMD="python3"
elif command -v python >/dev/null 2>&1; then
  PYTHON_CMD="python"
else
  echo "‚ùå python3/python not found. Cannot run install-time guards."
  exit 1
fi

# Use a local virtual environment for host-side Python checks to avoid
# polluting the global interpreter.
if [ ! -d ".venv" ]; then
  echo "[install] Creating local Python virtual environment (.venv)..." 
  $PYTHON_CMD -m venv .venv || { echo "‚ùå Failed to create .venv"; exit 1; }
fi
if [ -x ".venv/bin/python" ]; then
  PYTHON_CMD=".venv/bin/python"
fi

echo "üöÄ Starting Image Tagger v3 (v${VERSION})"
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üß¨ Running structural hollow-repo guard"
${PYTHON_CMD} scripts/hollow_repo_guard.py || { echo "‚ùå Hollow Repo Guard failed"; exit 1; }

echo "üß™ Running program integrity guard"
${PYTHON_CMD} scripts/program_integrity_guard.py || { echo "‚ùå Program Integrity Guard failed"; exit 1; }
echo "üßØ Running syntax guard"

${PYTHON_CMD} scripts/syntax_guard.py || { echo "‚ùå Syntax Guard failed"; exit 1; }

echo "ü™ù Running critical import guard"

${PYTHON_CMD} scripts/critical_import_guard.py || { echo "‚ùå Critical Import Guard failed"; exit 1; }


echo "üß™ Running canon guard"
${PYTHON_CMD} scripts/canon_guard.py || { echo "‚ùå Canon Guard failed"; exit 1; }

echo "üîí Running Guardian (governance) checks"
if command -v ${PYTHON_CMD} &> /dev/null; then
    if [ -f "governance.lock" ]; then
        ${PYTHON_CMD} scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        ${PYTHON_CMD} scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è ${PYTHON_CMD} not found; skipping Guardian checks."
fi

# Security warning: detect default API_SECRET and warn user
DEFAULT_API_SECRET="dev_secret_key_change_me"
if [ "${API_SECRET:-$DEFAULT_API_SECRET}" = "$DEFAULT_API_SECRET" ]; then
    echo "‚ö†Ô∏è  WARNING: API_SECRET is using the default value 'dev_secret_key_change_me'."
    echo "    This is fine for local demos, but you MUST change it for any shared or deployed environment."
fi

echo "üê≥ Building containers"
cd deploy
docker-compose up -d --build

echo "üå± Seeding database"
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)"
docker-compose exec -T api python3 scripts/smoke_api.py
echo "Running science smoke test (advisory)..."
docker-compose -f deploy/docker-compose.yml exec -T api \
  python -m scripts.smoke_science || echo "[install] WARNING: science smoketest failed (likely empty DB or misconfigured pipeline) ‚Äì continuing."
# Optional second pass inside the API container; failures are treated as advisory.
docker-compose -f deploy/docker-compose.yml exec -T api \
  ${PYTHON_CMD} scripts/smoke_science.py || echo "[install] smoke_science (second pass) failed ‚Äì continuing as advisory."

echo "[install] Running frontend smoketest..."
${PYTHON_CMD} scripts/smoke_frontend.py || { echo "[install] Frontend smoketest failed"; exit 1; }
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests"
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."

# --- GO / NO-GO checks (v3.4.36 additions) ---
echo "[go-check] Running BN naming guard (non-fatal)..."
$PYTHON_CMD -m backend.science.bn_naming_guard || echo "[go-check] bn_naming_guard completed with warnings."

----- CONTENT END -----
----- FILE PATH: release.keep.yml
----- CONTENT START -----
critical_paths:
- backend
- backend/science
- backend/api
- frontend
- scripts
- tests
- install.sh
- v3_governance.yml
- governance.lock
min_counts:
  backend: 64
  frontend: 29
  scripts: 8
  tests: 5
stub_allowlist:
- backend/science/semantics/arch_parts_vlm.py
- backend/science/semantics/arch_patterns_vlm.py
- backend/services/vlm.py
ellipsis_allowlist:
- backend/science/contracts.py
----- CONTENT END -----
----- FILE PATH: science_tag_coverage_v1.json
----- CONTENT START -----
{
  "coverage": {
    "N/A (Computed)": {
      "analyzers": [],
      "key": "N/A (Computed)",
      "source_type": "stub_only",
      "stub": true
    },
    "affordance.isovist_area": {
      "analyzers": [
        "backend.science.spatial.depth.DepthAnalyzer"
      ],
      "key": "affordance.isovist_area",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "affordance.seating_count": {
      "analyzers": [
        "backend.science.vision.objects.ObjectAnalyzer"
      ],
      "key": "affordance.seating_count",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.axial_circulation_clear": {
      "analyzers": [],
      "key": "arch.pattern.axial_circulation_clear",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.bay_window": {
      "analyzers": [],
      "key": "arch.pattern.bay_window",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.central_hearth": {
      "analyzers": [],
      "key": "arch.pattern.central_hearth",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.circulation_maze_like": {
      "analyzers": [],
      "key": "arch.pattern.circulation_maze_like",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.colonnade": {
      "analyzers": [],
      "key": "arch.pattern.colonnade",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.corner_window": {
      "analyzers": [],
      "key": "arch.pattern.corner_window",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.daylight_hard": {
      "analyzers": [
        "backend.science.semantics.arch_patterns_vlm.ArchPatternsVLMAnalyzer"
      ],
      "key": "arch.pattern.daylight_hard",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.daylight_soft": {
      "analyzers": [
        "backend.science.semantics.arch_patterns_vlm.ArchPatternsVLMAnalyzer"
      ],
      "key": "arch.pattern.daylight_soft",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.double_height_space": {
      "analyzers": [
        "backend.science.semantics.arch_patterns_vlm.ArchPatternsVLMAnalyzer"
      ],
      "key": "arch.pattern.double_height_space",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.gallery_edge": {
      "analyzers": [],
      "key": "arch.pattern.gallery_edge",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.loft_mezzanine": {
      "analyzers": [],
      "key": "arch.pattern.loft_mezzanine",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.long_view_corridor": {
      "analyzers": [],
      "key": "arch.pattern.long_view_corridor",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.perimeter_seating": {
      "analyzers": [],
      "key": "arch.pattern.perimeter_seating",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.prospect_strong": {
      "analyzers": [
        "backend.science.semantics.arch_patterns_vlm.ArchPatternsVLMAnalyzer"
      ],
      "key": "arch.pattern.prospect_strong",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.refuge_nook": {
      "analyzers": [],
      "key": "arch.pattern.refuge_nook",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.refuge_strong": {
      "analyzers": [
        "backend.science.semantics.arch_patterns_vlm.ArchPatternsVLMAnalyzer"
      ],
      "key": "arch.pattern.refuge_strong",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "arch.pattern.skylight_dominant": {
      "analyzers": [],
      "key": "arch.pattern.skylight_dominant",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.staircase_sculptural": {
      "analyzers": [],
      "key": "arch.pattern.staircase_sculptural",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.threshold_emphasized": {
      "analyzers": [],
      "key": "arch.pattern.threshold_emphasized",
      "source_type": "stub_only",
      "stub": true
    },
    "arch.pattern.window_seat_niche": {
      "analyzers": [],
      "key": "arch.pattern.window_seat_niche",
      "source_type": "stub_only",
      "stub": true
    },
    "biophilia.plant_count": {
      "analyzers": [
        "backend.science.vision.objects.ObjectAnalyzer"
      ],
      "key": "biophilia.plant_count",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "cnfa.biophilic.natural_material_ratio": {
      "analyzers": [],
      "key": "cnfa.biophilic.natural_material_ratio",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.cognitive.activity_zones_count": {
      "analyzers": [],
      "key": "cnfa.cognitive.activity_zones_count",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.cognitive.landmark_salience": {
      "analyzers": [],
      "key": "cnfa.cognitive.landmark_salience",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.cognitive.legibility_score": {
      "analyzers": [],
      "key": "cnfa.cognitive.legibility_score",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.dynamic.optic_flow_magnitude": {
      "analyzers": [],
      "key": "cnfa.dynamic.optic_flow_magnitude",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.dynamic.path_glare_max": {
      "analyzers": [],
      "key": "cnfa.dynamic.path_glare_max",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.dynamic.reflection_flow": {
      "analyzers": [],
      "key": "cnfa.dynamic.reflection_flow",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.dynamic.revelation_rate": {
      "analyzers": [],
      "key": "cnfa.dynamic.revelation_rate",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.dynamic.texture_gradient": {
      "analyzers": [],
      "key": "cnfa.dynamic.texture_gradient",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.anomaly_count": {
      "analyzers": [],
      "key": "cnfa.fluency.anomaly_count",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.clutter_density_count": {
      "analyzers": [],
      "key": "cnfa.fluency.clutter_density_count",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.color_palette_entropy": {
      "analyzers": [],
      "key": "cnfa.fluency.color_palette_entropy",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.edge_clarity_mean": {
      "analyzers": [],
      "key": "cnfa.fluency.edge_clarity_mean",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.figure_ground_clarity": {
      "analyzers": [],
      "key": "cnfa.fluency.figure_ground_clarity",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.hierarchy_depth": {
      "analyzers": [],
      "key": "cnfa.fluency.hierarchy_depth",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.pattern_rhythm_regularity": {
      "analyzers": [],
      "key": "cnfa.fluency.pattern_rhythm_regularity",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.processing_load_proxy": {
      "analyzers": [],
      "key": "cnfa.fluency.processing_load_proxy",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.symmetry_score_horizontal": {
      "analyzers": [],
      "key": "cnfa.fluency.symmetry_score_horizontal",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.visual_entropy_spatial": {
      "analyzers": [],
      "key": "cnfa.fluency.visual_entropy_spatial",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fluency.zoning_clarity": {
      "analyzers": [],
      "key": "cnfa.fluency.zoning_clarity",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.fractal_dimension": {
      "analyzers": [],
      "key": "cnfa.fractal_dimension",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.haptic.soft_surface_ratio": {
      "analyzers": [],
      "key": "cnfa.haptic.soft_surface_ratio",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.haptic.texture_variation_index": {
      "analyzers": [],
      "key": "cnfa.haptic.texture_variation_index",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.light.brightness_variance": {
      "analyzers": [],
      "key": "cnfa.light.brightness_variance",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.light.diffuse_vs_direct_ratio": {
      "analyzers": [],
      "key": "cnfa.light.diffuse_vs_direct_ratio",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.light.vertical_illuminance_proxy": {
      "analyzers": [],
      "key": "cnfa.light.vertical_illuminance_proxy",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.light.warm_vs_cool_ratio": {
      "analyzers": [],
      "key": "cnfa.light.warm_vs_cool_ratio",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.spatial.ceiling_height_avg": {
      "analyzers": [],
      "key": "cnfa.spatial.ceiling_height_avg",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.spatial.enclosure_index": {
      "analyzers": [],
      "key": "cnfa.spatial.enclosure_index",
      "source_type": "stub_only",
      "stub": true
    },
    "cnfa.spatial.prospect_to_refuge_ratio": {
      "analyzers": [],
      "key": "cnfa.spatial.prospect_to_refuge_ratio",
      "source_type": "stub_only",
      "stub": true
    },
    "cognitive.error": {
      "analyzers": [
        "backend.science.context.cognitive.CognitiveStateAnalyzer"
      ],
      "key": "cognitive.error",
      "source_type": "vlm_cognitive",
      "stub": false
    },
    "color.lab_volume": {
      "analyzers": [
        "backend.science.math.color.ColorAnalyzer"
      ],
      "key": "color.lab_volume",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.lightness_contrast": {
      "analyzers": [
        "backend.science.math.color.ColorAnalyzer"
      ],
      "key": "color.lightness_contrast",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.luminance": {
      "analyzers": [
        "backend.science.vision.VisionProcessor"
      ],
      "key": "color.luminance",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.perceptual_lightness": {
      "analyzers": [
        "backend.science.math.color.ColorAnalyzer"
      ],
      "key": "color.perceptual_lightness",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.saturation": {
      "analyzers": [
        "backend.science.vision.VisionProcessor"
      ],
      "key": "color.saturation",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.warmth": {
      "analyzers": [
        "backend.science.vision.VisionProcessor"
      ],
      "key": "color.warmth",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "color.warmth_ratio": {
      "analyzers": [
        "backend.science.math.color.ColorAnalyzer"
      ],
      "key": "color.warmth_ratio",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "complexity.edge_density": {
      "analyzers": [
        "backend.science.math.complexity.ComplexityAnalyzer",
        "backend.science.vision.VisionProcessor"
      ],
      "key": "complexity.edge_density",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "complexity.shannon_entropy": {
      "analyzers": [
        "backend.science.math.complexity.ComplexityAnalyzer"
      ],
      "key": "complexity.shannon_entropy",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "complexity.spatial_entropy": {
      "analyzers": [
        "backend.science.math.complexity.ComplexityAnalyzer"
      ],
      "key": "complexity.spatial_entropy",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "component.bathroom.fixture.material.brass": {
      "analyzers": [],
      "key": "component.bathroom.fixture.material.brass",
      "source_type": "stub_only",
      "stub": true
    },
    "component.bathroom.fixture.material.matte_black": {
      "analyzers": [],
      "key": "component.bathroom.fixture.material.matte_black",
      "source_type": "stub_only",
      "stub": true
    },
    "component.bathroom.fixture.type.rain_showerhead": {
      "analyzers": [],
      "key": "component.bathroom.fixture.type.rain_showerhead",
      "source_type": "stub_only",
      "stub": true
    },
    "component.ceiling.coffered": {
      "analyzers": [],
      "key": "component.ceiling.coffered",
      "source_type": "stub_only",
      "stub": true
    },
    "component.ceiling.cove_lighting": {
      "analyzers": [],
      "key": "component.ceiling.cove_lighting",
      "source_type": "stub_only",
      "stub": true
    },
    "component.ceiling.exposed_beam": {
      "analyzers": [],
      "key": "component.ceiling.exposed_beam",
      "source_type": "stub_only",
      "stub": true
    },
    "component.ceiling.tray": {
      "analyzers": [],
      "key": "component.ceiling.tray",
      "source_type": "stub_only",
      "stub": true
    },
    "component.ceiling.vaulted": {
      "analyzers": [],
      "key": "component.ceiling.vaulted",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.material.brass": {
      "analyzers": [],
      "key": "component.kitchen.hardware.material.brass",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.material.chrome": {
      "analyzers": [],
      "key": "component.kitchen.hardware.material.chrome",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.material.matte_black": {
      "analyzers": [],
      "key": "component.kitchen.hardware.material.matte_black",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.type.bar_pull": {
      "analyzers": [],
      "key": "component.kitchen.hardware.type.bar_pull",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.type.handleless": {
      "analyzers": [],
      "key": "component.kitchen.hardware.type.handleless",
      "source_type": "stub_only",
      "stub": true
    },
    "component.kitchen.hardware.type.knob": {
      "analyzers": [],
      "key": "component.kitchen.hardware.type.knob",
      "source_type": "stub_only",
      "stub": true
    },
    "component.wall.material.brick_exposed": {
      "analyzers": [],
      "key": "component.wall.material.brick_exposed",
      "source_type": "stub_only",
      "stub": true
    },
    "component.wall.material.stone_wall": {
      "analyzers": [],
      "key": "component.wall.material.stone_wall",
      "source_type": "stub_only",
      "stub": true
    },
    "component.wall.material.wood_slat": {
      "analyzers": [],
      "key": "component.wall.material.wood_slat",
      "source_type": "stub_only",
      "stub": true
    },
    "component.wall.treatment.wainscoting": {
      "analyzers": [],
      "key": "component.wall.treatment.wainscoting",
      "source_type": "stub_only",
      "stub": true
    },
    "component.wall.treatment.wallpaper_patterned": {
      "analyzers": [],
      "key": "component.wall.treatment.wallpaper_patterned",
      "source_type": "stub_only",
      "stub": true
    },
    "fluency.symmetry": {
      "analyzers": [
        "backend.science.vision.VisionProcessor"
      ],
      "key": "fluency.symmetry",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "fractal.D": {
      "analyzers": [
        "backend.science.math.fractals.FractalAnalyzer"
      ],
      "key": "fractal.D",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "isovist.area_25d": {
      "analyzers": [
        "backend.science.spatial.isovist_25d.Isovist25DAnalyzer"
      ],
      "key": "isovist.area_25d",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "isovist.compactness_25d": {
      "analyzers": [
        "backend.science.spatial.isovist_25d.Isovist25DAnalyzer"
      ],
      "key": "isovist.compactness_25d",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "isovist.confidence": {
      "analyzers": [
        "backend.science.spatial.isovist_25d.Isovist25DAnalyzer"
      ],
      "key": "isovist.confidence",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "material.glass_coverage": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "material.glass_coverage",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "material.metal_coverage": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "material.metal_coverage",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "material.wood_coverage": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "material.wood_coverage",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.cues.brightness_mean": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.cues.brightness_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.cues.saturation_mean": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.cues.saturation_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.cues.specularity_proxy": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.cues.specularity_proxy",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.cues.texture_variance": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.cues.texture_variance",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.cues.value_mean": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.cues.value_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.substrate.plaster_gypsum": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.substrate.plaster_gypsum",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.substrate.stone_concrete": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.substrate.stone_concrete",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "materials.substrate.tile_ceramic": {
      "analyzers": [
        "backend.science.vision.materials.MaterialAnalyzer"
      ],
      "key": "materials.substrate.tile_ceramic",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "provenance.architect_kengo_kuma": {
      "analyzers": [],
      "key": "provenance.architect_kengo_kuma",
      "source_type": "stub_only",
      "stub": true
    },
    "provenance.architect_le_corbusier": {
      "analyzers": [],
      "key": "provenance.architect_le_corbusier",
      "source_type": "stub_only",
      "stub": true
    },
    "provenance.iconic_chair_barcelona": {
      "analyzers": [],
      "key": "provenance.iconic_chair_barcelona",
      "source_type": "stub_only",
      "stub": true
    },
    "provenance.iconic_chair_eames": {
      "analyzers": [],
      "key": "provenance.iconic_chair_eames",
      "source_type": "stub_only",
      "stub": true
    },
    "psych.coziness": {
      "analyzers": [
        "backend.science.perception.PerceptionProcessor"
      ],
      "key": "psych.coziness",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "science.organized_complexity": {
      "analyzers": [
        "backend.science.summary.ScienceSummaryAnalyzer"
      ],
      "key": "science.organized_complexity",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "science.organized_complexity_bin": {
      "analyzers": [
        "backend.science.summary.ScienceSummaryAnalyzer"
      ],
      "key": "science.organized_complexity_bin",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "science.visual_richness": {
      "analyzers": [
        "backend.science.summary.ScienceSummaryAnalyzer"
      ],
      "key": "science.visual_richness",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "science.visual_richness_bin": {
      "analyzers": [
        "backend.science.summary.ScienceSummaryAnalyzer"
      ],
      "key": "science.visual_richness_bin",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "social.occupancy": {
      "analyzers": [
        "backend.science.vision.objects.ObjectAnalyzer"
      ],
      "key": "social.occupancy",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "social.privacy": {
      "analyzers": [
        "backend.science.context.social.SocialDispositionAnalyzer"
      ],
      "key": "social.privacy",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "social.sociopetal": {
      "analyzers": [
        "backend.science.context.social.SocialDispositionAnalyzer"
      ],
      "key": "social.sociopetal",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial.central_openness": {
      "analyzers": [
        "backend.science.spatial.depth.DepthAnalyzer"
      ],
      "key": "spatial.central_openness",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial.isovist_openness": {
      "analyzers": [
        "backend.science.spatial.isovist.IsovistAnalyzer"
      ],
      "key": "spatial.isovist_openness",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial.refuge_quality": {
      "analyzers": [
        "backend.science.spatial.depth.DepthAnalyzer"
      ],
      "key": "spatial.refuge_quality",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial.room_function.bathroom": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "spatial.room_function.bathroom",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "spatial.room_function.bedroom": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "spatial.room_function.bedroom",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "spatial.room_function.home_office": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "spatial.room_function.home_office",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "spatial.room_function.kitchen": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "spatial.room_function.kitchen",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "spatial.room_function.living_room": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "spatial.room_function.living_room",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "spatial.visual_clutter": {
      "analyzers": [
        "backend.science.spatial.depth.DepthAnalyzer"
      ],
      "key": "spatial.visual_clutter",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq.high_power": {
      "analyzers": [
        "backend.science.math.spatial_frequency.SpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq.high_power",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq.low_high_ratio": {
      "analyzers": [
        "backend.science.math.spatial_frequency.SpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq.low_high_ratio",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq.low_power": {
      "analyzers": [
        "backend.science.math.spatial_frequency.SpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq.low_power",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq.mid_power": {
      "analyzers": [
        "backend.science.math.spatial_frequency.SpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq.mid_power",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.high_mean": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.high_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.high_var": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.high_var",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.low_mean": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.low_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.low_var": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.low_var",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.mid_mean": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.mid_mean",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "spatial_freq_reg.mid_var": {
      "analyzers": [
        "backend.science.math.regional_frequency.RegionalSpatialFrequencyAnalyzer"
      ],
      "key": "spatial_freq_reg.mid_var",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "style.bohemian": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.bohemian",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.farmhouse": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.farmhouse",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.industrial": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.industrial",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.japandi": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.japandi",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.minimalist": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.minimalist",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.modern": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.modern",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.modernity": {
      "analyzers": [
        "backend.science.perception.PerceptionProcessor"
      ],
      "key": "style.modernity",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "style.rustic": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.rustic",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.scandinavian": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.scandinavian",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "style.traditional": {
      "analyzers": [
        "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
      ],
      "key": "style.traditional",
      "source_type": "vlm_semantic",
      "stub": false
    },
    "texture.macro.contrast": {
      "analyzers": [
        "backend.science.math.glcm.TextureAnalyzer"
      ],
      "key": "texture.macro.contrast",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "texture.macro.homogeneity": {
      "analyzers": [
        "backend.science.math.glcm.TextureAnalyzer"
      ],
      "key": "texture.macro.homogeneity",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "texture.micro.contrast": {
      "analyzers": [
        "backend.science.math.glcm.TextureAnalyzer"
      ],
      "key": "texture.micro.contrast",
      "source_type": "math_or_deterministic",
      "stub": false
    },
    "texture.micro.homogeneity": {
      "analyzers": [
        "backend.science.math.glcm.TextureAnalyzer"
      ],
      "key": "texture.micro.homogeneity",
      "source_type": "math_or_deterministic",
      "stub": false
    }
  },
  "meta": {
    "computed_count": 76,
    "counts_by_source_type": {
      "math_or_deterministic": 61,
      "stub_only": 69,
      "stub_total": 69,
      "vlm_cognitive": 1,
      "vlm_semantic": 14
    },
    "registry_count": 145,
    "stub_count": 69,
    "version": 1
  }
}----- CONTENT END -----
----- FILE PATH: v3_governance.yml
----- CONTENT START -----
# v3_governance.yml
# THE IRON DOME: Active Protection Rules

policy_version: "3.0.0"

protected_scopes:
  - "backend/database"
  - "backend/science"
  - "backend/models"
  - "backend/api"
  - "backend/services"
  - "scripts"
  - "frontend/shared"
  - "frontend/apps"

critical_files:
  - "backend/main.py"
  - "backend/services/vlm.py"
  - "backend/models/config.py"
  - "backend/database/core.py"
  - "backend/models/annotation.py"
  - "backend/api/v1_annotation.py"
  - "backend/science/pipeline.py"
  - "deploy/docker-compose.yml"
  - "deploy/nginx.conf"
  - "deploy/Dockerfile.backend"
  - "scripts/guardian.py"
  - "v3_governance.yml"
  - "README_v3.md"
  - "install.sh"
  - "backend/api/v1_admin.py"
  - "backend/api/v1_supervision.py"
  - "backend/api/v1_discovery.py"
  - "backend/schemas/supervision.py"
  - "scripts/smoke_api.py"
  - "scripts/smoke_science.py"
  - "tests/test_v3_api.py"

constraints:
  min_file_size_bytes: 20
  prevent_new_root_files: true
  enforce_science_tag_coverage: true
  check_bn_db_health: false

# Note: Interpreter caches like __pycache__/ and *.pyc are not part of the governed surface
# and MUST NOT be included in release artifacts.
----- CONTENT END -----
----- FILE PATH: .github/workflows/auto_installer_smoke.yml
----- CONTENT START -----

name: smoke
on:
  push: { branches: ["*"] }
  pull_request: {}
jobs:
  smoke:
     runs-on: ubuntu-latest
     steps:
       - uses: actions/checkout@v4
       - uses: actions/setup-python@v5
         with: { python-version: "3.11" }
       - name: Install toolchain
         run: |
           python -m pip install --upgrade pip wheel
       - name: Turnkey installer (dry run)
         run: |
           bash infra/turnkey_installer_v1.3/installer/install.sh || true
       - name: Tests (optional)
         run: |
           if [ -d tests ]; then python -m pip install pytest || true; pytest -q || true; fi
----- CONTENT END -----
----- FILE PATH: .github/workflows/ci.yml
----- CONTENT START -----
name: Image Tagger v3 CI

on:
  push:
  pull_request:

jobs:
  backend-tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install fastapi uvicorn sqlalchemy pydantic psycopg2-binary pytest requests

      - name: Run Guardian verification
        run: |
          python scripts/guardian.py verify

      - name: Run API and Guardian tests
        run: |
          pytest -q tests/test_v3_api.py tests/test_guardian.py
----- CONTENT END -----
----- FILE PATH: .github/workflows/ci_v3.yml
----- CONTENT START -----
name: image-tagger-v3-ci

on:
  push:
    paths:
      - "backend/**"
      - "frontend/**"
      - "scripts/**"
      - "tests/**"
      - "deploy/**"
      - "v3_governance.yml"
  pull_request:
    paths:
      - "backend/**"
      - "frontend/**"
      - "scripts/**"
      - "tests/**"
      - "deploy/**"
      - "v3_governance.yml"

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install minimal dependencies
        run: |
          if [ -f backend/requirements.txt ]; then
            pip install -r backend/requirements.txt
          else:
            pip install fastapi uvicorn sqlalchemy alembic pytest requests pydantic opencv-python-headless numpy scipy scikit-image
          fi

      - name: Guardian verify
        run: |
          if [ -f scripts/guardian.py ]; then
            python scripts/guardian.py verify
          else:
            echo "Guardian script not found; skipping drift check."
          fi

      - name: Run API tests
        run: |
          if [ -f tests/test_v3_api.py ]; then
            pytest -q tests/test_v3_api.py
          else:
            echo "API tests not found; skipping."
          fi

      - name: Science smoketest (optional)
        run: |
          if [ -f scripts/smoke_science.py ]; then
            python scripts/smoke_science.py || echo "smoke_science failed (likely due to DB config) ‚Äì treat as advisory."
          else:
            echo "smoke_science not found; skipping."
          fi


- name: Guard against __pycache__ in tree
  run: python scripts/check_no_pycache_in_tree.py

----- CONTENT END -----
----- FILE PATH: ai/copilot_policy.json
----- CONTENT START -----
{
  "allow_shell": false,
  "allow_fs_write": true,
  "allow_network": false,
  "allowed_commands": [
    "python -m pip install -r requirements.txt",
    "python -m pip install --upgrade pip setuptools wheel",
    "pytest -q"
  ]
}----- CONTENT END -----
----- FILE PATH: ai/installer_copilot.py
----- CONTENT START -----

#!/usr/bin/env python3
"""Read installer logs, redact secrets, propose a remediation plan, and write ai_plan.json.
This reference copilot does not call an external LLM by default; it builds a rule-based plan.
Integrators may extend providers.py to enable live model calls behind policy gates.
"""
import argparse, json, os, pathlib, re, sys
from typing import Dict, Any
from providers import make_llm_client, ProviderConfig  # optional

DEFAULT_POLICY = {
  "allow_shell": False,
  "allow_fs_write": True,
  "allow_network": False,
  "allowed_commands": ["pip install -r requirements.txt", "pytest -q"],
}

def redact_tokens(text: str) -> str:
    patterns = [
        r'(?i)(api[_-]?key|token|secret|passwd|password)\s*[:=]\s*[^\s]+',
        r'sk-[A-Za-z0-9]{20,}',
        r'AKIA[0-9A-Z]{16}',
        r'(?i)authorization:\s*bearer\s+[A-Za-z0-9._-]+',
        r'(?i)(x-api-key|x-auth-token)\s*[:=]\s*[^\s]+',
        r'(?:\?|&)token=[^&\s]+'
    ]
    redacted = text
    for pat in patterns:
        redacted = re.sub(pat, '<REDACTED>', redacted)
    return redacted

def simple_rule_plan(log_text: str) -> Dict[str, Any]:
    plan = {"summary": "Auto-generated remediation plan", "steps": [], "notes": []}
    t = log_text.lower()
    if "module not found" in t or "no module named" in t:
        plan["steps"].append({"action":"shell","cmd":"python -m pip install -r requirements.txt"})
    if "permission denied" in t:
        plan["steps"].append({"action":"advice","text":"Check file permissions and mark scripts executable (chmod +x *.sh)"})
    if "address already in use" in t:
        plan["steps"].append({"action":"advice","text":"Free the port or set a different port via env"})
    if "failed building wheel" in t or "error: subprocess-exited-with-error" in t:
        plan["steps"].append({"action":"advice","text":"Upgrade pip/setuptools/wheel then retry"})
        plan["steps"].append({"action":"shell","cmd":"python -m pip install --upgrade pip setuptools wheel"})
    if not plan["steps"]:
        plan["notes"].append("No specific errors recognized; propose running tests to collect traces.")
        plan["steps"].append({"action":"shell","cmd":"pytest -q"})
    return plan

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--logfile', required=True)
    ap.add_argument('--out', default='logs/ai_plan.json')
    ap.add_argument('--dry-run', default='1')  # keep non-exec by default
    ap.add_argument('--provider', default='none')  # 'none' uses rule-based plan
    ap.add_argument('--policy', default=None)     # path to policy json
    args = ap.parse_args()

    root = pathlib.Path('.').resolve()
    log_text = pathlib.Path(args.logfile).read_text(encoding='utf-8', errors='ignore')
    log_text = redact_tokens(log_text)

    policy = DEFAULT_POLICY
    if args.policy and pathlib.Path(args.policy).exists():
        try:
            policy = json.loads(pathlib.Path(args.policy).read_text(encoding='utf-8'))
        except Exception:
            pass

    if args.provider == 'none':
        plan = simple_rule_plan(log_text)
    else:
        # Optional LLM call (must implement in providers.py and secure with policy)
        cfg = ProviderConfig.from_env(args.provider)
        client = make_llm_client(cfg)
        plan = client.propose_plan(log_text, policy)

    outp = pathlib.Path(args.out)
    outp.parent.mkdir(parents=True, exist_ok=True)
    outp.write_text(json.dumps({
        "policy": policy,
        "dry_run": (args.dry_run == '1'),
        "plan": plan
    }, indent=2), encoding='utf-8')
    print(f"[copilot] wrote plan -> {outp}")

if __name__ == '__main__':
    main()
----- CONTENT END -----
----- FILE PATH: ai/providers.py
----- CONTENT START -----

# Minimal provider shim. Extend to call your preferred LLM vendor securely.
from dataclasses import dataclass
from typing import Any, Dict

@dataclass
class ProviderConfig:
    name: str = "none"
    api_key: str = ""

    @staticmethod
    def from_env(name: str) -> "ProviderConfig":
        key = ""
        if name.lower() == "openai":
            key = ""  # read from env if you wire it
        return ProviderConfig(name=name, api_key=key)

class DummyClient:
    def __init__(self, cfg: ProviderConfig):
        self.cfg = cfg

    def propose_plan(self, redacted_log: str, policy: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "summary": f"Dummy plan via {self.cfg.name} (no external calls)",
            "steps": [{"action":"advice","text":"Wire a real provider in providers.py and set provider flag"}]
        }

def make_llm_client(cfg: ProviderConfig):
    return DummyClient(cfg)
----- CONTENT END -----
----- FILE PATH: ai/providers.sample.env
----- CONTENT START -----

# Sample provider env (do not commit secrets)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=...
# GOOGLE_API_KEY=...
----- CONTENT END -----
----- FILE PATH: ai/triage_schema.json
----- CONTENT START -----
{
  "type": "object",
  "properties": {
    "summary": {
      "type": "string"
    },
    "steps": {
      "type": "array",
      "items": {
        "type": "object"
      }
    },
    "notes": {
      "type": "array",
      "items": {
        "type": "string"
      }
    }
  },
  "required": [
    "summary",
    "steps"
  ]
}----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/VERSION
----- CONTENT START -----
3.4.18_vlm_patterns_materials_seed
----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/install.sh
----- CONTENT START -----
#!/bin/bash
VERSION="dev"
if [ -f VERSION ]; then
  VERSION=$(cat VERSION)
fi
echo "üöÄ Starting Image Tagger v3 (v${VERSION})"
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üß¨ Running structural hollow-repo guard"
python3 scripts/hollow_repo_guard.py || { echo "‚ùå Hollow Repo Guard failed"; exit 1; }

echo "üß™ Running program integrity guard"
python3 scripts/program_integrity_guard.py || { echo "‚ùå Program Integrity Guard failed"; exit 1; }\n
echo "üîí Running Guardian (governance) checks"
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi

# Security warning: detect default API_SECRET and warn user
DEFAULT_API_SECRET="dev_secret_key_change_me"
if [ "${API_SECRET:-$DEFAULT_API_SECRET}" = "$DEFAULT_API_SECRET" ]; then
    echo "‚ö†Ô∏è  WARNING: API_SECRET is using the default value 'dev_secret_key_change_me'."
    echo "    This is fine for local demos, but you MUST change it for any shared or deployed environment."
fi

echo "üê≥ Building containers"
cd deploy
docker-compose up -d --build

echo "üå± Seeding database"
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)"
docker-compose exec -T api python3 scripts/smoke_api.py
echo "Running science smoke test (advisory)..."
docker-compose -f deploy/docker-compose.yml exec -T api \
  python -m scripts.smoke_science || echo "[install] WARNING: science smoketest failed (likely empty DB or misconfigured pipeline) ‚Äì continuing."
# Optional second pass inside the API container; failures are treated as advisory.
docker-compose -f deploy/docker-compose.yml exec -T api \
  python3 scripts/smoke_science.py || echo "[install] smoke_science (second pass) failed ‚Äì continuing as advisory."

echo "[install] Running frontend smoketest..."
python3 scripts/smoke_frontend.py || { echo "[install] Frontend smoketest failed"; exit 1; }
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests"
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."
----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/backend/science/spatial/depth_plugin.py
----- CONTENT START -----
"""
Depth provider plugin interface.

This module defines a minimal protocol for plugging in a monocular depth
estimator without tying the science pipeline to a particular model.
"""

from __future__ import annotations

from typing import Protocol

import numpy as np
from backend.science.core import AnalysisFrame
from backend.science.contracts import fail


class DepthProvider(Protocol):
    name: str

    def infer_depth(self, image_rgb: np.ndarray) -> np.ndarray:


class DepthPluginAnalyzer:
    """
    Optional analyzer that calls a configured DepthProvider and stores
    the result in the AnalysisFrame. By default this is a no-op stub
    until a provider is configured.
    """

    name = "depth_plugin"
    tier = "L2"
    requires = ["original_image"]
    provides = ["depth_map"]

    # TODO: wire in a real provider via configuration.
    provider: DepthProvider | None = None

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        if cls.provider is None:
            fail(frame, cls.name, "no depth provider configured")
            return

        depth = cls.provider.infer_depth(frame.original_image)
        frame.depth_map = depth
----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/backend/science/vision/materials.py
----- CONTENT START -----
"""
Material analysis module for v3 Science Pipeline.

Ported heuristics from v2.6.3 MaterialClassifier into the v3
AnalysisFrame pattern. This module is deliberately lightweight and
does not depend on the v2 AttributeExtractor base class.
"""

from __future__ import annotations

from typing import Optional

import numpy as np
import cv2

from backend.science.core import AnalysisFrame
from backend.services.vlm import get_vlm_engine, StubEngine


class MaterialAnalyzer:
    """
    Heuristic-based material classification.

    Uses simple HSV and luminance/texture rules to estimate coverage
    of wood, metal, and glass in the scene. Values are normalized
    coverage ratios in [0, 1].
    """

    @staticmethod
    def analyze(frame: AnalysisFrame) -> None:
        img = frame.original_image
        if img is None:
            return

        # Convert to uint8 RGB for OpenCV operations
        if img.dtype == np.float32 or img.dtype == np.float64:
            image_uint8 = (img * 255).astype(np.uint8)
        else:
            image_uint8 = img

        # Convert to HSV for material heuristics
        hsv = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2HSV)

        # --- Wood heuristic (ported from v2 logic) ---
        # Brown-ish hues (approx 5‚Äì30 in OpenCV HSV),
        # with reasonable saturation and value.
        wood_mask = (
            (hsv[:, :, 0] >= 5)
            & (hsv[:, :, 0] <= 30)
            & (hsv[:, :, 1] > 30)
            & (hsv[:, :, 2] > 50)
        )
        wood_coverage = float(np.sum(wood_mask) / wood_mask.size)
        frame.add_attribute("material.wood_coverage", wood_coverage)

        # --- Metal heuristic (simplified from v2) ---
        # Low saturation, mid-to-high value ‚Üí shiny / metallic regions.
        metal_mask = (hsv[:, :, 1] < 30) & (hsv[:, :, 2] > 150)
        metal_coverage = float(np.sum(metal_mask) / metal_mask.size)
        frame.add_attribute("material.metal_coverage", metal_coverage)

        # --- Glass heuristic (ported from v2) ---
        # High luminance + low local variance ‚Üí smooth bright panes.
        gray = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2GRAY)
        bright_mask = gray > 200

        # Local variance proxy using a smoothing kernel
        kernel = np.ones((5, 5), np.float32) / 25.0
        local_mean = cv2.filter2D(gray.astype(float), -1, kernel)
        local_var = (gray.astype(float) - local_mean) ** 2
        smooth_mask = local_var < 100.0

        glass_mask = bright_mask & smooth_mask
        glass_coverage = float(np.sum(glass_mask) / glass_mask.size)
        frame.add_attribute("material.glass_coverage", glass_coverage)
        # --- L1 material cues (read-only; support higher tiers) ---
        # Normalized brightness (0-1) from grayscale.
        gray_float = gray.astype(float) / 255.0
        brightness_mean = float(gray_float.mean())
        frame.add_attribute("materials.cues.brightness_mean", brightness_mean, confidence=0.7)

        # Global texture variance (roughness proxy).
        texture_variance = float(gray_float.var())
        frame.add_attribute("materials.cues.texture_variance", min(texture_variance * 10.0, 1.0), confidence=0.6)

        # Mean saturation and value in HSV.
        sat_mean = float(hsv[:, :, 1].mean() / 255.0)
        val_mean = float(hsv[:, :, 2].mean() / 255.0)
        frame.add_attribute("materials.cues.saturation_mean", sat_mean, confidence=0.7)
        frame.add_attribute("materials.cues.value_mean", val_mean, confidence=0.7)

        # Specularity proxy: proportion of high-value, low-saturation pixels.
        spec_mask = (hsv[:, :, 1] < 40) & (hsv[:, :, 2] > 200)
        specularity_proxy = float(spec_mask.sum() / spec_mask.size)
        frame.add_attribute("materials.cues.specularity_proxy", specularity_proxy, confidence=0.6)

        # --- Substrate heuristics beyond wood/metal/glass ---
        # Stone/Concrete: low saturation, mid value, higher roughness.
        stone_mask = (
            (hsv[:, :, 1] < 60) &
            (hsv[:, :, 2] > 60) &
            (hsv[:, :, 2] < 200)
        )
        stone_coverage = float(stone_mask.sum() / stone_mask.size)
        frame.add_attribute("materials.substrate.stone_concrete", stone_coverage, confidence=0.5)

        # Plaster/Gypsum: very low saturation, high value, low variance.
        plaster_mask = (
            (hsv[:, :, 1] < 30) &
            (hsv[:, :, 2] > 180)
        )
        plaster_coverage = float(plaster_mask.sum() / plaster_mask.size)
        frame.add_attribute("materials.substrate.plaster_gypsum", plaster_coverage, confidence=0.5)

        # Tile/Ceramic: bright and moderately saturated with elevated local variance.
        # We reuse local_var from the glass heuristic as a crude texture cue.
        tile_mask = (
            (hsv[:, :, 2] > 150) &
            (hsv[:, :, 1] > 40) &
            (local_var > 50.0)
        )
        tile_coverage = float(tile_mask.sum() / tile_mask.size)
        frame.add_attribute("materials.substrate.tile_ceramic", tile_coverage, confidence=0.4)


def _maybe_run_materials_vlm(frame: AnalysisFrame, image_uint8: np.ndarray) -> None:
    """Optional VLM pass for materials.

    This function is intentionally conservative: it never writes numeric
    materials.* attributes. Instead, it records a structured candidate list
    under frame.metadata["materials.vlm_candidates"] when a real VLM is configured.

    When running under StubEngine, we record only a note so downstream science
    knows that no VLM data were present.
    """
    try:
        ok, buffer = cv2.imencode(".jpg", image_uint8)
        if not ok:
            return
        image_bytes = buffer.tobytes()
        engine = get_vlm_engine()

        prompt = (
            "You are an architectural materials analyst. "
            "Given this interior image, estimate the presence of the following materials substrates, "
            "each from 0.0 to 1.0, and provide a brief evidence string. "
            "Substrates:
"
            "- materials.substrate.stone_concrete
"
            "- materials.substrate.plaster_gypsum
"
            "- materials.substrate.tile_ceramic
"
            "Return STRICT JSON as a list of objects with fields "
            "{'key': <substrate_key>, 'present': <0-1>, 'confidence': <0-1>, 'evidence': <short text>}."
        )
        result = engine.analyze_image(image_bytes, prompt)
    except Exception:
        # We silently skip VLM errors for materials; core CV cues remain available.
        return

    # Stub / classroom path.
    if isinstance(engine, StubEngine) or (isinstance(result, dict) and result.get("stub")):
        frame.metadata["materials.vlm_candidates"] = {
            "note": "VLM in stub mode; no materials.* VLM candidates.",
            "engine": type(engine).__name__,
        }
        return

    candidates = []
    if isinstance(result, list):
        candidates = result
    elif isinstance(result, dict) and "materials" in result and isinstance(result["materials"], list):
        candidates = result["materials"]

    frame.metadata["materials.vlm_candidates"] = {
        "engine": type(engine).__name__,
        "candidates": candidates,
    }
----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/scripts/science_harness.py
----- CONTENT START -----
"""Standalone science-only harness.

Usage (folder mode):

  python -m scripts.science_harness --input-dir path/to/images --output science_attributes.csv

This does not require a running database; it loads images directly from a folder,
runs the science analyzers, and writes a CSV with one row per image.
"""

from __future__ import annotations

import argparse
import csv
from pathlib import Path

from backend.services.vlm import describe_vlm_configuration
from typing import List

import numpy as np

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer


def iter_images(input_dir: Path) -> List[Path]:
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}

files = []
    for p in sorted(input_dir.rglob("*")):
        if p.is_file() and p.suffix.lower() in exts:
            files.append(p)
    return files


def main() -> None:
    parser = argparse.ArgumentParser(description="Science-only harness for Image Tagger v3.3")
    parser.add_argument("--input-dir", type=str, required=True, help="Folder of images to analyze")
    parser.add_argument("--output", type=str, default="science_attributes.csv", help="Output CSV path")
    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        raise SystemExit(f"Input directory does not exist: {input_dir}")

    if cv2 is None:
        raise SystemExit("OpenCV (cv2) is required to run the science harness.")

    images = iter_images(input_dir)
    if not images:
        raise SystemExit(f"No images found in {input_dir}")

    color = ColorAnalyzer()
    comp = ComplexityAnalyzer()
    tex = TextureAnalyzer()
    frac = FractalAnalyzer()
    depth = DepthAnalyzer()
    cognitive = CognitiveStateAnalyzer()  # neutral baseline

    rows = []
    all_keys = set()

    for idx, path in enumerate(images):
        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            continue
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

        frame = AnalysisFrame(image_id=idx, original_image=rgb)

        color.analyze(frame)
        comp.analyze(frame)
        tex.analyze(frame)
        frac.analyze(frame)
        depth.analyze(frame)
        cognitive.analyze(frame)

        row = {"filename": path.name}
        row.update(frame.attributes)
        rows.append(row)
        all_keys.update(frame.attributes.keys())

    all_keys = sorted(all_keys)
    fieldnames = ["filename"] + all_keys

    out_path = Path(args.output)
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)

    print(f"Wrote {len(rows)} rows to {out_path}")


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: archive/sprint_s0s1s2_pre/tests/test_governance_integrity.py
----- CONTENT START -----
"""
Regression tests for governance guard scripts.

These are intentionally light-weight: they just assert that the guard
scripts run without raising SystemExit under the current tree.
"""

from __future__ import annotations

import runpy
from pathlib import Path


ROOT = Path(__file__).resolve().parents[1]


def test_hollow_repo_guard_runs():
    runpy.run_path(str(ROOT / "scripts" / "hollow_repo_guard.py"))


def test_program_integrity_guard_runs():
    runpy.run_path(str(ROOT / "scripts" / "program_integrity_guard.py"))
----- CONTENT END -----
----- FILE PATH: archive/step10_admin_training_export_ui/CHANGELOG_step10_admin_training_export_ui.txt
----- CONTENT START -----
Image Tagger v3.2.10 ‚Äì Step 10: Admin Training Export UI

- Extended frontend/apps/admin/src/App.jsx to surface the training export capability:
  * Added state: exportIds (comma-separated image IDs), exportBusy, exportMessage.
  * Implemented handleTrainingExport(), which:
    - Parses exportIds into a list of integers.
    - Calls POST /v1/admin/training/export with payload { image_ids, format: "json" }.
    - Downloads the returned list[TrainingExample] as training_export.json via Blob.
  * Added a "Training Export" card under the right-hand column (below Cost Overview),
    with a textarea for IDs and a Download JSON button.
- Extended lucide-react imports to include the Download icon for this card.----- CONTENT END -----
----- FILE PATH: archive/step11_api_smoketests/CHANGELOG_step11_api_smoketests.txt
----- CONTENT START -----
Image Tagger v3.2.11 ‚Äì Step 11: API Smoketests (FastAPI)

- Replaced the stubbed tests/test_v3_api.py (which contained ellipses) with a
  concrete FastAPI smoketest suite:
    * test_health_check(): asserts /health returns {status: healthy, version: 3.0.0}.
    * test_root_descriptor(): asserts / advertises docs and workbench_api.
    * test_admin_models_smoke(): GET /v1/admin/models with admin headers
      should return a JSON list (will fail if router or RBAC wiring is broken).
    * test_admin_training_export_empty_list_ok(): POST /v1/admin/training/export
      with empty image_ids should succeed and return a JSON list.
    * test_explorer_export_empty_list_ok(): POST /v1/explorer/export with empty
      image_ids and tagger headers should succeed and return a JSON list.
    * test_404_handling_json(): asserts that /v1/non_existent returns a JSON
      404 payload with {detail: "Not Found"}.
- These tests are intentionally minimal but will catch missing router mounts,
  broken RBAC decorators, or schema mismatches in the training/export paths.----- CONTENT END -----
----- FILE PATH: archive/step12_ux_help_layer/CHANGELOG_step12_ux_help_layer.txt
----- CONTENT START -----
Image Tagger v3.2.12 ‚Äì Step 12: UX Help Layer (Explorer + Workbench)

- Explorer (frontend/apps/explorer/src/App.jsx):
  * Extended lucide-react imports to include HelpCircle.
  * Added a "How to use Explorer" quick-help banner directly under the header. It explains:
    - Start with a query or browse images.
    - Use Filters to narrow by attributes/tags/source.
    - Click thumbnails to inspect tags/attributes.
    - Use the checkbox on each card to add images to the export cart.
    - Click "Export Dataset" to download a JSON file for training/analysis.
- Workbench (frontend/apps/workbench/src/App.jsx):
  * Extended lucide-react imports to include HelpCircle.
  * Added a slim quick-help strip under the header that reminds taggers:
    - One binary question at a time.
    - Keyboard shortcuts: 1 = NO, 2 = YES.
    - They can use mouse buttons instead, and Retry if the task looks wrong.
- These UX hints are intentionally lightweight, visible by default, and designed to make
  the attribute tagging and export workflows more idiot-proof for new users.----- CONTENT END -----
----- FILE PATH: archive/step13_router_wiring/CHANGELOG_step13_router_wiring.txt
----- CONTENT START -----
Image Tagger v3.2.13 ‚Äì Step 13: Router Wiring for Smoketests

- Rewrote backend/main.py to explicitly mount all v1 routers:
    * v1_annotation.router   -> /v1/workbench/...
    * v1_admin.router        -> /v1/admin/...
    * v1_supervision.router  -> /v1/monitor/...
    * v1_discovery.router    -> /v1/explorer/...
- Kept FastAPI app metadata stable (title, description, version, docs URLs).
- Preserved /health and root (/) endpoints so that:
    * /health returns {status: "healthy", version: "3.0.0"}.
    * / returns a JSON descriptor with "docs" and "workbench_api" keys.
- This makes the smoketests in tests/test_v3_api.py meaningful in a real environment:
    * GET /v1/admin/models
    * POST /v1/admin/training/export
    * POST /v1/explorer/export
    * and the basic health/root checks.----- CONTENT END -----
----- FILE PATH: archive/step14_guardian_install_tests/CHANGELOG_step14_guardian_install_tests.txt
----- CONTENT START -----
Image Tagger v3.2.14 ‚Äì Step 14: Guardian + Install-Time Tests

- Hardened install.sh to integrate governance and tests:
  * Guardian:
    - If governance.lock exists, run `python3 scripts/guardian.py verify` and abort
      the install if verification fails.
    - If governance.lock does not exist, run `python3 scripts/guardian.py freeze`
      to create the initial baseline, aborting on failure.
    - If python3 is not available, print a warning and skip Guardian.
  * Tests:
    - After docker-compose brings up the stack, seeding, and existing smoke_api /
      smoke_science checks, run:
          docker-compose exec -T api pytest -q tests/test_v3_api.py
      and abort if the pytest smoketests fail.
- This makes install.sh behave like a mini CI gate: a release that passes Guardian
  + smoke tests + pytest is much closer to "enterprise-ready".----- CONTENT END -----
----- FILE PATH: archive/step6_monitor/CHANGELOG_step6_monitor_supervision.txt
----- CONTENT START -----
Image Tagger v3.2.6 ‚Äì Step 6: Supervisor Monitor wired to real data

- Implemented backend/api/v1_supervision.py with:
  * /v1/monitor/velocity ‚Üí TaggerPerformance[]
  * /v1/monitor/irr ‚Üí IRRStat[]
- Updated frontend/apps/monitor/src/App.jsx to call the real API:
  * Uses ApiClient('/api/v1/monitor')
  * Renders team velocity table and IRR table from live data
- Ensured backend/main.py includes v1_supervision.router.----- CONTENT END -----
----- FILE PATH: archive/step7_tag_inspector/CHANGELOG_step7_tag_inspector.txt
----- CONTENT START -----
Image Tagger v3.2.7 ‚Äì Step 7: Tag Inspector (Supervisor Deep Dive)

- Extended backend/schemas/supervision.py with ValidationDetail.
- Extended backend/api/v1_supervision.py with:
  * GET /v1/monitor/image/{image_id}/validations ‚Üí ValidationDetail[] for that image.
- Updated frontend/apps/monitor/src/App.jsx:
  * IRR table now has an Inspect action that opens a Tag Inspector drawer.
  * Tag Inspector shows per-user validations for the selected image, including attribute, value, dwell time, and timestamp.----- CONTENT END -----
----- FILE PATH: archive/step8_admin_cockpit/CHANGELOG_step8_admin_cockpit.txt
----- CONTENT START -----
Image Tagger v3.2.8 ‚Äì Step 8: Admin Cockpit wired to real ToolConfig & budget endpoints

- Rewrote backend/api/v1_admin.py with concrete endpoints:
  * GET /v1/admin/models ‚Üí list[ToolConfigRead] from tool_configs table.
  * PATCH /v1/admin/models/{tool_id} ‚Üí update is_enabled / cost_per_1k_tokens.
  * GET /v1/admin/budget ‚Üí BudgetStatus with derived kill-switch state.
  * POST /v1/admin/kill-switch?active=bool ‚Üí disables all paid models and returns BudgetStatus.
- Rewrote frontend/apps/admin/src/App.jsx to:
  * Load models and budget from the real API using ApiClient('/api/v1/admin').
  * Toggle models on/off via PATCH requests.
  * Edit cost_per_1k_tokens via number input + PATCH on blur.
  * Activate/deactivate the kill switch by calling POST /v1/admin/kill-switch.----- CONTENT END -----
----- FILE PATH: archive/step9_training_export/CHANGELOG_step9_training_export.txt
----- CONTENT START -----
Image Tagger v3.2.9 ‚Äì Step 9: Training Export / Active Learning wiring

- Added backend/services/training_export.py with TrainingExporter:
  * export_for_images(db, image_ids) joins Validation with Image to produce
    JSON-ready training examples (image_id, filename, attribute_key, value,
    user_id, region_id, duration_ms, created_at, source).
- Added backend/schemas/training.py with TrainingExample Pydantic model.
- Replaced the stubbed /v1/explorer/export implementation in
  backend/api/v1_discovery.py with a real exporter-backed endpoint:
  * POST /v1/explorer/export (via router prefix) now returns
    list[TrainingExample] instead of a fake JSON blob.
- Added an admin alias endpoint:
  * POST /v1/admin/training/export uses the same TrainingExporter but is
    protected by require_admin and can be extended later with richer
    filters (quality thresholds, time windows).----- CONTENT END -----
----- FILE PATH: archive/v3.4.18_pre_s0s1_2025-11-24_s0s1/install.sh
----- CONTENT START -----
#!/bin/bash
VERSION="dev"
if [ -f VERSION ]; then
  VERSION=$(cat VERSION)
fi
echo "üöÄ Starting Image Tagger v3 (v${VERSION})"
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üß¨ Running structural hollow-repo guard"
python3 scripts/hollow_repo_guard.py || { echo "‚ùå Hollow Repo Guard failed"; exit 1; }

echo "üß™ Running program integrity guard"
python3 scripts/program_integrity_guard.py || { echo "‚ùå Program Integrity Guard failed"; exit 1; }\n
echo "üîí Running Guardian (governance) checks"
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi

# Security warning: detect default API_SECRET and warn user
DEFAULT_API_SECRET="dev_secret_key_change_me"
if [ "${API_SECRET:-$DEFAULT_API_SECRET}" = "$DEFAULT_API_SECRET" ]; then
    echo "‚ö†Ô∏è  WARNING: API_SECRET is using the default value 'dev_secret_key_change_me'."
    echo "    This is fine for local demos, but you MUST change it for any shared or deployed environment."
fi

echo "üê≥ Building containers"
cd deploy
docker-compose up -d --build

echo "üå± Seeding database"
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)"
docker-compose exec -T api python3 scripts/smoke_api.py
echo "Running science smoke test (advisory)..."
docker-compose -f deploy/docker-compose.yml exec -T api \
  python -m scripts.smoke_science || echo "[install] WARNING: science smoketest failed (likely empty DB or misconfigured pipeline) ‚Äì continuing."
# Optional second pass inside the API container; failures are treated as advisory.
docker-compose -f deploy/docker-compose.yml exec -T api \
  python3 scripts/smoke_science.py || echo "[install] smoke_science (second pass) failed ‚Äì continuing as advisory."

echo "[install] Running frontend smoketest..."
python3 scripts/smoke_frontend.py || { echo "[install] Frontend smoketest failed"; exit 1; }
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests"
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."
----- CONTENT END -----
----- FILE PATH: archive/v3.4.18_pre_s0s1_2025-11-24_s0s1/backend/science/spatial/depth_plugin.py
----- CONTENT START -----
"""
Depth provider plugin interface.

This module defines a minimal protocol for plugging in a monocular depth
estimator without tying the science pipeline to a particular model.
"""

from __future__ import annotations

from typing import Protocol

import numpy as np
from backend.science.core import AnalysisFrame
from backend.science.contracts import fail


class DepthProvider(Protocol):
    name: str

    def infer_depth(self, image_rgb: np.ndarray) -> np.ndarray:


class DepthPluginAnalyzer:
    """
    Optional analyzer that calls a configured DepthProvider and stores
    the result in the AnalysisFrame. By default this is a no-op stub
    until a provider is configured.
    """

    name = "depth_plugin"
    tier = "L2"
    requires = ["original_image"]
    provides = ["depth_map"]

    # TODO: wire in a real provider via configuration.
    provider: DepthProvider | None = None

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        if cls.provider is None:
            fail(frame, cls.name, "no depth provider configured")
            return

        depth = cls.provider.infer_depth(frame.original_image)
        frame.depth_map = depth
----- CONTENT END -----
----- FILE PATH: archive/v3.4.18_pre_s0s1_2025-11-24_s0s1/backend/science/vision/materials.py
----- CONTENT START -----
"""
Material analysis module for v3 Science Pipeline.

Ported heuristics from v2.6.3 MaterialClassifier into the v3
AnalysisFrame pattern. This module is deliberately lightweight and
does not depend on the v2 AttributeExtractor base class.
"""

from __future__ import annotations

from typing import Optional

import numpy as np
import cv2

from backend.science.core import AnalysisFrame
from backend.services.vlm import get_vlm_engine, StubEngine


class MaterialAnalyzer:
    """
    Heuristic-based material classification.

    Uses simple HSV and luminance/texture rules to estimate coverage
    of wood, metal, and glass in the scene. Values are normalized
    coverage ratios in [0, 1].
    """

    @staticmethod
    def analyze(frame: AnalysisFrame) -> None:
        img = frame.original_image
        if img is None:
            return

        # Convert to uint8 RGB for OpenCV operations
        if img.dtype == np.float32 or img.dtype == np.float64:
            image_uint8 = (img * 255).astype(np.uint8)
        else:
            image_uint8 = img

        # Convert to HSV for material heuristics
        hsv = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2HSV)

        # --- Wood heuristic (ported from v2 logic) ---
        # Brown-ish hues (approx 5‚Äì30 in OpenCV HSV),
        # with reasonable saturation and value.
        wood_mask = (
            (hsv[:, :, 0] >= 5)
            & (hsv[:, :, 0] <= 30)
            & (hsv[:, :, 1] > 30)
            & (hsv[:, :, 2] > 50)
        )
        wood_coverage = float(np.sum(wood_mask) / wood_mask.size)
        frame.add_attribute("material.wood_coverage", wood_coverage)

        # --- Metal heuristic (simplified from v2) ---
        # Low saturation, mid-to-high value ‚Üí shiny / metallic regions.
        metal_mask = (hsv[:, :, 1] < 30) & (hsv[:, :, 2] > 150)
        metal_coverage = float(np.sum(metal_mask) / metal_mask.size)
        frame.add_attribute("material.metal_coverage", metal_coverage)

        # --- Glass heuristic (ported from v2) ---
        # High luminance + low local variance ‚Üí smooth bright panes.
        gray = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2GRAY)
        bright_mask = gray > 200

        # Local variance proxy using a smoothing kernel
        kernel = np.ones((5, 5), np.float32) / 25.0
        local_mean = cv2.filter2D(gray.astype(float), -1, kernel)
        local_var = (gray.astype(float) - local_mean) ** 2
        smooth_mask = local_var < 100.0

        glass_mask = bright_mask & smooth_mask
        glass_coverage = float(np.sum(glass_mask) / glass_mask.size)
        frame.add_attribute("material.glass_coverage", glass_coverage)
        # --- L1 material cues (read-only; support higher tiers) ---
        # Normalized brightness (0-1) from grayscale.
        gray_float = gray.astype(float) / 255.0
        brightness_mean = float(gray_float.mean())
        frame.add_attribute("materials.cues.brightness_mean", brightness_mean, confidence=0.7)

        # Global texture variance (roughness proxy).
        texture_variance = float(gray_float.var())
        frame.add_attribute("materials.cues.texture_variance", min(texture_variance * 10.0, 1.0), confidence=0.6)

        # Mean saturation and value in HSV.
        sat_mean = float(hsv[:, :, 1].mean() / 255.0)
        val_mean = float(hsv[:, :, 2].mean() / 255.0)
        frame.add_attribute("materials.cues.saturation_mean", sat_mean, confidence=0.7)
        frame.add_attribute("materials.cues.value_mean", val_mean, confidence=0.7)

        # Specularity proxy: proportion of high-value, low-saturation pixels.
        spec_mask = (hsv[:, :, 1] < 40) & (hsv[:, :, 2] > 200)
        specularity_proxy = float(spec_mask.sum() / spec_mask.size)
        frame.add_attribute("materials.cues.specularity_proxy", specularity_proxy, confidence=0.6)

        # --- Substrate heuristics beyond wood/metal/glass ---
        # Stone/Concrete: low saturation, mid value, higher roughness.
        stone_mask = (
            (hsv[:, :, 1] < 60) &
            (hsv[:, :, 2] > 60) &
            (hsv[:, :, 2] < 200)
        )
        stone_coverage = float(stone_mask.sum() / stone_mask.size)
        frame.add_attribute("materials.substrate.stone_concrete", stone_coverage, confidence=0.5)

        # Plaster/Gypsum: very low saturation, high value, low variance.
        plaster_mask = (
            (hsv[:, :, 1] < 30) &
            (hsv[:, :, 2] > 180)
        )
        plaster_coverage = float(plaster_mask.sum() / plaster_mask.size)
        frame.add_attribute("materials.substrate.plaster_gypsum", plaster_coverage, confidence=0.5)

        # Tile/Ceramic: bright and moderately saturated with elevated local variance.
        # We reuse local_var from the glass heuristic as a crude texture cue.
        tile_mask = (
            (hsv[:, :, 2] > 150) &
            (hsv[:, :, 1] > 40) &
            (local_var > 50.0)
        )
        tile_coverage = float(tile_mask.sum() / tile_mask.size)
        frame.add_attribute("materials.substrate.tile_ceramic", tile_coverage, confidence=0.4)


def _maybe_run_materials_vlm(frame: AnalysisFrame, image_uint8: np.ndarray) -> None:
    """Optional VLM pass for materials.

    This function is intentionally conservative: it never writes numeric
    materials.* attributes. Instead, it records a structured candidate list
    under frame.metadata["materials.vlm_candidates"] when a real VLM is configured.

    When running under StubEngine, we record only a note so downstream science
    knows that no VLM data were present.
    """
    try:
        ok, buffer = cv2.imencode(".jpg", image_uint8)
        if not ok:
            return
        image_bytes = buffer.tobytes()
        engine = get_vlm_engine()

        prompt = (
            "You are an architectural materials analyst. "
            "Given this interior image, estimate the presence of the following materials substrates, "
            "each from 0.0 to 1.0, and provide a brief evidence string. "
            "Substrates:
"
            "- materials.substrate.stone_concrete
"
            "- materials.substrate.plaster_gypsum
"
            "- materials.substrate.tile_ceramic
"
            "Return STRICT JSON as a list of objects with fields "
            "{'key': <substrate_key>, 'present': <0-1>, 'confidence': <0-1>, 'evidence': <short text>}."
        )
        result = engine.analyze_image(image_bytes, prompt)
    except Exception:
        # We silently skip VLM errors for materials; core CV cues remain available.
        return

    # Stub / classroom path.
    if isinstance(engine, StubEngine) or (isinstance(result, dict) and result.get("stub")):
        frame.metadata["materials.vlm_candidates"] = {
            "note": "VLM in stub mode; no materials.* VLM candidates.",
            "engine": type(engine).__name__,
        }
        return

    candidates = []
    if isinstance(result, list):
        candidates = result
    elif isinstance(result, dict) and "materials" in result and isinstance(result["materials"], list):
        candidates = result["materials"]

    frame.metadata["materials.vlm_candidates"] = {
        "engine": type(engine).__name__,
        "candidates": candidates,
    }
----- CONTENT END -----
----- FILE PATH: archive/v3.4.18_pre_s0s1_2025-11-24_s0s1/scripts/science_harness.py
----- CONTENT START -----
"""Standalone science-only harness.

Usage (folder mode):

  python -m scripts.science_harness --input-dir path/to/images --output science_attributes.csv

This does not require a running database; it loads images directly from a folder,
runs the science analyzers, and writes a CSV with one row per image.
"""

from __future__ import annotations

import argparse
import csv
from pathlib import Path

from backend.services.vlm import describe_vlm_configuration
from typing import List

import numpy as np

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer


def iter_images(input_dir: Path) -> List[Path]:
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}

files = []
    for p in sorted(input_dir.rglob("*")):
        if p.is_file() and p.suffix.lower() in exts:
            files.append(p)
    return files


def main() -> None:
    parser = argparse.ArgumentParser(description="Science-only harness for Image Tagger v3.3")
    parser.add_argument("--input-dir", type=str, required=True, help="Folder of images to analyze")
    parser.add_argument("--output", type=str, default="science_attributes.csv", help="Output CSV path")
    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        raise SystemExit(f"Input directory does not exist: {input_dir}")

    if cv2 is None:
        raise SystemExit("OpenCV (cv2) is required to run the science harness.")

    images = iter_images(input_dir)
    if not images:
        raise SystemExit(f"No images found in {input_dir}")

    color = ColorAnalyzer()
    comp = ComplexityAnalyzer()
    tex = TextureAnalyzer()
    frac = FractalAnalyzer()
    depth = DepthAnalyzer()
    cognitive = CognitiveStateAnalyzer()  # neutral baseline

    rows = []
    all_keys = set()

    for idx, path in enumerate(images):
        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            continue
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

        frame = AnalysisFrame(image_id=idx, original_image=rgb)

        color.analyze(frame)
        comp.analyze(frame)
        tex.analyze(frame)
        frac.analyze(frame)
        depth.analyze(frame)
        cognitive.analyze(frame)

        row = {"filename": path.name}
        row.update(frame.attributes)
        rows.append(row)
        all_keys.update(frame.attributes.keys())

    all_keys = sorted(all_keys)
    fieldnames = ["filename"] + all_keys

    out_path = Path(args.output)
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)

    print(f"Wrote {len(rows)} rows to {out_path}")


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: archive/v3.4.18_pre_s0s1_2025-11-24_s0s1/tests/test_governance_integrity.py
----- CONTENT START -----
"""
Regression tests for governance guard scripts.

These are intentionally light-weight: they just assert that the guard
scripts run without raising SystemExit under the current tree.
"""

from __future__ import annotations

import runpy
from pathlib import Path


ROOT = Path(__file__).resolve().parents[1]


def test_hollow_repo_guard_runs():
    runpy.run_path(str(ROOT / "scripts" / "hollow_repo_guard.py"))


def test_program_integrity_guard_runs():
    runpy.run_path(str(ROOT / "scripts" / "program_integrity_guard.py"))
----- CONTENT END -----
----- FILE PATH: archive/v3.4.20_pre_merge_3.4.21_2025-11-23/VERSION
----- CONTENT START -----
3.4.20_s0s1_governance_hardening_full
----- CONTENT END -----
----- FILE PATH: archive/v3.4.20_pre_merge_3.4.21_2025-11-23/install.sh
----- CONTENT START -----
#!/bin/bash
VERSION="dev"
if [ -f VERSION ]; then
  VERSION=$(cat VERSION)
fi
echo "üöÄ Starting Image Tagger v3 (v${VERSION})"
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üß¨ Running structural hollow-repo guard"
python3 scripts/hollow_repo_guard.py || { echo "‚ùå Hollow Repo Guard failed"; exit 1; }

echo "üß™ Running program integrity guard"
python3 scripts/program_integrity_guard.py || { echo "‚ùå Program Integrity Guard failed"; exit 1; }
echo "üßØ Running syntax guard"

python3 scripts/syntax_guard.py || { echo "‚ùå Syntax Guard failed"; exit 1; }

echo "ü™ù Running critical import guard"

python3 scripts/critical_import_guard.py || { echo "‚ùå Critical Import Guard failed"; exit 1; }

echo "üîí Running Guardian (governance) checks"
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi

# Security warning: detect default API_SECRET and warn user
DEFAULT_API_SECRET="dev_secret_key_change_me"
if [ "${API_SECRET:-$DEFAULT_API_SECRET}" = "$DEFAULT_API_SECRET" ]; then
    echo "‚ö†Ô∏è  WARNING: API_SECRET is using the default value 'dev_secret_key_change_me'."
    echo "    This is fine for local demos, but you MUST change it for any shared or deployed environment."
fi

echo "üê≥ Building containers"
cd deploy
docker-compose up -d --build

echo "üå± Seeding database"
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)"
docker-compose exec -T api python3 scripts/smoke_api.py
echo "Running science smoke test (advisory)..."
docker-compose -f deploy/docker-compose.yml exec -T api \
  python -m scripts.smoke_science || echo "[install] WARNING: science smoketest failed (likely empty DB or misconfigured pipeline) ‚Äì continuing."
# Optional second pass inside the API container; failures are treated as advisory.
docker-compose -f deploy/docker-compose.yml exec -T api \
  python3 scripts/smoke_science.py || echo "[install] smoke_science (second pass) failed ‚Äì continuing as advisory."

echo "[install] Running frontend smoketest..."
python3 scripts/smoke_frontend.py || { echo "[install] Frontend smoketest failed"; exit 1; }
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests"
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."
----- CONTENT END -----
----- FILE PATH: archive/v3.4.20_pre_merge_3.4.21_2025-11-23/backend/api/v1_debug.py
----- CONTENT START -----
from __future__ import annotations

import io
import os
from pathlib import Path
from typing import Optional

import numpy as np

try:
    import cv2  # type: ignore
except Exception:  # pragma: no cover - cv2 may not be available in tiny CI images
    cv2 = None  # type: ignore

from fastapi import APIRouter, Depends, HTTPException, Response, status
from sqlalchemy.orm import Session

from backend.database.core import get_db
from backend.models.assets import Image  # type: ignore
from backend.services.auth import CurrentUser, require_tagger

router = APIRouter(prefix="/v1/debug", tags=["Debug / Science"])

def _resolve_image_path(storage_path: str) -> Path:
    """Resolve the on-disk path for a stored image.

    The storage_path column is expected to contain either an absolute
    path or a path relative to the working directory / IMAGE_STORAGE_ROOT.
    We first try the path as-is; if it does not exist and is relative,
    we fall back to IMAGE_STORAGE_ROOT + storage_path.
    """
    raw = Path(storage_path)
    if raw.is_file():
        return raw

    # Try prefixing with IMAGE_STORAGE_ROOT if provided
    root = os.getenv("IMAGE_STORAGE_ROOT")
    if root:
        candidate = Path(root) / storage_path
        if candidate.is_file():
            return candidate

    return raw  # Best-effort; caller will handle missing file

def _compute_edge_map_bytes(path: Path, t1: int = 50, t2: int = 150, l2: bool = True) -> bytes:
    """Compute a Canny edge map PNG for the given image path.

    This mirrors the logic in backend.science.core.AnalysisFrame.compute_derived,
    but is implemented locally to keep the debug endpoint self-contained.

    To keep things efficient in classroom settings, we maintain a tiny on-disk
    cache keyed by (image, thresholds, L2 flag). If a matching PNG already
    exists, we serve it directly instead of recomputing.
    """
    if cv2 is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="cv2 (OpenCV) is not available; cannot compute edge maps.",
        )

    # Compute cache path
    cache_root = os.getenv("IMAGE_DEBUG_CACHE_ROOT") or os.path.join("backend", "data", "debug_edges")
    cache_root_path = Path(cache_root)
    cache_root_path.mkdir(parents=True, exist_ok=True)

    cache_name = f"{path.stem}_edges_{t1}_{t2}_{1 if l2 else 0}.png"
    cache_path = cache_root_path / cache_name

    if cache_path.is_file():
        try:
            return cache_path.read_bytes()
        except Exception:
            # Fall through to recomputation on any read error
            pass

    img_bgr = cv2.imread(str(path))
    if img_bgr is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Could not read image from storage: {path}",
        )

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    # Allow experimentation with thresholds and the L2gradient flag
    edges = cv2.Canny(gray, t1, t2, L2gradient=l2)

    ok, buf = cv2.imencode(".png", edges)
    if not ok:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to encode edge map as PNG.",
        )

    data = buf.tobytes()
    try:
        cache_path.write_bytes(data)
    except Exception:
        # Cache write failure should not break the endpoint
        pass
    return data


@router.get("/images/{image_id}/edges", summary="Return edge-map debug view for an image")
def get_image_edge_map(
    image_id: int,
    t1: int = 50,
    t2: int = 150,
    l2: bool = True,
    db: Session = Depends(get_db),
    user: CurrentUser = Depends(require_tagger),
) -> Response:
    """Serve a PNG edge map for the requested image.

    This endpoint is intended purely for *debug / teaching* purposes. It
    allows Explorer (and other tools) to show "what the algorithm sees"
    when computing complexity and related metrics.
    """
    image: Optional[Image] = db.query(Image).filter(Image.id == image_id).first()
    if image is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Image not found")

    storage_path = getattr(image, "storage_path", None)
    if not storage_path:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Image has no storage_path configured",
        )

    path = _resolve_image_path(storage_path)
    if not path.is_file():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Image file not found on disk: {path}",
        )

    data = _compute_edge_map_bytes(path, t1=t1, t2=t2, l2=l2)
    return Response(content=data, media_type="image/png")
----- CONTENT END -----
----- FILE PATH: archive/v3.4.20_pre_merge_3.4.21_2025-11-23/tests/test_governance_integrity.py
----- CONTENT START -----
"""Regression tests for governance guard scripts.

These tests assert that all guard scripts *execute* successfully
under the current repo tree.
"""

from __future__ import annotations

import runpy
from pathlib import Path


ROOT = Path(__file__).resolve().parents[1]


def _run_guard(script_name: str):
    ns = runpy.run_path(str(ROOT / "scripts" / script_name))
    if "main" in ns:
        ns["main"]()


def test_hollow_repo_guard_runs():
    _run_guard("hollow_repo_guard.py")


def test_program_integrity_guard_runs():
    _run_guard("program_integrity_guard.py")


def test_syntax_guard_runs():
    _run_guard("syntax_guard.py")


def test_critical_import_guard_runs():
    _run_guard("critical_import_guard.py")
----- CONTENT END -----
----- FILE PATH: archive/v3.4.21_pre_merge_3.4.22_canon_guard/VERSION
----- CONTENT START -----
3.4.21_canonical_integrated_full
----- CONTENT END -----
----- FILE PATH: archive/v3.4.21_pre_merge_3.4.22_canon_guard/scripts/canon_guard.py
----- CONTENT START -----
"""Canonical feature registry guard.

Mirrors the logic of tests/test_feature_registry_coverage.py but is
packaged as a standalone script suitable for CI/install-time checks.

A feature key is allowed to exist in the registry if and only if it is:
- computed somewhere in backend/science via frame.add_attribute, or
- explicitly listed in backend.science.feature_stubs.STUB_FEATURE_KEYS.
"""

from __future__ import annotations

import json
import re
import sys
from pathlib import Path

from backend.science import feature_stubs

ROOT = Path(__file__).resolve().parents[1]


def _load_registry_keys() -> set[str]:
    registry_path = ROOT / "backend" / "science" / "feature_registry.json"
    data = json.loads(registry_path.read_text(encoding="utf-8"))
    return set(data.keys())


def _load_computed_keys() -> set[str]:
    base = ROOT / "backend" / "science"
    keys: set[str] = set()
    for path in base.rglob("*.py"):
        text = path.read_text(encoding="utf-8", errors="ignore")
        for m in re.finditer(r'add_attribute\("([^"]+)"', text):
            keys.add(m.group(1))
    return keys


def main() -> None:
    registry_keys = _load_registry_keys()
    computed_keys = _load_computed_keys()
    stub_keys = feature_stubs.STUB_FEATURE_KEYS

    dangling = registry_keys - computed_keys - stub_keys
    if dangling:
        print("[canon_guard] Dangling registry keys with no compute or stub:", file=sys.stderr)
        for key in sorted(dangling)[:20]:
            print(f"  - {key}", file=sys.stderr)
        raise SystemExit(1)

    print("[canon_guard] OK: registry keys covered by compute or stubs.")

if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/install.sh
----- CONTENT START -----
#!/bin/bash
echo "üöÄ Starting Image Tagger v3.1..."
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üîí Running Guardian (governance) checks..."
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi




echo "üê≥ Building Containers..."
cd deploy
docker-compose up -d --build

echo "üå± Seeding Database..."
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)..."
docker-compose exec -T api python3 scripts/smoke_api.py
docker-compose exec -T api python3 scripts/smoke_science.py
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests..."
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."
----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/backend/api/v1_discovery.py
----- CONTENT START -----
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
import json
import io
from fastapi.responses import StreamingResponse

from backend.database.core import get_db
from backend.services.auth import require_tagger\nfrom backend.services.training_export import TrainingExporter\nfrom backend.schemas.training import TrainingExample\n
from backend.services.query_builder import QueryBuilder
from backend.schemas.discovery import SearchQuery, ImageSearchResult, ExportRequest, AttributeRead
from backend.models.attribute import Attribute

router = APIRouter(prefix="/v1/explorer", tags=["Research Explorer"])

@router.post("/search", response_model=List[ImageSearchResult])
async def search_images(
    query: SearchQuery,
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
):
    """
    High-level search endpoint for the Research Explorer.

    This currently delegates to QueryBuilder, which can be progressively
    enhanced without changing the API contract.
    """
    qb = QueryBuilder(db=db, user_id=user.id)
    results = qb.search_images(query)
    return results


@router.post("/export", response_model=list[TrainingExample])
async def export_dataset(
    request: ExportRequest,
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
) -> list[TrainingExample]:
    """
    Export a slice of the validated dataset for fine-tuning / active learning.

    This is the Explorer-facing endpoint. It uses TrainingExporter to pull
    all Validation rows for the selected image_ids and returns them as a
    JSON list of TrainingExample records that the frontend can download.
    """
    exporter = TrainingExporter(db=db)
    examples = exporter.export_for_images(request.image_ids)
    return [TrainingExample(**e) for e in examples]


@router.get("/attributes", response_model=List[AttributeRead])
async def list_attributes(
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
    category: str | None = None,
    limit: int = 500,
):
    """
    Return rows from the Attribute registry.

    This is the v3 counterpart of the v2 Feature Explorer's underlying
    taxonomy, now backed by the SQL model seeded from attributes.yml.
    """
    q = db.query(Attribute).filter(Attribute.is_active.is_(True))
    if category:
        q = q.filter(Attribute.category == category)
    q = q.order_by(Attribute.key).limit(limit)
    return q.all()----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/backend/schemas/annotation.py
----- CONTENT START -----
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional, Dict, Any
from backend.schemas.common import TimestampSchema

# --- INPUT SCHEMAS (Frontend -> Backend) ---

class ValidationRequest(BaseModel):
    """
    Payload sent when a Tagger presses 'Confirm' or uses a keyboard shortcut.
    """
    image_id: int
    attribute_key: str = Field(..., description="e.g. 'spatial.prospect'")
    value: float = Field(..., ge=0.0, le=1.0, description="Normalized value 0-1")
    duration_ms: int = Field(..., ge=0, description="Time spent looking at image (velocity tracking)")
    
class RegionCreateRequest(BaseModel):
    """
    Payload sent when a Tagger draws a box or polygon.
    """
    image_id: int
    geometry: Dict[str, Any] = Field(..., description="GeoJSON or {x,y,w,h}")
    manual_label: str = Field(..., description="Human assigned class")

# --- OUTPUT SCHEMAS (Backend -> Frontend) ---

class ImageWorkItem(TimestampSchema):
    """
    The 'Next Image' payload. Optimized for the Workbench Canvas.
    """
    id: int
    filename: str
    # Pre-signed URL or local path served via Nginx
    url: str 
    # Context for the tagger (e.g. "Is this Modern?")
    meta_data: Dict[str, Any] = {} 

class ValidationResponse(TimestampSchema):
    id: int
    status: str = "success"
    agreement_score: Optional[float] = None # Calculated async if other validators exist----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/backend/science/vision/objects.py
----- CONTENT START -----
import numpy as np
from backend.science.core import AnalysisFrame
import logging

# Lazy load ultralytics to keep startup fast if not used
YOLO_MODEL = None

logger = logging.getLogger("v3.science.objects")

class ObjectAnalyzer:
    """
    Wraps YOLOv8 for counting architectural elements (chairs, people, plants).
    Essential for 'Social Affordance' metrics.
    """
    
    @staticmethod
    def load_model():
        global YOLO_MODEL
        if YOLO_MODEL is None:
            from ultralytics import YOLO
            # Downloads 'yolov8n.pt' (nano) automatically on first run (~6MB)
            # Use 'yolov8x.pt' for production accuracy
            logger.info("Loading YOLOv8 model...")
            YOLO_MODEL = YOLO("yolov8n.pt")
            
    @staticmethod
    def analyze(frame: AnalysisFrame):
        ObjectAnalyzer.load_model()
        
        # Run Inference
        results = YOLO_MODEL(frame.original_image, verbose=False)
        
        # Count classes
        counts = {}
        for result in results:
            for box in result.boxes:
                cls_id = int(box.cls[0])
                label = YOLO_MODEL.names[cls_id]
                counts[label] = counts.get(label, 0) + 1
                
        # Mapping to CNfA attributes
        # 1. Seating (Social)
        seating_count = counts.get('chair', 0) + counts.get('couch', 0) + counts.get('bench', 0)
        frame.add_attribute("affordance.seating_count", seating_count)
        
        # 2. Biophilia (Plants)
        plant_count = counts.get('potted plant', 0)
        frame.add_attribute("biophilia.plant_count", plant_count)
        
        # 3. Occupancy
        person_count = counts.get('person', 0)
        frame.add_attribute("social.occupancy", person_count)----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/backend/scripts/seed_attributes.py
----- CONTENT START -----
"""
Seed script for attribute taxonomy.

Reads contracts/attributes.yml (v2.6.3-compatible format) and inserts
Attribute rows into the v3 database if they are missing.

Intended usage (from repo root):

    python -m backend.scripts.seed_attributes

In Docker (install.sh):

    docker-compose exec -T api python backend/scripts/seed_attributes.py
"""
from __future__ import annotations

import sys
from pathlib import Path
from typing import List, Dict, Any

import yaml
from sqlalchemy.orm import Session

from backend.database.core import SessionLocal, engine
from backend.models import Base, Attribute


REPO_ROOT = Path(__file__).resolve().parents[2]
ATTRIBUTES_YML = REPO_ROOT / "contracts" / "attributes.yml"


def parse_attributes(lines: List[str]) -> List[Dict[str, Any]]:
    """
    Parse the slightly non-standard attributes.yml shipped in v2.6.3.

    The file has a header:

        schema:
          - id: string
          ...
        attributes:
          - id: geometry.curvilinearity
            name: ...

    followed by a long list of '- id:' blocks. We do a lightweight
    streaming parse that:

    - ignores the 'schema' section
    - starts collecting once we see 'attributes:'
    - treats any line starting with '- id:' as a new record
    - collects subsequent 'key: value' lines into the same record
    """
    entries: List[Dict[str, Any]] = []
    in_attrs = False
    current: Dict[str, Any] | None = None

    for raw in lines:
        line = raw.rstrip("\n")
        stripped = line.strip()

        if not in_attrs:
            if stripped.startswith("attributes:"):
                in_attrs = True
            continue

        if not stripped or stripped.startswith("#"):
            continue

        if stripped.startswith("- id:"):
            if current:
                entries.append(current)
            current = {}
            _, _, val = stripped.partition(":")
            current["id"] = val.strip()
            continue

        if current is not None and ":" in stripped:
            key, _, val = stripped.partition(":")
            key = key.strip()
            val = val.strip()
            current[key] = val

    if current:
        entries.append(current)

    return entries


def seed() -> None:
    if not ATTRIBUTES_YML.exists():
        print(f"[seed_attributes] attributes.yml not found at {ATTRIBUTES_YML}")
        return

    lines = ATTRIBUTES_YML.read_text(encoding="utf-8").splitlines()
    raw_entries = parse_attributes(lines)
    print(f"[seed_attributes] Parsed {len(raw_entries)} attribute entries from YAML.")

    # Ensure tables exist
    Base.metadata.create_all(bind=engine)

    db: Session = SessionLocal()
    created = 0
    try:
        for raw in raw_entries:
            key = raw.get("id")
            if not key:
                continue
            existing = db.query(Attribute).filter_by(key=key).first()
            if existing:
                continue

            attr = Attribute(
                key=key,
                name=raw.get("name", key),
                category=None,  # Could be inferred later from canonical tree
                level=raw.get("level"),
                range=raw.get("range"),
                sources=raw.get("sources"),
                notes=raw.get("notes"),
                is_active=True,
                source_version="v2.6.3",
            )
            db.add(attr)
            created += 1

        db.commit()
    finally:
        db.close()

    print(f"[seed_attributes] Created {created} new Attribute rows.")


if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/backend/scripts/seed_tool_configs.py
----- CONTENT START -----
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy.orm import Session
from backend.database.core import SessionLocal, engine
from backend.models.config import ToolConfig, Base

def seed():
    # Ensure tables exist
    Base.metadata.create_all(bind=engine)
    db = SessionLocal()
    
    print("üå± Seeding Tool Configurations...")
    
    tools = [
        # Segmentation Models
        {"name": "sam_vit_b", "type": "segmentation", "provider": "local", "cost_per_image": 0.0001, "settings": {"speed": "fast"}},
        {"name": "sam_vit_l", "type": "segmentation", "provider": "local", "cost_per_image": 0.0003, "settings": {"speed": "medium"}},
        {"name": "sam_vit_h", "type": "segmentation", "provider": "local", "cost_per_image": 0.0008, "settings": {"speed": "slow"}},
        
        # VLM Labeling Models
        {"name": "gemini-1.5-flash", "type": "labeling", "provider": "google", "cost_per_1k_tokens": 0.0001, "settings": {"context": "1m"}},
        {"name": "gpt-4-vision", "type": "labeling", "provider": "openai", "cost_per_1k_tokens": 0.01, "settings": {"context": "128k"}},
        {"name": "claude-3-sonnet", "type": "labeling", "provider": "anthropic", "cost_per_1k_tokens": 0.003, "settings": {"context": "200k"}},
        {"name": "deepseek-vl", "type": "labeling", "provider": "local", "cost_per_1k_tokens": 0.0, "settings": {"quantization": "4bit"}},
    ]

    for tool in tools:
        exists = db.query(ToolConfig).filter_by(name=tool["name"]).first()
        if not exists:
            t = ToolConfig(
                name=tool["name"],
                tool_type=tool["type"],
                provider=tool["provider"],
                cost_per_image=tool.get("cost_per_image", 0.0),
                cost_per_1k_tokens=tool.get("cost_per_1k_tokens", 0.0),
                settings=tool["settings"],
                is_enabled=True
            )
            db.add(t)
            print(f"  + Added {tool['name']}")
        else:
            print(f"  . Skipped {tool['name']} (Exists)")
            
    db.commit()
    db.close()
    print("‚úÖ Seeding Complete.")

if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: archive/v3_2_14_phase0/tests/test_v3_api.py
----- CONTENT START -----
import pytest
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)


def _admin_headers():
    """Minimal admin identity for RBAC-protected endpoints."""
    return {
        "X-User-Id": "1",
        "X-User-Role": "admin",
    }


def _tagger_headers():
    """Minimal tagger identity for workbench / explorer endpoints."""
    return {
        "X-User-Id": "10",
        "X-User-Role": "tagger",
    }


def test_health_check():
    """Verify the system is alive and versioned correctly."""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data.get("status") == "healthy"
    assert data.get("version") == "3.0.0"


def test_root_descriptor():
    """Root endpoint should advertise docs and workbench API entry point."""
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "docs" in data
    assert "workbench_api" in data


def test_admin_models_smoke():
    """Admin models endpoint should respond with a JSON list.

    This will fail if the Admin router is not mounted or if RBAC wiring is
    broken, which is the intended signal for CI.
    """
    response = client.get("/v1/admin/models", headers=_admin_headers())
    assert response.status_code == 200
    models = response.json()
    assert isinstance(models, list)


def test_admin_training_export_empty_list_ok():
    """Admin training export should accept an empty image_ids list.

    This exercise the TrainingExporter path without requiring any DB rows.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/admin/training/export",
        headers=_admin_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_explorer_export_empty_list_ok():
    """Explorer export should be wired and RBAC-protected for taggers.

    Like the admin export, this uses an empty image_ids list to avoid
    depending on seeded data while still verifying router + schema wiring.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/explorer/export",
        headers=_tagger_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_404_handling_json():
    """Invalid routes should return JSON 404, not HTML."""
    response = client.get("/v1/non_existent")
    assert response.status_code == 404
    body = response.json()
    assert isinstance(body, dict)
    assert body.get("detail") == "Not Found"


if __name__ == "__main__":
    print("Running basic API smoketests...")
    test_health_check()
    test_root_descriptor()
    print("Selected smoketests: PASS (if no assertion errors shown)")----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/install.sh
----- CONTENT START -----
#!/bin/bash
echo "üöÄ Starting Image Tagger v3.1..."
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üîí Running Guardian (governance) checks..."
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi




echo "üê≥ Building Containers..."
cd deploy
docker-compose up -d --build

echo "üå± Seeding Database..."
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)..."
docker-compose exec -T api python3 scripts/smoke_api.py
docker-compose exec -T api python3 scripts/smoke_science.py
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests..."
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/backend/api/v1_discovery.py
----- CONTENT START -----
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
import json
import io
from fastapi.responses import StreamingResponse

from backend.database.core import get_db
from backend.services.auth import require_tagger\nfrom backend.services.training_export import TrainingExporter\nfrom backend.schemas.training import TrainingExample\n
from backend.services.query_builder import QueryBuilder
from backend.schemas.discovery import SearchQuery, ImageSearchResult, ExportRequest, AttributeRead
from backend.models.attribute import Attribute

router = APIRouter(prefix="/v1/explorer", tags=["Research Explorer"])

@router.post("/search", response_model=List[ImageSearchResult])
async def search_images(
    query: SearchQuery,
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
):
    """
    High-level search endpoint for the Research Explorer.

    This currently delegates to QueryBuilder, which can be progressively
    enhanced without changing the API contract.
    """
    qb = QueryBuilder(db=db, user_id=user.id)
    results = qb.search_images(query)
    return results


@router.post("/export", response_model=list[TrainingExample])
async def export_dataset(
    request: ExportRequest,
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
) -> list[TrainingExample]:
    """
    Export a slice of the validated dataset for fine-tuning / active learning.

    This is the Explorer-facing endpoint. It uses TrainingExporter to pull
    all Validation rows for the selected image_ids and returns them as a
    JSON list of TrainingExample records that the frontend can download.
    """
    exporter = TrainingExporter(db=db)
    examples = exporter.export_for_images(request.image_ids)
    return [TrainingExample(**e) for e in examples]


@router.get("/attributes", response_model=List[AttributeRead])
async def list_attributes(
    db: Session = Depends(get_db),
    user = Depends(require_tagger),
    category: str | None = None,
    limit: int = 500,
):
    """
    Return rows from the Attribute registry.

    This is the v3 counterpart of the v2 Feature Explorer's underlying
    taxonomy, now backed by the SQL model seeded from attributes.yml.
    """
    q = db.query(Attribute).filter(Attribute.is_active.is_(True))
    if category:
        q = q.filter(Attribute.category == category)
    q = q.order_by(Attribute.key).limit(limit)
    return q.all()----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/backend/schemas/annotation.py
----- CONTENT START -----
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional, Dict, Any
from backend.schemas.common import TimestampSchema

# --- INPUT SCHEMAS (Frontend -> Backend) ---

class ValidationRequest(BaseModel):
    """
    Payload sent when a Tagger presses 'Confirm' or uses a keyboard shortcut.
    """
    image_id: int
    attribute_key: str = Field(..., description="e.g. 'spatial.prospect'")
    value: float = Field(..., ge=0.0, le=1.0, description="Normalized value 0-1")
    duration_ms: int = Field(..., ge=0, description="Time spent looking at image (velocity tracking)")
    
class RegionCreateRequest(BaseModel):
    """
    Payload sent when a Tagger draws a box or polygon.
    """
    image_id: int
    geometry: Dict[str, Any] = Field(..., description="GeoJSON or {x,y,w,h}")
    manual_label: str = Field(..., description="Human assigned class")

# --- OUTPUT SCHEMAS (Backend -> Frontend) ---

class ImageWorkItem(TimestampSchema):
    """
    The 'Next Image' payload. Optimized for the Workbench Canvas.
    """
    id: int
    filename: str
    # Pre-signed URL or local path served via Nginx
    url: str 
    # Context for the tagger (e.g. "Is this Modern?")
    meta_data: Dict[str, Any] = {} 

class ValidationResponse(TimestampSchema):
    id: int
    status: str = "success"
    agreement_score: Optional[float] = None # Calculated async if other validators exist----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/backend/science/vision/objects.py
----- CONTENT START -----
import numpy as np
from backend.science.core import AnalysisFrame
import logging

# Lazy load ultralytics to keep startup fast if not used
YOLO_MODEL = None

logger = logging.getLogger("v3.science.objects")

class ObjectAnalyzer:
    """
    Wraps YOLOv8 for counting architectural elements (chairs, people, plants).
    Essential for 'Social Affordance' metrics.
    """
    
    @staticmethod
    def load_model():
        global YOLO_MODEL
        if YOLO_MODEL is None:
            from ultralytics import YOLO
            # Downloads 'yolov8n.pt' (nano) automatically on first run (~6MB)
            # Use 'yolov8x.pt' for production accuracy
            logger.info("Loading YOLOv8 model...")
            YOLO_MODEL = YOLO("yolov8n.pt")
            
    @staticmethod
    def analyze(frame: AnalysisFrame):
        ObjectAnalyzer.load_model()
        
        # Run Inference
        results = YOLO_MODEL(frame.original_image, verbose=False)
        
        # Count classes
        counts = {}
        for result in results:
            for box in result.boxes:
                cls_id = int(box.cls[0])
                label = YOLO_MODEL.names[cls_id]
                counts[label] = counts.get(label, 0) + 1
                
        # Mapping to CNfA attributes
        # 1. Seating (Social)
        seating_count = counts.get('chair', 0) + counts.get('couch', 0) + counts.get('bench', 0)
        frame.add_attribute("affordance.seating_count", seating_count)
        
        # 2. Biophilia (Plants)
        plant_count = counts.get('potted plant', 0)
        frame.add_attribute("biophilia.plant_count", plant_count)
        
        # 3. Occupancy
        person_count = counts.get('person', 0)
        frame.add_attribute("social.occupancy", person_count)----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/backend/scripts/seed_attributes.py
----- CONTENT START -----
"""
Seed script for attribute taxonomy.

Reads contracts/attributes.yml (v2.6.3-compatible format) and inserts
Attribute rows into the v3 database if they are missing.

Intended usage (from repo root):

    python -m backend.scripts.seed_attributes

In Docker (install.sh):

    docker-compose exec -T api python backend/scripts/seed_attributes.py
"""
from __future__ import annotations

import sys
from pathlib import Path
from typing import List, Dict, Any

import yaml
from sqlalchemy.orm import Session

from backend.database.core import SessionLocal, engine
from backend.models import Base, Attribute


REPO_ROOT = Path(__file__).resolve().parents[2]
ATTRIBUTES_YML = REPO_ROOT / "contracts" / "attributes.yml"


def parse_attributes(lines: List[str]) -> List[Dict[str, Any]]:
    """
    Parse the slightly non-standard attributes.yml shipped in v2.6.3.

    The file has a header:

        schema:
          - id: string
          ...
        attributes:
          - id: geometry.curvilinearity
            name: ...

    followed by a long list of '- id:' blocks. We do a lightweight
    streaming parse that:

    - ignores the 'schema' section
    - starts collecting once we see 'attributes:'
    - treats any line starting with '- id:' as a new record
    - collects subsequent 'key: value' lines into the same record
    """
    entries: List[Dict[str, Any]] = []
    in_attrs = False
    current: Dict[str, Any] | None = None

    for raw in lines:
        line = raw.rstrip("\n")
        stripped = line.strip()

        if not in_attrs:
            if stripped.startswith("attributes:"):
                in_attrs = True
            continue

        if not stripped or stripped.startswith("#"):
            continue

        if stripped.startswith("- id:"):
            if current:
                entries.append(current)
            current = {}
            _, _, val = stripped.partition(":")
            current["id"] = val.strip()
            continue

        if current is not None and ":" in stripped:
            key, _, val = stripped.partition(":")
            key = key.strip()
            val = val.strip()
            current[key] = val

    if current:
        entries.append(current)

    return entries


def seed() -> None:
    if not ATTRIBUTES_YML.exists():
        print(f"[seed_attributes] attributes.yml not found at {ATTRIBUTES_YML}")
        return

    lines = ATTRIBUTES_YML.read_text(encoding="utf-8").splitlines()
    raw_entries = parse_attributes(lines)
    print(f"[seed_attributes] Parsed {len(raw_entries)} attribute entries from YAML.")

    # Ensure tables exist
    Base.metadata.create_all(bind=engine)

    db: Session = SessionLocal()
    created = 0
    try:
        for raw in raw_entries:
            key = raw.get("id")
            if not key:
                continue
            existing = db.query(Attribute).filter_by(key=key).first()
            if existing:
                continue

            attr = Attribute(
                key=key,
                name=raw.get("name", key),
                category=None,  # Could be inferred later from canonical tree
                level=raw.get("level"),
                range=raw.get("range"),
                sources=raw.get("sources"),
                notes=raw.get("notes"),
                is_active=True,
                source_version="v2.6.3",
            )
            db.add(attr)
            created += 1

        db.commit()
    finally:
        db.close()

    print(f"[seed_attributes] Created {created} new Attribute rows.")


if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/backend/scripts/seed_tool_configs.py
----- CONTENT START -----
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy.orm import Session
from backend.database.core import SessionLocal, engine
from backend.models.config import ToolConfig, Base

def seed():
    # Ensure tables exist
    Base.metadata.create_all(bind=engine)
    db = SessionLocal()
    
    print("üå± Seeding Tool Configurations...")
    
    tools = [
        # Segmentation Models
        {"name": "sam_vit_b", "type": "segmentation", "provider": "local", "cost_per_image": 0.0001, "settings": {"speed": "fast"}},
        {"name": "sam_vit_l", "type": "segmentation", "provider": "local", "cost_per_image": 0.0003, "settings": {"speed": "medium"}},
        {"name": "sam_vit_h", "type": "segmentation", "provider": "local", "cost_per_image": 0.0008, "settings": {"speed": "slow"}},
        
        # VLM Labeling Models
        {"name": "gemini-1.5-flash", "type": "labeling", "provider": "google", "cost_per_1k_tokens": 0.0001, "settings": {"context": "1m"}},
        {"name": "gpt-4-vision", "type": "labeling", "provider": "openai", "cost_per_1k_tokens": 0.01, "settings": {"context": "128k"}},
        {"name": "claude-3-sonnet", "type": "labeling", "provider": "anthropic", "cost_per_1k_tokens": 0.003, "settings": {"context": "200k"}},
        {"name": "deepseek-vl", "type": "labeling", "provider": "local", "cost_per_1k_tokens": 0.0, "settings": {"quantization": "4bit"}},
    ]

    for tool in tools:
        exists = db.query(ToolConfig).filter_by(name=tool["name"]).first()
        if not exists:
            t = ToolConfig(
                name=tool["name"],
                tool_type=tool["type"],
                provider=tool["provider"],
                cost_per_image=tool.get("cost_per_image", 0.0),
                cost_per_1k_tokens=tool.get("cost_per_1k_tokens", 0.0),
                settings=tool["settings"],
                is_enabled=True
            )
            db.add(t)
            print(f"  + Added {tool['name']}")
        else:
            print(f"  . Skipped {tool['name']} (Exists)")
            
    db.commit()
    db.close()
    print("‚úÖ Seeding Complete.")

if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase0_rebuild/tests/test_v3_api.py
----- CONTENT START -----
import pytest
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)


def _admin_headers():
    """Minimal admin identity for RBAC-protected endpoints."""
    return {
        "X-User-Id": "1",
        "X-User-Role": "admin",
    }


def _tagger_headers():
    """Minimal tagger identity for workbench / explorer endpoints."""
    return {
        "X-User-Id": "10",
        "X-User-Role": "tagger",
    }


def test_health_check():
    """Verify the system is alive and versioned correctly."""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data.get("status") == "healthy"
    assert data.get("version") == "3.0.0"


def test_root_descriptor():
    """Root endpoint should advertise docs and workbench API entry point."""
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "docs" in data
    assert "workbench_api" in data


def test_admin_models_smoke():
    """Admin models endpoint should respond with a JSON list.

    This will fail if the Admin router is not mounted or if RBAC wiring is
    broken, which is the intended signal for CI.
    """
    response = client.get("/v1/admin/models", headers=_admin_headers())
    assert response.status_code == 200
    models = response.json()
    assert isinstance(models, list)


def test_admin_training_export_empty_list_ok():
    """Admin training export should accept an empty image_ids list.

    This exercise the TrainingExporter path without requiring any DB rows.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/admin/training/export",
        headers=_admin_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_explorer_export_empty_list_ok():
    """Explorer export should be wired and RBAC-protected for taggers.

    Like the admin export, this uses an empty image_ids list to avoid
    depending on seeded data while still verifying router + schema wiring.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/explorer/export",
        headers=_tagger_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_404_handling_json():
    """Invalid routes should return JSON 404, not HTML."""
    response = client.get("/v1/non_existent")
    assert response.status_code == 404
    body = response.json()
    assert isinstance(body, dict)
    assert body.get("detail") == "Not Found"


if __name__ == "__main__":
    print("Running basic API smoketests...")
    test_health_check()
    test_root_descriptor()
    print("Selected smoketests: PASS (if no assertion errors shown)")----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase1/README_v3.md
----- CONTENT START -----
# Image Tagger v3.0 - Enterprise Edition

This is the production-ready, micro-frontend architecture for the Image Tagger system.

## üèóÔ∏è Architecture
* **Frontend:** Monorepo with 4 distinct React Apps (Workbench, Monitor, Admin, Explorer).
* **Backend:** Unified FastAPI service with PostgreSQL.
* **Infrastructure:** Docker Compose orchestration with Nginx Gateway.

## üöÄ Quick Start (The "Enterprise Go")

1.  **Ensure Docker is installed.**
2.  **Run:**
    ```bash
    cd deploy
    docker-compose up --build
    ```
3.  **Access the GUIs:**
    * **Research Explorer:** [http://localhost:8080/explorer](http://localhost:8080/explorer)
    * **Tagger Workbench:** [http://localhost:8080/workbench](http://localhost:8080/workbench)
    * **Supervisor Monitor:** [http://localhost:8080/monitor](http://localhost:8080/monitor)
    * **Admin Cockpit:** [http://localhost:8080/admin](http://localhost:8080/admin)

## üß™ Running Tests

To verify the API logic without Docker:
1.  `pip install pytest httpx`
2.  `pytest tests/test_v3_api.py`----- CONTENT END -----
----- FILE PATH: archive/v3_2_15_phase1/v3_governance.yml
----- CONTENT START -----
# v3_governance.yml
# THE IRON DOME: Active Protection Rules
# -------------------------------------------------------------------------
# The Guardian script uses this file to determine which areas of the codebase
# are "Immutable" or "Strictly Governed".

policy_version: "3.0.0"

# 1. Protected Directories
# Files in these folders cannot be deleted or renamed without manual override.
# Logic changes are allowed, but Structural Drift is forbidden.
protected_scopes:
  - "backend/database"    # The Source of Truth (Schema)
  - "backend/science"     # The Core Intellectual Property (Algorithms)
  - "backend/models"      # The Data Contracts
  - "frontend/shared"     # The Interface Contracts
  - "frontend/apps"       # The 4 GUIs (Existence check only)

# 2. Critical Files
# These files must NEVER be empty or missing.
critical_files:
  - "backend/main.py"
  - "backend/config.py"
  - "deploy/docker-compose.yml"
  - "deploy/nginx.conf"

# 3. Constraints
constraints:
  min_file_size_bytes: 50   # Prevents "empty file" hallucinations
  prevent_new_root_files: true # Stops AI from dumping scripts in /root----- CONTENT END -----
----- FILE PATH: archive/v3_2_16_phase1_rebuild/README_v3.md
----- CONTENT START -----
# Image Tagger v3.0 - Enterprise Edition

This is the production-ready, micro-frontend architecture for the Image Tagger system.

## üèóÔ∏è Architecture
* **Frontend:** Monorepo with 4 distinct React Apps (Workbench, Monitor, Admin, Explorer).
* **Backend:** Unified FastAPI service with PostgreSQL.
* **Infrastructure:** Docker Compose orchestration with Nginx Gateway.

## üöÄ Quick Start (The "Enterprise Go")

1.  **Ensure Docker is installed.**
2.  **Run:**
    ```bash
    cd deploy
    docker-compose up --build
    ```
3.  **Access the GUIs:**
    * **Research Explorer:** [http://localhost:8080/explorer](http://localhost:8080/explorer)
    * **Tagger Workbench:** [http://localhost:8080/workbench](http://localhost:8080/workbench)
    * **Supervisor Monitor:** [http://localhost:8080/monitor](http://localhost:8080/monitor)
    * **Admin Cockpit:** [http://localhost:8080/admin](http://localhost:8080/admin)

## üß™ Running Tests

To verify the API logic without Docker:
1.  `pip install pytest httpx`
2.  `pytest tests/test_v3_api.py`----- CONTENT END -----
----- FILE PATH: archive/v3_2_16_phase1_rebuild/v3_governance.yml
----- CONTENT START -----
# v3_governance.yml
# THE IRON DOME: Active Protection Rules
# -------------------------------------------------------------------------
# The Guardian script uses this file to determine which areas of the codebase
# are "Immutable" or "Strictly Governed".

policy_version: "3.0.0"

# 1. Protected Directories
# Files in these folders cannot be deleted or renamed without manual override.
# Logic changes are allowed, but Structural Drift is forbidden.
protected_scopes:
  - "backend/database"    # The Source of Truth (Schema)
  - "backend/science"     # The Core Intellectual Property (Algorithms)
  - "backend/models"      # The Data Contracts
  - "frontend/shared"     # The Interface Contracts
  - "frontend/apps"       # The 4 GUIs (Existence check only)

# 2. Critical Files
# These files must NEVER be empty or missing.
critical_files:
  - "backend/main.py"
  - "backend/config.py"
  - "deploy/docker-compose.yml"
  - "deploy/nginx.conf"

# 3. Constraints
constraints:
  min_file_size_bytes: 50   # Prevents "empty file" hallucinations
  prevent_new_root_files: true # Stops AI from dumping scripts in /root----- CONTENT END -----
----- FILE PATH: archive/v3_2_16_phase2/deploy/Dockerfile.backend
----- CONTENT START -----
# Use official Python runtime
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (Required for OpenCV and Postgres)
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libpq-dev \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
# We install directly to avoid copying a requirements file if we don't want to
RUN pip install --no-cache-dir \
    fastapi==0.109.0 \
    uvicorn==0.27.0 \
    sqlalchemy==2.0.25 \
    psycopg2-binary==2.9.9 \
    pydantic==2.6.1 \
    pydantic-settings==2.1.0 \
    python-jose[cryptography]==3.3.0 \
    passlib[bcrypt]==1.7.4 \
    python-multipart==0.0.7 \
    pillow==10.2.0 \
    numpy==1.26.3 \
    opencv-python-headless==4.9.0.80

# Copy the backend code
COPY backend /app/backend

# Expose port (internal only, Nginx routes to this)
EXPOSE 8000

# Startup Command
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]----- CONTENT END -----
----- FILE PATH: archive/v3_2_17_phase2_rebuild/deploy/Dockerfile.backend
----- CONTENT START -----
# Use official Python runtime
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (Required for OpenCV and Postgres)
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libpq-dev \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
# We install directly to avoid copying a requirements file if we don't want to
RUN pip install --no-cache-dir \
    fastapi==0.109.0 \
    uvicorn==0.27.0 \
    sqlalchemy==2.0.25 \
    psycopg2-binary==2.9.9 \
    pydantic==2.6.1 \
    pydantic-settings==2.1.0 \
    python-jose[cryptography]==3.3.0 \
    passlib[bcrypt]==1.7.4 \
    python-multipart==0.0.7 \
    pillow==10.2.0 \
    numpy==1.26.3 \
    opencv-python-headless==4.9.0.80

# Copy the backend code
COPY backend /app/backend

# Expose port (internal only, Nginx routes to this)
EXPOSE 8000

# Startup Command
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]----- CONTENT END -----
----- FILE PATH: archive/v3_2_18_phase3_rebuild/backend/science/pipeline.py
----- CONTENT START -----
import cv2
import numpy as np
from sqlalchemy.orm import Session
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame

# Import our new modules
from backend.science.math.fractals import FractalAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.color import ColorAnalyzer
from backend.science.vision.materials import MaterialAnalyzer
from backend.science.context.social import SocialDispositionAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer
from backend.science.perception import PerceptionProcessor

class SciencePipeline:
    def __init__(self, db: Session):
        self.db = db
        self.perception = PerceptionProcessor()

    async def process_image(self, image_id: int):
        # 1. Load Image
        img_record = self.db.query(Image).filter(Image.id == image_id).first()
        if not img_record: return False
        
        # In production, load from S3/Disk
        # Mocking a blank image for logic verification if file missing
        try:
            local_path = f"data_store/{img_record.storage_path}"
            pixels = cv2.imread(local_path)
            pixels = cv2.cvtColor(pixels, cv2.COLOR_BGR2RGB)
        except:
            pixels = np.zeros((800, 600, 3), dtype=np.uint8)

        frame = AnalysisFrame(image_id=image_id, original_image=pixels)

        # 2. Run Math (Fast / CPU)
        # Calculates Entropy, Edge Density, and Organization Ratio
        ComplexityAnalyzer.analyze(frame)
        
        # Calculates Global Fractal Dimension (D)
        d_score = FractalAnalyzer.fractal_dimension(frame.original_image)
        frame.add_attribute("fractal.D.global", d_score)
        # Material & color analysis (ported from v2 heuristics)
        MaterialAnalyzer.analyze(frame)
        ColorAnalyzer.analyze(frame)


        # 3. Run Context (Slow / GPU / VLM)
        await SocialDispositionAnalyzer.analyze(frame, self.perception)
        await CognitiveStateAnalyzer.analyze(frame, self.perception)

        # 4. Save to Database
        self._save_results(image_id, frame.attributes)
        
        return True

    def _save_results(self, image_id, attributes):
        for key, value in attributes.items():
            val = Validation(
                user_id=1, # System User
                image_id=image_id,
                attribute_key=key,
                value=value,
                duration_ms=0
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: archive/v3_2_18_phase3_rebuild/backend/science/math/glcm.py
----- CONTENT START -----
import numpy as np
from skimage.feature import graycomatrix, graycoprops
from backend.science.core import AnalysisFrame

class TextureAnalyzer:
    """
    Uses Gray-Level Co-occurrence Matrices (GLCM) to measure texture quality.
    """
    
    @staticmethod
    def analyze(frame: AnalysisFrame):
        # GLCM requires uint8 image
        gray = frame.gray_image
        
        # Compute GLCM
        # distances=[1], angles=[0, 45, 90, 135]
        glcm = graycomatrix(gray, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], 
                            levels=256, symmetric=True, normed=True)
        
        # Extract Properties
        contrast = graycoprops(glcm, 'contrast')[0, 0]
        homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
        energy = graycoprops(glcm, 'energy')[0, 0]
        correlation = graycoprops(glcm, 'correlation')[0, 0]
        
        # Normalize Contrast (heuristic)
        contrast_norm = min(contrast / 1000.0, 1.0)
        
        frame.add_attribute("texture.glcm.contrast", contrast_norm)
        frame.add_attribute("texture.glcm.homogeneity", homogeneity)
        frame.add_attribute("texture.glcm.energy", energy)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/core.py
----- CONTENT START -----
import numpy as np
from dataclasses import dataclass, field
from typing import Dict, Any, List, Tuple

@dataclass
class AnalysisFrame:
    """
    Standard unit of analysis for the v3 pipeline.
    Replaces the old 'ImageData' and 'SegmentationData' sprawl.
    """
    image_id: int
    original_image: np.ndarray  # RGB
    
    # Derived data (populated by pipeline)
    gray_image: np.ndarray = None
    edges: np.ndarray = None
    
    # Results
    attributes: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        # Lazy load opencv only when needed
        import cv2
        if self.gray_image is None:
            self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_RGB2GRAY)
        if self.edges is None:
            self.edges = cv2.Canny(self.gray_image, 50, 150)

    def add_attribute(self, key: str, value: float, confidence: float = 1.0):
        self.attributes[key] = float(value)
        self.metadata[key] = {"confidence": confidence}----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/pipeline.py
----- CONTENT START -----
import logging
from typing import Optional

import numpy as np
from sqlalchemy.orm import Session

from backend.database.core import SessionLocal
from backend.database.core import get_db  # kept for FastAPI wiring
from backend.database.models import Image, Validation
from backend.science.core import AnalysisFrame
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.vision.materials import MaterialAnalyzer
from backend.science.math.color import ColorAnalyzer
from backend.science.vision import VisionProcessor
from backend.science.context.cognitive import CognitiveStateAnalyzer
from backend.science.context.social import SocialDispositionAnalyzer

logger = logging.getLogger(__name__)


def run_full_analysis(frame: AnalysisFrame, enable_expensive: bool = False) -> None:
    """Run the core science analyzers on an in-memory AnalysisFrame.

    This helper is designed for scripts and unit tests. It does *not* touch the
    database; it only populates `frame.attributes`.
    """

    # L0: fast numeric metrics
    ComplexityAnalyzer.analyze(frame)
    TextureAnalyzer.analyze(frame)
    try:
        d_score = FractalAnalyzer.fractal_dimension(frame.original_image)
        frame.add_attribute("fractal.D.global", float(d_score), confidence=0.8)
    except Exception as exc:  # pragma: no cover - defensive
        logger.exception("FractalAnalyzer failed: %s", exc)

    # L1: material + color heuristics
    try:
        MaterialAnalyzer.analyze(frame)
    except Exception as exc:  # pragma: no cover
        logger.exception("MaterialAnalyzer failed: %s", exc)

    try:
        ColorAnalyzer.analyze(frame)
    except Exception as exc:  # pragma: no cover
        logger.exception("ColorAnalyzer failed: %s", exc)

    # L2/L3: higher-level context (kept light for now)
    try:
        CognitiveStateAnalyzer.analyze(frame)
    except Exception as exc:  # pragma: no cover
        logger.exception("CognitiveStateAnalyzer failed: %s", exc)

    try:
        SocialDispositionAnalyzer.analyze(frame)
    except Exception as exc:  # pragma: no cover
        logger.exception("SocialDispositionAnalyzer failed: %s", exc)


class SciencePipeline:
    """DB-backed science pipeline.

    Usage pattern (inside a FastAPI dependency or script):

    >>> db = SessionLocal()
    >>> pipeline = SciencePipeline(db)
    >>> pipeline.process_image(image_id=1)

    This will:
      1. Load the Image row.
      2. Load pixels from the data_store.
      3. Run the science analyzers.
      4. Persist attributes to Validation rows.
    """

    def __init__(self, db: Session):
        self.db = db

    async def process_image(self, image_id: int) -> bool:
        image: Optional[Image] = self.db.query(Image).filter(Image.id == image_id).one_or_none()
        if image is None:
            logger.warning("SciencePipeline: image %s not found", image_id)
            return False

        # Load pixels from the data_store
        try:
            from backend.services.data_store import load_image  # lazy import
            pil_image = load_image(image.storage_path)
        except Exception as exc:  # pragma: no cover - defensive
            logger.exception("SciencePipeline: failed to load image %s: %s", image_id, exc)
            # Fallback: create a neutral gray image
            import PIL.Image as ImageLib
            pil_image = ImageLib.new("RGB", (256, 256), color=(128, 128, 128))

        frame = AnalysisFrame.from_pil(pil_image, image_id=image.id)

        # Run analyzers
        run_full_analysis(frame)

        # Persist attributes to Validation rows
        for attr in frame.attributes:
            validation = Validation(
                image_id=image.id,
                attribute_key=attr.key,
                value=float(attr.value),
                user_id=None,
                region_id=None,
                duration_ms=None,
                source="science_pipeline",
            )
            self.db.add(validation)

        self.db.commit()
        logger.info("SciencePipeline: processed image %s with %d attributes", image_id, len(frame.attributes))
        return True----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/context/cognitive.py
----- CONTENT START -----
from backend.science.core import AnalysisFrame

class CognitiveStateAnalyzer:
    """
    Analyzes features affecting cognitive load and emotional state.
    """

    PROMPT_TEMPLATE = """
    Evaluate this space for its impact on human cognitive state:
    
    1. Restoration (Kaplan's ART): Likelihood of attention restoration.
    2. Mystery: Promise of more information if one moves deeper.
    3. Legibility: Ease of mapping the space mentally.
    4. Coherence: Order and organization of elements.
    
    Output strictly in JSON format.
    """

    @staticmethod
    async def analyze(frame: AnalysisFrame, perception_engine):
        # MOCK RESULT
        frame.add_attribute("cognitive.restoration", 0.65, confidence=0.6)
        frame.add_attribute("cognitive.mystery", 0.82, confidence=0.7)
        frame.add_attribute("cognitive.legibility", 0.90, confidence=0.9)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/math/color.py
----- CONTENT START -----
"""
Color analysis module for v3 Science Pipeline.

Adapts key ideas from v2.6.3 color extractor into the v3 AnalysisFrame
pattern. Computes a small set of robust, interpretable metrics:
- luminance mean / std
- color temperature proxy
- saturation mean
- hue entropy (color diversity)
- warm vs cool dominance
- composite "color richness"
"""

from __future__ import annotations

import numpy as np
import cv2

from backend.science.core import AnalysisFrame


class ColorAnalyzer:
    """Static color-analysis utilities for an AnalysisFrame."""

    @staticmethod
    def analyze(frame: AnalysisFrame) -> None:
        img = frame.original_image
        if img is None:
            return

        # Normalize to uint8 RGB
        if img.dtype == np.float32 or img.dtype == np.float64:
            image_uint8 = (img * 255).astype(np.uint8)
        else:
            image_uint8 = img

        # Grayscale for luminance
        gray = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2GRAY)
        mean_lum = float(np.mean(gray) / 255.0)
        std_lum = float(np.std(gray) / 255.0)

        # HSV for hue/saturation analysis
        hsv = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2HSV)
        hue = hsv[:, :, 0]
        sat = hsv[:, :, 1]

        # Color temperature proxy (ported from v2 logic)
        color_temp = ColorAnalyzer._compute_color_temperature(image_uint8)

        # Hue entropy (diversity of colors)
        hue_entropy = ColorAnalyzer._compute_hue_entropy(hue)

        # Saturation mean
        sat_mean = float(np.mean(sat) / 255.0)

        # Warm vs cool dominance
        warm_ratio = ColorAnalyzer._compute_warm_cool_ratio(hsv)

        # Composite color richness: saturation √ó hue entropy
        color_richness = sat_mean * hue_entropy

        frame.add_attribute("color.luminance_mean", mean_lum)
        frame.add_attribute("color.luminance_std", std_lum)
        frame.add_attribute("color.temperature", color_temp)
        frame.add_attribute("color.saturation_mean", sat_mean)
        frame.add_attribute("color.hue_entropy", hue_entropy)
        frame.add_attribute("color.warm_color_dominance", warm_ratio)
        frame.add_attribute("color.richness", color_richness)

    @staticmethod
    def _compute_color_temperature(image_rgb: np.ndarray) -> float:
        """
        Estimate normalized color temperature based on red/blue ratio.

        Adapted from v2: maps approximate Kelvin range [2000, 10000]K
        into a [0, 1] scalar for downstream use.
        """
        r_mean = float(np.mean(image_rgb[:, :, 0]))
        b_mean = float(np.mean(image_rgb[:, :, 2]))

        if b_mean == 0.0:
            b_mean = 1.0

        rb_ratio = r_mean / b_mean

        # Simplified mapping from v2
        if rb_ratio > 1.2:
            # Warm (2000‚Äì4000K)
            temp_k = 2000.0 + (rb_ratio - 1.2) * 2000.0
        elif rb_ratio < 0.8:
            # Cool (6000‚Äì10000K)
            temp_k = 10000.0 - (0.8 - rb_ratio) * 4000.0
        else:
            # Neutral (4000‚Äì6000K)
            temp_k = 4000.0 + (rb_ratio - 0.8) * 5000.0

        temp_normalized = (temp_k - 2000.0) / 8000.0
        return float(np.clip(temp_normalized, 0.0, 1.0))

    @staticmethod
    def _compute_hue_entropy(hue_channel: np.ndarray) -> float:
        """
        Compute normalized entropy of hue distribution.

        Direct adaptation of v2 _compute_hue_entropy implementation,
        but as a static utility.
        """
        hist, _ = np.histogram(hue_channel.flatten(), bins=180, range=(0, 180))
        hist = hist.astype(np.float64)
        hist = hist / (np.sum(hist) + 1e-7)

        entropy = -np.sum(hist * np.log2(hist + 1e-7))
        max_entropy = np.log2(180.0)
        return float(entropy / max_entropy)

    @staticmethod
    def _compute_warm_cool_ratio(hsv: np.ndarray) -> float:
        """
        Compute ratio of warm to cool colors.

        Warm hues: 0‚Äì60 in OpenCV HSV
        Cool hues: 90‚Äì150 in OpenCV HSV
        """
        hue = hsv[:, :, 0]

        warm_mask = (hue >= 0) & (hue <= 60)
        cool_mask = (hue >= 90) & (hue <= 150)

        warm_count = float(np.sum(warm_mask))
        cool_count = float(np.sum(cool_mask))
        total = warm_count + cool_count

        if total == 0.0:
            return 0.5  # Neutral

        warm_ratio = warm_count / total
        return float(warm_ratio)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/math/complexity.py
----- CONTENT START -----
import numpy as np
import cv2
from scipy.stats import entropy
from backend.science.core import AnalysisFrame

class ComplexityAnalyzer:
    """
    Quantifies 'Visual Complexity' using Shannon Entropy and Edge Density.
    High Complexity + High Order = 'Organized Complexity' (Goldilocks Zone).
    """

    @staticmethod
    def calculate_shannon_entropy(image: np.ndarray) -> float:
        """
        Measures the 'information content' of the image texture.
        """
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image
            
        # Calculate histogram
        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
        hist = hist.ravel() / hist.sum()
        
        # Compute entropy
        return entropy(hist, base=2)

    @staticmethod
    def calculate_edge_density(frame: AnalysisFrame) -> float:
        """
        Ratio of edge pixels to total pixels. A proxy for visual clutter.
        """
        total_pixels = frame.edges.size
        edge_pixels = np.count_nonzero(frame.edges)
        return edge_pixels / total_pixels

    @staticmethod
    def analyze(frame: AnalysisFrame):
        # 1. Raw Complexity
        ent = ComplexityAnalyzer.calculate_shannon_entropy(frame.original_image)
        frame.add_attribute("complexity.entropy", ent)
        
        # 2. Structural Density
        dens = ComplexityAnalyzer.calculate_edge_density(frame)
        frame.add_attribute("complexity.edge_density", dens)
        
        # 3. 'Organized Complexity' Proxy
        # If Entropy is High but Edge Density is Moderate -> Organized
        # If Entropy High and Edge Density High -> Chaotic
        ratio = ent / (dens + 0.01)
        frame.add_attribute("complexity.organization_ratio", ratio)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/math/fractals.py
----- CONTENT START -----
import numpy as np
import cv2
from backend.science.core import AnalysisFrame

class FractalAnalyzer:
    """
    Implements the 'Box Counting' method to estimate Fractal Dimension (D).
    Can be applied globally or to specific segmentation masks (regions).
    """

    @staticmethod
    def fractal_dimension(image_array: np.ndarray, threshold=0.9) -> float:
        """
        Calculates Minkowski‚ÄìBouligand dimension (Box-counting dimension).
        """
        # 1. Binarize (Edges or Threshold)
        # If image is not binary, run Canny edge detection first
        if len(image_array.shape) > 2:
            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            Z = edges > 0
        else:
            Z = image_array > threshold

        # 2. Minimal checking
        if np.sum(Z) == 0:
            return 0.0

        # 3. Box Counting
        p = min(Z.shape)
        n = 2**np.floor(np.log(p)/np.log(2))
        n = int(np.log(n)/np.log(2))
        sizes = 2**np.arange(n, 1, -1)
        counts = []

        for size in sizes:
            counts.append(FractalAnalyzer._box_count(Z, size))

        # 4. Linear Regression to find D (Slope of log-log plot)
        coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)
        return -coeffs[0]

    @staticmethod
    def _box_count(Z, k):
        S = np.add.reduceat(
            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),
                               np.arange(0, Z.shape[1], k), axis=1)
        return len(np.where((S > 0) & (S < k*k))[0])

    @staticmethod
    def analyze_regions(frame: AnalysisFrame, segmentation_masks: dict):
        """
        Calculates D for specific architectural elements (e.g., Walls vs Floors).
        """
        for label, mask in segmentation_masks.items():
            # Mask the edge map with the region
            masked_edges = np.logical_and(frame.edges > 0, mask > 0)
            d_score = FractalAnalyzer.fractal_dimension(masked_edges)
            
            # Store as a localized attribute
            frame.add_attribute(f"fractal.D.{label}", d_score)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/backend/science/math/glcm.py
----- CONTENT START -----
import logging
from typing import Optional

import numpy as np

try:
    from skimage.feature import graycomatrix, graycoprops
    _HAS_SKIMAGE = True
except Exception:  # pragma: no cover - optional dependency
    _HAS_SKIMAGE = False

from backend.science.core import AnalysisFrame

logger = logging.getLogger(__name__)


class TextureAnalyzer:
    """Compute simple GLCM-based texture metrics.

    This is deliberately conservative: we use a single distance and a small set
    of angles, then aggregate by taking the mean across orientations. The goal
    is to produce stable, interpretable scalar attributes that can feed
    downstream CNfA models.
    """

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        if frame.gray_image is None:
            frame.ensure_gray()

        gray = frame.gray_image
        if gray is None:
            # Fallback: neutral texture values
            logger.warning("TextureAnalyzer: gray_image missing; writing neutral texture attributes.")
            cls._write_neutral(frame)
            return

        # Normalize to uint8 for skimage
        try:
            arr = np.asarray(gray)
            if arr.ndim == 3:
                # Defensive: convert RGB to luminance
                arr = np.dot(arr[:, :, :3], [0.299, 0.587, 0.114])
            arr = arr.astype(np.float32)
            if arr.max() > 0:
                arr = arr / arr.max()
            arr = (arr * 255).astype(np.uint8)
        except Exception as exc:  # pragma: no cover - defensive
            logger.exception("TextureAnalyzer: failed to normalize image: %s", exc)
            cls._write_neutral(frame)
            return

        if not _HAS_SKIMAGE:
            logger.warning("TextureAnalyzer: skimage not available; writing neutral texture attributes.")
            cls._write_neutral(frame)
            return

        try:
            distances = [1]
            angles = [0, np.pi / 4.0, np.pi / 2.0, 3.0 * np.pi / 4.0]
            glcm = graycomatrix(
                arr,
                distances=distances,
                angles=angles,
                levels=256,
                symmetric=True,
                normed=True,
            )

            contrast = graycoprops(glcm, "contrast").mean()
            homogeneity = graycoprops(glcm, "homogeneity").mean()
            energy = graycoprops(glcm, "energy").mean()

            # Normalize contrast into [0,1] by a simple monotonic transform
            norm_contrast = contrast / (contrast + 1.0)

            frame.add_attribute("texture.glcm.contrast", float(norm_contrast), confidence=0.9)
            frame.add_attribute("texture.glcm.homogeneity", float(homogeneity), confidence=0.9)
            frame.add_attribute("texture.glcm.energy", float(energy), confidence=0.9)
        except Exception as exc:  # pragma: no cover - defensive
            logger.exception("TextureAnalyzer: GLCM computation failed: %s", exc)
            cls._write_neutral(frame)

    @classmethod
    def _write_neutral(cls, frame: AnalysisFrame) -> None:
        frame.add_attribute("texture.glcm.contrast", 0.0, confidence=0.1)
        frame.add_attribute("texture.glcm.homogeneity", 0.5, confidence=0.1)
        frame.add_attribute("texture.glcm.energy", 0.5, confidence=0.1)----- CONTENT END -----
----- FILE PATH: archive/v3_2_21_science_upgrade/scripts/run_science_on_sample.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Run the science pipeline on a single sample image.

This is a convenience script to verify end-to-end wiring:

- Database connectivity via SQLAlchemy
- Image loading via the data_store service
- Science analyzers (complexity, texture, fractals, materials, color, context)
- Persistence of attributes into Validation rows
"""

import argparse
import asyncio
import logging

from backend.database.core import SessionLocal
from backend.science.pipeline import SciencePipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def _run(image_id: int) -> None:
    db = SessionLocal()
    try:
        pipeline = SciencePipeline(db)
        ok = await pipeline.process_image(image_id=image_id)
        if not ok:
            logger.error("Science pipeline failed for image_id=%s", image_id)
        else:
            logger.info("Science pipeline completed for image_id=%s", image_id)
    finally:
        db.close()


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--image-id", type=int, required=True, help="ID of the Image row to process.")
    args = parser.parse_args()
    asyncio.run(_run(args.image_id))


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: archive/v3_2_22_discovery_upgrade/backend/api/v1_discovery.py
----- CONTENT START -----
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
import io
from fastapi.responses import StreamingResponse

from backend.database.core import get_db
from backend.services.auth import require_tagger
from backend.services.training_export import TrainingExporter
from backend.services.query_builder import QueryBuilder
from backend.schemas.training import TrainingExample
from backend.schemas.discovery import SearchQuery, ImageSearchResult, ExportRequest, AttributeRead
from backend.models.attribute import Attribute
from backend.models.annotation import Validation


router = APIRouter(
    prefix="/v1/explorer",
    tags=["explorer"],
    dependencies=[Depends(require_tagger)],
)


@router.post("/search", response_model=ImageSearchResult)
def search_images(
    query: SearchQuery,
    db: Session = Depends(get_db),
):
    """
    Explorer GUI search endpoint.
    Aligns with QueryBuilder.execute(filters, page, page_size).
    """
    qb = QueryBuilder(db)
    # Expect SearchQuery.filters to be a dict of filter fields supported by QueryBuilder.
    results = qb.execute(
        filters=query.filters or {},
        page=query.page or 1,
        page_size=query.page_size or 24,
    )

    # Convert raw ORM Image rows to ImageSearchResult schema
    items = []
    for img in results.get("items", []):
        # Gather positive tags from validations
        try:
            vals = (
                db.query(Validation)
                .filter(Validation.image_id == img.id)
                .all()
            )
            tags = [
                v.attribute.name
                for v in vals
                if (v.value is not None and v.value > 0.5 and v.attribute is not None)
            ]
        except Exception:
            tags = []

        items.append(
            {
                "image_id": img.id,
                "url": getattr(img, "storage_path", "") or "",
                "tags": tags,
                "meta_data": getattr(img, "meta_data", None),
            }
        )

    return ImageSearchResult(
        items=items,
        total=results.get("total", len(items)),
        page=query.page or 1,
        page_size=query.page_size or 24,
    )


@router.post("/export")
def export_training_examples(
    req: ExportRequest,
    db: Session = Depends(get_db),
):
    exporter = TrainingExporter(db)
    stream = exporter.export_stream(req.format)
    filename = f"training_export.{req.format}"
    return StreamingResponse(
        io.BytesIO(stream.getvalue()),
        media_type="application/octet-stream",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


@router.get("/attributes", response_model=List[AttributeRead])
def list_attributes(db: Session = Depends(get_db)):
    attrs = db.query(Attribute).order_by(Attribute.name.asc()).all()
    return [AttributeRead.model_validate(a) for a in attrs]----- CONTENT END -----
----- FILE PATH: archive/v3_2_23_smokescience_upgrade/install.sh
----- CONTENT START -----
#!/bin/bash
echo "üöÄ Starting Image Tagger v3.1"
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker Desktop."
    exit 1
fi

# Governance check (Guardian)
echo "üîí Running Guardian (governance) checks"
if command -v python3 &> /dev/null; then
    if [ -f "governance.lock" ]; then
        python3 scripts/guardian.py verify
        GUARDIAN_RC=$?
        if [ "$GUARDIAN_RC" -ne 0 ]; then
            echo "‚ùå Guardian verification failed (rc=$GUARDIAN_RC). Aborting install."
            exit $GUARDIAN_RC
        fi
    else
        echo "‚ö†Ô∏è governance.lock not found; creating initial baseline with 'guardian.py freeze'."
        python3 scripts/guardian.py freeze
        if [ "$?" -ne 0 ]; then
            echo "‚ùå Guardian freeze failed. Aborting install."
            exit 1
        fi
    fi
else
    echo "‚ö†Ô∏è python3 not found; skipping Guardian checks."
fi




echo "üê≥ Building containers"
cd deploy
docker-compose up -d --build

echo "üå± Seeding database"
sleep 5
docker-compose exec -T api python3 backend/scripts/seed_tool_configs.py
docker-compose exec -T api python3 backend/scripts/seed_attributes.py

echo "‚úÖ SYSTEM ONLINE at http://localhost:8080"


echo "üß™ Running smoke tests (API + Science)"
docker-compose exec -T api python3 scripts/smoke_api.py
docker-compose exec -T api python3 scripts/smoke_science.py
echo "‚úÖ Smoke tests passed."
echo "üß™ Running pytest API smoketests"
docker-compose exec -T api pytest -q tests/test_v3_api.py
if [ "$?" -ne 0 ]; then
    echo "‚ùå Pytest API smoketests failed."
    exit 1
fi
echo "‚úÖ Pytest API smoketests passed."----- CONTENT END -----
----- FILE PATH: archive/v3_2_23_smokescience_upgrade/scripts/smoke_science.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Lightweight science pipeline smoke test for Image Tagger v3.2.

We import the pipeline and run it on a tiny synthetic image to make
sure the core analysis functions can execute without crashing.
"""

import numpy as np

from backend.science.core import AnalysisFrame
from backend.science.pipeline import run_full_analysis


def main() -> None:
    # Simple 64x64 RGB gradient
    h, w = 64, 64
    y = np.linspace(0, 255, h, dtype=np.uint8).reshape(-1, 1)
    x = np.linspace(0, 255, w, dtype=np.uint8).reshape(1, -1)
    img = np.stack([x.repeat(h, axis=0), y.repeat(w, axis=1), np.full((h, w), 128, dtype=np.uint8)], axis=-1)

    frame = AnalysisFrame(original_image=img, image_id="smoke-test")
    run_full_analysis(frame)
    # Print a small subset of attributes as a sanity check
    sample = dict(list(frame.attributes.items())[:10])
    print("[smoke_science] ran analysis; sample attributes:", sample)


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: archive/v3_2_24_monitor_tightening/backend/api/v1_supervision.py
----- CONTENT START -----
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from sqlalchemy import select, func
from typing import List

from backend.database.core import get_db
from backend.services.auth import require_admin
from backend.models.users import User
from backend.models.annotation import Validation
from backend.models.assets import Image
from backend.schemas.supervision import TaggerPerformance, IRRStat, ValidationDetail


router = APIRouter(prefix="/v1/monitor", tags=["Supervisor Dashboard"])


@router.get("/velocity", response_model=List[TaggerPerformance])
async def get_tagger_velocity(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> List[TaggerPerformance]:
    """Aggregate basic per-tagger velocity and status from Validation records.

    This endpoint powers the Team Velocity table in the Supervisor dashboard.
    """
    stmt = (
        select(
            Validation.user_id,
            func.count(Validation.id).label("count"),
            func.avg(Validation.duration_ms).label("avg_duration"),
        )
        .group_by(Validation.user_id)
    )
    rows = db.execute(stmt).all()
    if not rows:
        return []

    user_ids = [r.user_id for r in rows if r.user_id is not None]
    users_by_id: dict[int, User] = {}
    if user_ids:
        res = db.execute(select(User).where(User.id.in_(user_ids)))
        users_by_id = {u.id: u for u in res.scalars().all()}

    stats: List[TaggerPerformance] = []
    for r in rows:
        uid = r.user_id
        u = users_by_id.get(uid)
        username = u.username if u is not None else f"user-{uid}"

        avg_ms = int(r.avg_duration or 0)

        # Simple heuristic: if someone is making many decisions with
        # very low dwell time, flag them as suspicious.
        status = "active"
        if r.count and avg_ms > 0 and r.count >= 50 and avg_ms < 400:
            status = "flagged"

        stats.append(
            TaggerPerformance(
                user_id=uid,
                username=username,
                images_validated=int(r.count or 0),
                avg_duration_ms=avg_ms,
                status=status,
            )
        )

    return stats


@router.get("/irr", response_model=List[IRRStat])
async def get_irr_stats(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> List[IRRStat]:
    """Compute a simple per-image agreement score across raters.

    This is *not* full Fleiss‚Äô kappa. Instead it provides:
      - agreement_score: proportion of ratings in the majority (0‚Äì1)
      - conflict_count: min(#positives, #negatives) as a crude conflict indicator
      - raters: list of user identifiers contributing to the image

    This is enough to drive a meaningful IRR heatmap in the Supervisor GUI and
    can later be upgraded to true kappa without breaking the contract.
    """
    stmt = select(
        Validation.image_id,
        Validation.user_id,
        Validation.value,
    )
    rows = db.execute(stmt).all()
    if not rows:
        return []

    per_image: dict[int, list[tuple[int, float]]] = {}
    for image_id, user_id, value in rows:
        if image_id is None or user_id is None:
            continue
        per_image.setdefault(image_id, []).append((user_id, value))

    if not per_image:
        return []

    image_ids = list(per_image.keys())
    images_by_id: dict[int, Image] = {}
    if image_ids:
        res = db.execute(select(Image).where(Image.id.in_(image_ids)))
        images_by_id = {img.id: img for img in res.scalars().all()}

    irr_stats: List[IRRStat] = []
    # Limit to a reasonable sample for dashboard purposes
    for image_id, tuples in list(per_image.items())[:100]:
        values = [v for (_, v) in tuples if v is not None]
        if not values:
            continue

        # Binary discretisation: value > 0.5 ‚Üí positive, else negative
        labels = [1 if v > 0.5 else 0 for v in values]
        n = len(labels)
        pos = sum(labels)
        neg = n - pos

        majority = max(pos, neg)
        agreement = majority / n if n > 0 else 0.0
        conflict_count = int(min(pos, neg))

        img = images_by_id.get(image_id)
        filename = img.filename if img is not None else f"image-{image_id}"

        raters = [str(uid) for (uid, _) in tuples]

        irr_stats.append(
            IRRStat(
                image_id=image_id,
                filename=filename,
                agreement_score=float(agreement),
                conflict_count=conflict_count,
                raters=raters,
            )
        )

    return irr_stats

@router.get("/image/{image_id}/validations", response_model=list[ValidationDetail])
async def get_image_validations(
    image_id: int,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> list[ValidationDetail]:
    """Return all validations for a given image, joined with user info.

    This powers the Tag Inspector view in the Supervisor dashboard.
    """
    stmt = (
        select(
            Validation.id,
            Validation.user_id,
            Validation.attribute_key,
            Validation.value,
            Validation.duration_ms,
            Validation.created_at,
            User.username,
        )
        .join(User, User.id == Validation.user_id)
        .where(Validation.image_id == image_id)
        .order_by(Validation.created_at.desc())
    )
    rows = db.execute(stmt).all()
    results: list[ValidationDetail] = []
    for vid, uid, attr_key, value, duration_ms, created_at, username in rows:
        results.append(
            ValidationDetail(
                id=vid,
                user_id=uid,
                username=username or f"user-{uid}",
                attribute_key=attr_key,
                value=float(value) if value is not None else 0.0,
                duration_ms=duration_ms,
                created_at=created_at,
            )
        )
    return results----- CONTENT END -----
----- FILE PATH: archive/v3_2_24_monitor_tightening/backend/models/annotation.py
----- CONTENT START -----
from sqlalchemy import String, Float, ForeignKey, Integer
from sqlalchemy.orm import Mapped, mapped_column, relationship
from backend.database.core import Base
from backend.database.mixins import TimestampMixin

class Validation(Base, TimestampMixin):
    """
    HITL Record.
    Records a human decision on a specific tag or region.
    Powers the 'Supervisor Dashboard' (IRR calculations).
    """
    __tablename__ = "validations"

    id: Mapped[int] = mapped_column(primary_key=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id"))
    image_id: Mapped[int] = mapped_column(ForeignKey("images.id"))
    
    # The attribute being validated (e.g., "spatial.prospect")
    attribute_key: Mapped[str] = mapped_column(String, index=True)
    
    # The value assigned (0.0 - 1.0 for continuous, or categorical)
    value: Mapped[float] = mapped_column(Float)
    
    # Optional: Link to a specific region if this is a local attribute
    region_id: Mapped[int] = mapped_column(Integer, nullable=True)

    # Velocity Tracking: How long did the user look at this before clicking?
    # Critical for detecting "spam clicking" by tired taggers
    duration_ms: Mapped[int] = mapped_column(Integer, default=0)
    
    user = relationship("User", back_populates="validations")
    image = relationship("Image", back_populates="validations")----- CONTENT END -----
----- FILE PATH: archive/v3_2_24_monitor_tightening/backend/schemas/supervision.py
----- CONTENT START -----
from pydantic import BaseModel
from datetime import datetime, ConfigDict
from typing import List

class TaggerPerformance(BaseModel):
    """Contract for Tagger Velocity Charts"""
    user_id: int
    username: str
    images_validated: int
    avg_duration_ms: int
    status: str = "active"
    
    model_config = ConfigDict(from_attributes=True)

class IRRStat(BaseModel):
    """Contract for Inter-Rater Reliability Heatmap"""
    image_id: int
    filename: str
    agreement_score: float
    conflict_count: int
    raters: List[str]

from datetime import datetime

class ValidationDetail(BaseModel):
    id: int
    user_id: int
    username: str
    attribute_key: str
    value: float
    duration_ms: int | None = None
    created_at: datetime | None = None----- CONTENT END -----
----- FILE PATH: archive/v3_2_24_monitor_tightening/tests/test_v3_api.py
----- CONTENT START -----
import pytest
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)


def _admin_headers():
    """Minimal admin identity for RBAC-protected endpoints."""
    return {
        "X-User-Id": "1",
        "X-User-Role": "admin",
    }


def _tagger_headers():
    """Minimal tagger identity for workbench / explorer endpoints."""
    return {
        "X-User-Id": "10",
        "X-User-Role": "tagger",
    }


def test_health_check():
    """Verify the system is alive and versioned correctly."""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data.get("status") == "healthy"
    assert data.get("version") == "3.0.0"


def test_root_descriptor():
    """Root endpoint should advertise docs and workbench API entry point."""
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "docs" in data
    assert "workbench_api" in data


def test_admin_models_smoke():
    """Admin models endpoint should respond with a JSON list.

    This will fail if the Admin router is not mounted or if RBAC wiring is
    broken, which is the intended signal for CI.
    """
    response = client.get("/v1/admin/models", headers=_admin_headers())
    assert response.status_code == 200
    models = response.json()
    assert isinstance(models, list)


def test_admin_training_export_empty_list_ok():
    """Admin training export should accept an empty image_ids list.

    This exercise the TrainingExporter path without requiring any DB rows.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/admin/training/export",
        headers=_admin_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_explorer_export_empty_list_ok():
    """Explorer export should be wired and RBAC-protected for taggers.

    Like the admin export, this uses an empty image_ids list to avoid
    depending on seeded data while still verifying router + schema wiring.
    """
    payload = {"image_ids": [], "format": "json"}
    response = client.post(
        "/v1/explorer/export",
        headers=_tagger_headers(),
        json=payload,
    )
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)


def test_404_handling_json():
    """Invalid routes should return JSON 404, not HTML."""
    response = client.get("/v1/non_existent")
    assert response.status_code == 404
    body = response.json()
    assert isinstance(body, dict)
    assert body.get("detail") == "Not Found"


if __name__ == "__main__":
    print("Running basic API smoketests")
    test_health_check()
    test_root_descriptor()
    print("Selected smoketests: PASS (if no assertion errors shown)")

def test_explorer_attributes_endpoint():
    client = TestClient(app)
    resp = client.get("/v1/explorer/attributes", headers={"X-Role": "tagger"})
    assert resp.status_code in (200, 204)


def test_explorer_search_endpoint_minimal():
    client = TestClient(app)
    resp = client.post(
        "/v1/explorer/search",
        json={"text": None, "limit": 5, "offset": 0},
        headers={"X-Role": "tagger"},
    )
    assert resp.status_code in (200, 204)


def test_explorer_export_empty_list():
    client = TestClient(app)
    resp = client.post(
        "/v1/explorer/export",
        json={"image_ids": []},
        headers={"X-Role": "tagger"},
    )
    assert resp.status_code in (200, 204)----- CONTENT END -----
----- FILE PATH: archive/v3_2_25_guardian_upgrade/v3_governance.yml
----- CONTENT START -----
# v3_governance.yml
# THE IRON DOME: Active Protection Rules

policy_version: "3.0.0"

protected_scopes:
  - "backend/database"
  - "backend/science"
  - "backend/models"
  - "frontend/shared"
  - "frontend/apps"

critical_files:
  - "backend/main.py"
  - "backend/models/config.py"
  - "backend/database/core.py"
  - "backend/models/annotation.py"
  - "backend/api/v1_annotation.py"
  - "backend/science/pipeline.py"
  - "deploy/docker-compose.yml"
  - "deploy/nginx.conf"
  - "deploy/Dockerfile.backend"
  - "scripts/guardian.py"
  - "v3_governance.yml"
  - "README_v3.md"
  - "install.sh"

constraints:
  min_file_size_bytes: 20
  prevent_new_root_files: true----- CONTENT END -----
----- FILE PATH: archive/v3_2_25_guardian_upgrade/scripts/guardian.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
THE GUARDIAN - Governance Enforcement Script (v3.2)

Usage:
  python scripts/guardian.py freeze   -> Snapshot current state to governance.lock
  python scripts/guardian.py verify   -> Check current state against governance.lock
"""

import sys
import hashlib
import json
from pathlib import Path
from typing import Any, Dict, List

import yaml  # Requires PyYAML

REPO_ROOT = Path(__file__).resolve().parents[1]
CONFIG_FILE = REPO_ROOT / "v3_governance.yml"
LOCK_FILE = REPO_ROOT / "governance.lock"


def load_config() -> Dict[str, Any]:
    """Load the governance config from v3_governance.yml."""
    if not CONFIG_FILE.exists():
        print("[guardian] v3_governance.yml not found; using empty config.")
        return {}
    with CONFIG_FILE.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
    return data


def sha256_file(path: Path) -> str:
    """Compute SHA256 hash of a file."""
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def snapshot(conf: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a baseline snapshot:
    - Hashes of all files under protected scopes
    - List of critical files
    - Root-level files (for prevent_new_root_files)
    """
    protected_scopes: List[str] = conf.get("protected_scopes", []) or []
    critical_files: List[str] = conf.get("critical_files", []) or []
    constraints: Dict[str, Any] = conf.get("constraints", {}) or {}

    protected_files: Dict[str, Dict[str, Any]] = {}

    for scope in protected_scopes:
        scope_path = REPO_ROOT / scope
        if not scope_path.exists():
            continue
        if scope_path.is_file():
            rel = scope_path.relative_to(REPO_ROOT).as_posix()
            protected_files[rel] = {
                "hash": sha256_file(scope_path),
                "size": scope_path.stat().st_size,
            }
            continue

        for p in scope_path.rglob("*"):
            if p.is_file():
                rel = p.relative_to(REPO_ROOT).as_posix()
                protected_files[rel] = {
                    "hash": sha256_file(p),
                    "size": p.stat().st_size,
                }

    root_files = sorted([p.name for p in REPO_ROOT.iterdir() if p.is_file()])

    snapshot_obj: Dict[str, Any] = {
        "policy_version": conf.get("policy_version", "3.0.0"),
        "protected_files": protected_files,
        "critical_files": critical_files,
        "constraints": constraints,
        "root_files": root_files,
    }
    return snapshot_obj


def freeze(conf: Dict[str, Any]) -> None:
    """Write a new governance.lock baseline."""
    snap = snapshot(conf)
    with LOCK_FILE.open("w", encoding="utf-8") as f:
        json.dump(snap, f, indent=2, sort_keys=True)
    print(f"[guardian] Wrote baseline to {LOCK_FILE}")


def verify(conf: Dict[str, Any]) -> int:
    """
    Verify the current repo state against governance.lock.

    Rules:
    - All critical_files must exist and be >= min_file_size_bytes.
    - All protected_files from the baseline must still exist and not be trivially small.
    - If prevent_new_root_files is true, no new root-level files may appear
      (except governance.lock itself).
    - Hash changes in protected files are treated as failures; they should
      be followed by a manual freeze when intentionally updating the baseline.
    """
    if not LOCK_FILE.exists():
        print("[guardian] No governance.lock baseline; nothing to verify yet.")
        # Treat as success to avoid blocking installs before first freeze.
        return 0

    with LOCK_FILE.open("r", encoding="utf-8") as f:
        baseline = json.load(f)

    constraints: Dict[str, Any] = baseline.get("constraints") or conf.get("constraints", {}) or {}
    min_size = int(constraints.get("min_file_size_bytes", 0) or 0)
    prevent_new_root = bool(constraints.get("prevent_new_root_files", False))

    failures: List[str] = []

    # Critical files: existence + minimum size
    critical_files: List[str] = baseline.get("critical_files") or conf.get("critical_files", []) or []
    for rel in critical_files:
        p = REPO_ROOT / rel
        if not p.exists():
            failures.append(f"Critical file missing: {rel}")
        else:
            size = p.stat().st_size
            if size < min_size:
                failures.append(f"Critical file too small: {rel} ({size} bytes)")

    # Protected files: existence + minimum size + hash stability
    protected_files: Dict[str, Dict[str, Any]] = baseline.get("protected_files") or {}
    for rel, info in protected_files.items():
        p = REPO_ROOT / rel
        if not p.exists():
            failures.append(f"Protected file missing: {rel}")
            continue

        size = p.stat().st_size
        if size < min_size:
            failures.append(f"Protected file unexpectedly small: {rel} ({size} bytes)")

        old_hash = info.get("hash")
        if old_hash:
            new_hash = sha256_file(p)
            if new_hash != old_hash:
                failures.append(f"Protected file hash changed: {rel}")

    # Root-level file drift
    if prevent_new_root:
        baseline_root_files = set(baseline.get("root_files") or [])
        current_root_files = {p.name for p in REPO_ROOT.iterdir() if p.is_file()}
        extras = sorted(current_root_files - baseline_root_files)
        # governance.lock is expected and safe as a new root file
        extras = [name for name in extras if name not in {"governance.lock"}]
        if extras:
            failures.append(
                "New root-level files detected: " + ", ".join(extras)
            )

    if failures:
        print("[guardian] VERIFICATION FAILED:")
        for msg in failures:
            print(" -", msg)
        sys.exit(1)
    else:
        print("[guardian] Verification OK.")
        return 0


def main(argv: List[str]) -> int:
    if len(argv) < 2:
        print("Usage: python scripts/guardian.py [freeze|verify]")
        return 1

    mode = argv[1]
    conf = load_config()

    if mode == "freeze":
        freeze(conf)
        return 0
    elif mode == "verify":
        return verify(conf)
    else:
        print(f"Unknown mode: {mode}")
        return 1


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))----- CONTENT END -----
----- FILE PATH: archive/v3_2_26_phase5_docs_ux_ci/README_v3.md
----- CONTENT START -----
# Image Tagger v3.0 - Enterprise Edition

This is the production-ready, micro-frontend architecture for the Image Tagger system.

## üèóÔ∏è Architecture
* **Frontend:** Monorepo with 4 distinct React Apps (Workbench, Monitor, Admin, Explorer).
* **Backend:** Unified FastAPI service with PostgreSQL.
* **Infrastructure:** Docker Compose orchestration with Nginx Gateway.

## üöÄ Quick Start (The "Enterprise Go")

1.  **Ensure Docker is installed.**
2.  **Run:**
    ```bash
    cd deploy
    docker-compose up --build
    ```
3.  **Access the GUIs:**
    * **Research Explorer:** [http://localhost:8080/explorer](http://localhost:8080/explorer)
    * **Tagger Workbench:** [http://localhost:8080/workbench](http://localhost:8080/workbench)
    * **Supervisor Monitor:** [http://localhost:8080/monitor](http://localhost:8080/monitor)
    * **Admin Cockpit:** [http://localhost:8080/admin](http://localhost:8080/admin)

## üß™ Running Tests

To verify the API logic without Docker:
1.  `pip install pytest httpx`
2.  `pytest tests/test_v3_api.py`

## ü§ñ AI Collaboration Workflow

For guidelines on how to use LLMs (ChatGPT, Claude, Gemini, etc.) with this
repository ‚Äî including ZIP + concatenated TXT expectations and Guardian
governance rules ‚Äî see:

- `docs/AI_COLLAB_WORKFLOW.md`----- CONTENT END -----
----- FILE PATH: archive/v3_2_26_phase5_docs_ux_ci/frontend/apps/admin/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Admin | Image Tagger v3</title>
  </head>
  <body class="bg-gray-50 h-screen">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: archive/v3_2_26_phase5_docs_ux_ci/frontend/apps/explorer/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Explorer | Image Tagger v3</title>
  </head>
  <body class="bg-white h-screen overflow-hidden">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: archive/v3_2_26_phase5_docs_ux_ci/frontend/apps/monitor/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Supervisor | Image Tagger v3</title>
  </head>
  <body class="bg-gray-100 h-screen">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: archive/v3_2_26_phase5_docs_ux_ci/frontend/apps/workbench/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Workbench | Image Tagger v3</title>
  </head>
  <body class="bg-gray-100 h-screen overflow-hidden">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: archive/v3_2_27_science_deepening/backend/science/math/color.py
----- CONTENT START -----
"""Perceptual color analysis in CIELAB space."""

from __future__ import annotations

import numpy as np
from scipy.spatial import ConvexHull
from backend.science.core import AnalysisFrame


class ColorAnalyzer:
    """Extract color descriptors aligned with perceptual space.

    All outputs are normalized to [0, 1] where possible.
    """

    def __init__(self, max_samples: int = 4096, random_seed: int = 42):
        self.max_samples = max_samples
        self.random_seed = random_seed

    def analyze(self, frame: AnalysisFrame) -> None:
        lab = frame.ensure_lab()

        # L* channel in [0, 100]
        L = lab[:, :, 0]
        a = lab[:, :, 1]
        b = lab[:, :, 2]

        # 1. Perceptual lightness (mean L* scaled to [0, 1])
        mean_L = float(np.mean(L))
        lightness = np.clip(mean_L / 100.0, 0.0, 1.0)

        # 2. Lightness contrast (std L* normalized by a nominal max, e.g. 25)
        std_L = float(np.std(L))
        lightness_contrast = np.clip(std_L / 25.0, 0.0, 1.0)

        # 3. Warm‚Äìcool index: map mean a* and b* to [0, 1]
        mean_a = float(np.mean(a))
        mean_b = float(np.mean(b))
        warm_radius = np.sqrt(mean_a ** 2 + mean_b ** 2)
        warm_radius_norm = np.clip(warm_radius / 60.0, 0.0, 1.0)
        warm_sign = 0.0
        denom = abs(mean_a) + abs(mean_b)
        if warm_radius > 1e-6 and denom > 1e-6:
            warm_sign = (mean_a + mean_b) / denom
        warm_sign_norm = (warm_sign + 1.0) / 2.0
        warmth_index = 0.5 * warm_radius_norm + 0.5 * warm_sign_norm

        # 4. Gamut volume in a‚Äìb plane via convex hull on a subsample
        h, w = a.shape
        ab = np.stack([a.reshape(-1), b.reshape(-1)], axis=1)

        if ab.shape[0] > self.max_samples:
            rng = np.random.default_rng(self.random_seed)
            idx = rng.choice(ab.shape[0], size=self.max_samples, replace=False)
            ab_sample = ab[idx]
        else:
            ab_sample = ab

        try:
            hull = ConvexHull(ab_sample)
            raw_area = float(hull.volume)
            gamut_volume = np.clip(raw_area / (200.0 * 200.0), 0.0, 1.0)
        except Exception:
            gamut_volume = 0.0

        frame.set_attributes(
            {
                "color.lightness_mean": lightness,
                "color.lightness_contrast": lightness_contrast,
                "color.warmth_index": warmth_index,
                "color.gamut_volume": gamut_volume,
            }
        )----- CONTENT END -----
----- FILE PATH: archive/v3_2_27_science_deepening/backend/science/math/complexity.py
----- CONTENT START -----
"""Image complexity metrics: Shannon entropy, spatial entropy, edge density."""

from __future__ import annotations

import numpy as np
from scipy.stats import entropy
from skimage.feature import greycomatrix
from backend.science.core import AnalysisFrame


class ComplexityAnalyzer:
    """Compute basic complexity metrics on gray image and edges.

    All outputs are normalized roughly into [0, 1] for downstream BN use.
    """

    def __init__(self, glcm_levels: int = 32, glcm_downsample_max: int = 256):
        self.glcm_levels = glcm_levels
        self.glcm_downsample_max = glcm_downsample_max

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        edges = frame.ensure_edges()

        # Normalize gray to 0..1
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray

        # 1. Shannon entropy of gray histogram
        hist, _ = np.histogram(gray_norm, bins=64, range=(0.0, 1.0))
        if hist.sum() > 0:
            hist = hist.astype(np.float32) / float(hist.sum())
            shannon = float(entropy(hist, base=2))
            shannon_norm = np.clip(shannon / np.log2(64.0), 0.0, 1.0)
        else:
            shannon_norm = 0.0

        # 2. Spatial entropy using GLCM on downsampled image
        spatial_entropy_norm = self._spatial_entropy(gray_norm)

        # 3. Edge density
        edge_density = float((edges > 0).sum()) / float(edges.size) if edges.size > 0 else 0.0
        edge_density = float(np.clip(edge_density, 0.0, 1.0))

        frame.set_attributes(
            {
                "complexity.shannon_entropy": shannon_norm,
                "complexity.spatial_entropy": spatial_entropy_norm,
                "complexity.edge_density": edge_density,
            }
        )

    def _spatial_entropy(self, gray_norm: np.ndarray) -> float:
        h, w = gray_norm.shape
        # Downsample if needed to keep GLCM manageable
        factor = max(h, w) / float(self.glcm_downsample_max)
        if factor > 1.0:
            new_h = max(1, int(h / factor))
            new_w = max(1, int(w / factor))
            gray_ds = gray_norm[0:h:new_h, 0:w:new_w]
        else:
            gray_ds = gray_norm

        if gray_ds.size < 4:
            return 0.0

        # Quantize
        bins = self.glcm_levels
        quant = np.clip((gray_ds * (bins - 1)).astype(np.int32), 0, bins - 1)

        glcm = greycomatrix(
            quant,
            distances=[1],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=bins,
            symmetric=True,
            normed=True,
        )

        # Joint distribution over gray-level pairs
        P = glcm[:, :, 0, :]  # shape (bins, bins, angles)
        P = P.mean(axis=-1)    # average over angles
        P_flat = P.reshape(-1)
        P_flat = P_flat[P_flat > 0]
        if P_flat.size == 0:
            return 0.0
        H = float(entropy(P_flat, base=2))
        # maximum possible entropy is log2(bins^2)
        H_max = 2.0 * np.log2(float(bins))
        return float(np.clip(H / H_max, 0.0, 1.0))----- CONTENT END -----
----- FILE PATH: archive/v3_2_27_science_deepening/backend/science/math/fractals.py
----- CONTENT START -----
"""Fractal dimension estimates based on box-counting on edge maps."""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame


class FractalAnalyzer:
    """Estimate fractal dimension of edge structure.

    We use a simple box-counting method over dyadic box sizes.
    """

    def __init__(self, min_box_size: int = 2):
        self.min_box_size = min_box_size

    def analyze(self, frame: AnalysisFrame) -> None:
        edges = frame.ensure_edges()
        if edges.size == 0:
            return

        # binary mask
        Z = (edges > 0).astype(np.uint8)
        d_raw = self._box_count_dim(Z)
        # For typical natural scenes, D is in [1, 2]; map linearly to [0, 1]
        if np.isnan(d_raw):
            d_norm = 0.0
        else:
            d_norm = (d_raw - 1.0) / 1.0
            d_norm = float(np.clip(d_norm, 0.0, 1.0))

        frame.set_attribute("fractal.dimension", d_norm)

    def _box_count_dim(self, Z: np.ndarray) -> float:
        # Pad to square
        h, w = Z.shape
        n = max(h, w)
        # Next power of two
        n2 = 1 << (n - 1).bit_length()
        pad_h = n2 - h
        pad_w = n2 - w
        Z_padded = np.pad(Z, ((0, pad_h), (0, pad_w)), mode="constant", constant_values=0)

        sizes = []
        counts = []

        size = n2
        while size >= self.min_box_size:
            num = n2 // size
            if num == 0:
                break
            reshaped = Z_padded.reshape(num, size, num, size)
            reshaped = reshaped.swapaxes(1, 2)
            occupied = reshaped.max(axis=(-1, -2))
            count = int(np.count_nonzero(occupied))
            if count > 0:
                sizes.append(size)
                counts.append(count)
            size //= 2

        if len(sizes) < 2:
            return float("nan")

        sizes = np.array(sizes, dtype=np.float64)
        counts = np.array(counts, dtype=np.float64)

        logs = np.log(sizes)
        log_counts = np.log(counts)
        A = np.vstack([logs, np.ones_like(logs)]).T
        coeffs, *_ = np.linalg.lstsq(A, log_counts, rcond=None)
        slope = coeffs[0]
        return float(-slope)----- CONTENT END -----
----- FILE PATH: archive/v3_2_27_science_deepening/backend/science/math/glcm.py
----- CONTENT START -----
"""Texture metrics using GLCM at multiple scales."""

from __future__ import annotations

import numpy as np
from skimage.feature import greycomatrix, greycoprops
from backend.science.core import AnalysisFrame


class TextureAnalyzer:
    """Compute micro- and macro-texture metrics using GLCM.

    The outputs are aggregated across orientations and normalized for BN use.
    """

    def __init__(self, levels: int = 32, micro_distance: int = 1, macro_distance: int = 5):
        self.levels = levels
        self.micro_distance = micro_distance
        self.macro_distance = macro_distance

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        if gray.size == 0:
            return

        # Normalize gray to 0..1 then quantize
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray
        quant = np.clip((gray_norm * (self.levels - 1)).astype(np.int32), 0, self.levels - 1)

        micro = self._compute_props(quant, self.micro_distance)
        macro = self._compute_props(quant, self.macro_distance)

        frame.set_attributes(
            {
                "texture.micro.contrast": micro["contrast"],
                "texture.micro.homogeneity": micro["homogeneity"],
                "texture.micro.energy": micro["energy"],
                "texture.macro.contrast": macro["contrast"],
                "texture.macro.homogeneity": macro["homogeneity"],
                "texture.macro.energy": macro["energy"],
            }
        )

    def _compute_props(self, quant: np.ndarray, distance: int) -> dict:
        glcm = greycomatrix(
            quant,
            distances=[distance],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=self.levels,
            symmetric=True,
            normed=True,
        )
        props = {}
        for name in ("contrast", "homogeneity", "energy"):
            vals = greycoprops(glcm, name)
            mean_val = float(np.mean(vals))
            # simple normalization heuristics
            if name == "contrast":
                # contrast can be large; squash via 1 - exp(-x / c)
                c = 4.0
                props[name] = float(1.0 - np.exp(-mean_val / c))
            elif name == "energy":
                # energy already in [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
            else:
                # homogeneity is [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
        return props----- CONTENT END -----
----- FILE PATH: archive/v3_2_28_science_composites/README_v3.md
----- CONTENT START -----
# Image Tagger v3.2.27 (Enterprise) ‚Äì Multi-GUI HITL Tagging System

This repository hosts the **Image Tagger v3** backend + frontends used for
architecture / CNfA-style tagging workflows. It is designed to support:

- Human-in-the-loop (**HITL**) annotation for architectural images.
- A minimal but **truthful science pipeline** that computes basic visual metrics.
- Monitoring and governance for classroom and research deployments.

The system is split into **four GUIs**, all backed by a single FastAPI backend.

---

## 1. System Overview

### 1.1 Four GUIs and their roles

1. **Tagger Workbench** (worker GUI)
   - Target user: student / junior researcher taggers.
   - Purpose: fast, keyboard-driven binary tagging (one attribute question at a time).
   - Backend endpoints:
     - `/v1/annotation/*`

2. **Supervisor Monitor** (monitor GUI)
   - Target user: supervisors / TAs / lab managers.
   - Purpose: monitor team velocity and disagreement patterns.
   - Backend endpoints:
     - `/v1/monitor/velocity`
     - `/v1/monitor/irr`
     - `/v1/monitor/image/{image_id}/validations`

3. **Admin Cockpit** (admin GUI)
   - Target user: PI / course staff / system admins.
   - Purpose: configure tools, budgets, and export training data.
   - Backend endpoints:
     - `/v1/admin/models`
     - `/v1/admin/budget`
     - `/v1/admin/training/export`

4. **Research Explorer** (explorer GUI)
   - Target user: researchers / advanced students.
   - Purpose: search images by attributes and export training datasets.
   - Backend endpoints:
     - `/v1/explorer/attributes`
     - `/v1/explorer/search`
     - `/v1/explorer/export`

Each GUI is served as a separate SPA-style frontend under `frontend/apps/`.

---

## 2. Install and Run

### 2.1 One-command install

The recommended entry point is:

```bash
./install.sh
```

From a machine with **Docker** and **docker-compose** installed, this script will:

1. Build and start the Docker services defined in `deploy/docker-compose.yml`.
2. Run seed scripts inside the API container:
   - `backend/scripts/seed_tool_configs.py`
   - `backend/scripts/seed_attributes.py`
3. Run smoketests:
   - `python -m scripts.smoke_api` (API health and core routers).
   - `python -m scripts.smoke_science` (science pipeline + DB write).
4. Run a small pytest suite:
   - `pytest tests/test_v3_api.py`
   - `pytest tests/test_guardian.py` (optional in CI).

If any of these steps fail, `install.sh` will exit non-zero and print a
human-readable error message indicating which stage failed.

> **Note**: The install script will **not** silently ignore errors; it is
> deliberately opinionated so students notice when something is wrong.

### 2.2 Guardian integration

This repo ships with a light-weight governance layer:

- Config: `v3_governance.yml`
- Tool: `scripts/guardian.py`
- Baseline file: `governance.lock`

Typical usage:

```bash
# First time on a new clone (create baseline)
python3 scripts/guardian.py freeze

# Subsequent checks (ensure no drift)
python3 scripts/guardian.py verify
```

- `freeze()` creates or refreshes the baseline `governance.lock`.
- `verify()` checks:
  - All **critical files** exist and exceed a minimum size.
  - All **protected files** still exist and have unchanged hashes.
  - No unexpected root-level files appear when `prevent_new_root_files` is enabled.

If `governance.lock` does not yet exist, `verify()` returns success and logs
that there is no baseline. This avoids stranding first-time users.

---

## 3. Science Pipeline: ‚ÄúMinimum Viable Truthful‚Äù

The **science pipeline** in `backend/science/` is designed to be:

- Deterministic (no random outputs).
- Fast enough for classroom / batch use.
- Scientifically modest but honest.

### 3.1 What it does today

The pipeline operates on each `Image` and records attributes into the DB via
`Validation` rows (source = `"science_pipeline_v3.3"`). Current capabilities:

- **Color metrics** (`backend/science/math/color.py`)
  - Luminance / brightness summary.
  - Simple warm‚Äìcool index (channel balance).
  - Basic saturation measures.

- **Texture metrics** (`backend/science/math/glcm.py`)
  - GLCM-based contrast, homogeneity, and related texture stats.

- **Complexity / structure metrics** (`backend/science/math/complexity.py`)
  - Edge density and simple entropy-like measures.

- **Fractals** (`backend/science/math/fractals.py`)
  - Approximate fractal dimension, tuned for architectural images.

- **Materials heuristics** (`backend/science/vision/materials.py`)
  - HSV-rule heuristics for wood, vegetation, and related materials.

- **Context and perception** (`backend/science/context/*.py`, `backend/science/perception.py`)
  - Light-weight, rule-based proxies for social density and cognitive load.
  - These are explicitly labeled as *heuristics* in code.

The orchestrator is `backend/science/pipeline.py`:

- Loads image pixels via `Image.storage_path`.
- Creates an `AnalysisFrame`.
- Runs the analyzers above.
- Writes derived attributes as `Validation` rows.

The smoketest `scripts/smoke_science.py` verifies this end-to-end.

### 3.2 What is *not* promised

To keep the documentation honest:

- The current science stack **does not** claim:
  - Full VLM-based semantic understanding.
  - Production-grade depth maps or 3D reasoning.
- API stubs for more advanced science are present, but calls are either:
  - Guarded behind config flags, or
  - Implemented as clearly-marked deterministic heuristics.

Any future VLM / depth integration should:

- Be implemented behind explicit configuration.
- Update both the code and this README before being advertised in GUIs.

---

## 4. Monitor, Admin, Explorer ‚Äì Data Truthfulness

### 4.1 Supervisor Monitor

Monitor endpoints:

- `/v1/monitor/velocity`
  - Aggregates `Validation` rows per user over a recent time window.
  - Reports images validated, average dwell time (`duration_ms`), and inferred status.

- `/v1/monitor/irr`
  - Computes simple inter-rater reliability (IRR) based on overlapping validations.
  - Uses a conservative ‚Äúagreement ratio‚Äù between raters (exact value match).

- `/v1/monitor/image/{image_id}/validations`
  - Returns per-user validations for a specific image to drive the Tag Inspector.

These views are **live**: they read from the same `validations` table that
Tagger Workbench and the science pipeline use. If no data exists, views may be
empty but they are *not* backed by hard-coded demo arrays.

### 4.2 Admin & Explorer

- Admin Cockpit:
  - Lists and updates `ToolConfig` rows.
  - Exports training data using `backend/services/training_export.py`.

- Research Explorer:
  - Lists registered attributes (keys + descriptions).
  - Uses real DB-backed image search where possible.
  - Exports JSONL / JSON training sets via the training exporter.

---

## 5. CI: Example GitHub Actions Workflow

For teams using GitHub, this repo includes an example CI workflow at:

- `.github/workflows/ci.yml`

It performs the following checks on push / pull request:

1. Set up Python.
2. Install backend dependencies (via `requirements.txt`, if present).
3. Run Guardian verification:
   - `python scripts/guardian.py verify`
4. Run API and Guardian tests:
   - `pytest -q tests/test_v3_api.py tests/test_guardian.py`

You may adapt or expand this workflow to add linting, type-checking, or
end-to-end front-end tests.

---

## 6. Known Limitations (Truth-in-Advertising)

To avoid over-claiming:

- Science metrics are **heuristic and approximate**, suitable for:
  - Exploratory CNfA research.
  - Classroom projects and demos.
- VLM, depth, and other heavy models are:
  - Either not enabled by default, or
  - Present as stubs that should be clearly marked before use.
- The current CI workflow focuses on:
  - Governance (Guardian) and
  - Core API wiring tests.

If you extend science, RBAC, or front-end behavior, please:

1. Update `v3_governance.yml` if new critical paths are added.
2. Extend tests in `tests/` and, if appropriate, the CI workflow.
3. Refresh this README so students and collaborators have an accurate picture
   of what the system actually does.
----- CONTENT END -----
----- FILE PATH: archive/v3_2_28_science_composites/backend/science/pipeline.py
----- CONTENT START -----
"""Science pipeline orchestrator for Image Tagger v3.3."""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import numpy as np
from sqlalchemy.orm import Session

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer

logger = logging.getLogger(__name__)


class SciencePipelineConfig:
    """Runtime configuration flags for the science pipeline."""

    def __init__(
        self,
        enable_color: bool = True,
        enable_complexity: bool = True,
        enable_texture: bool = True,
        enable_fractals: bool = True,
        enable_spatial: bool = True,
        enable_cognitive: bool = False,
        image_root: str = "data_store",
    ) -> None:
        self.enable_color = enable_color
        self.enable_complexity = enable_complexity
        self.enable_texture = enable_texture
        self.enable_fractals = enable_fractals
        self.enable_spatial = enable_spatial
        self.enable_cognitive = enable_cognitive
        self.image_root = image_root


class SciencePipeline:
    """Orchestrates science analyzers over Image records."""

    def __init__(self, db: Session, config: Optional[SciencePipelineConfig] = None):
        self.db = db
        self.config = config or SciencePipelineConfig()
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.spatial = DepthAnalyzer()
        self.cognitive = CognitiveStateAnalyzer()

    def process_image(self, image_id: int) -> bool:
        """Run science analyses for a single image_id.

        Returns True on success, False on any fatal error. Partial attribute
        extraction is allowed; we commit whatever was computed.
        """
        image_record = self.db.query(Image).get(image_id)
        if image_record is None:
            logger.warning("SciencePipeline: image %s not found", image_id)
            return False

        rgb = self._load_image(image_record)
        if rgb is None:
            logger.warning("SciencePipeline: could not load pixels for %s", image_id)
            return False

        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            if self.config.enable_color:
                self.color.analyze(frame)
            if self.config.enable_complexity:
                self.complexity.analyze(frame)
            if self.config.enable_texture:
                self.texture.analyze(frame)
            if self.config.enable_fractals:
                self.fractals.analyze(frame)
            if self.config.enable_spatial:
                self.spatial.analyze(frame)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)
        except Exception:
            logger.exception("SciencePipeline: error while analyzing image %s", image_id)

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        """Load an RGB uint8 image for the given record.

        This implementation assumes a local file under config.image_root.
        In future we can replace this with a storage abstraction.
        """
        rel = Path(image_record.storage_path)
        path = Path(self.config.image_root) / rel
        if not path.exists():
            logger.error("SciencePipeline: file does not exist: %s", path)
            return None
        if cv2 is None:
            logger.error("SciencePipeline: cv2 not available, cannot load image")
            return None

        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            return None
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        return rgb

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=value,
                source="science_pipeline_v3.3",
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: archive/v3_2_29_workbench_responsive/frontend/apps/workbench/src/App.jsx
----- CONTENT START -----
import React, { useEffect, useState, useCallback } from 'react';
import { ApiClient, Button, Header } from '@shared';
import { AlertCircle, Zap, Keyboard, CheckCircle2, XCircle, HelpCircle } from 'lucide-react';

const api = new ApiClient('/api/v1/workbench');

export default function WorkbenchApp() {
    const [image, setImage] = useState(null);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [streak, setStreak] = useState(0);
    const [lastAction, setLastAction] = useState(null); // 'accept' or 'reject' for feedback animation

    useEffect(() => {
        loadNextImage();
    }, []);

    const loadNextImage = async () => {
        setLoading(true);
        setError(null);
        try {
            const data = await api.get('/next');
            setImage(data);
        } catch (err) {
            setError(err.message);
        } finally {
            setLoading(false);
        }
    };

    const handleDecision = async (value) => {
        if (!image) return;
        
        const currentId = image.id;
        const action = value === 1.0 ? 'accept' : 'reject';
        
        // 1. Optimistic UI Update
        setLastAction(action);
        setImage(null); 
        setLoading(true);
        setStreak(s => s + 1);

        // Clear animation trigger after short delay
        setTimeout(() => setLastAction(null), 500);

        try {
            // 2. Fire and Forget (in this simple version)
            await api.post('/validate', {
                image_id: currentId,
                attribute_key: "global.relevance", 
                value: value,
                duration_ms: 1200 
            });
            // 3. Fetch Next
            loadNextImage();
        } catch (err) {
            setError("Failed to save decision: " + err.message);
            setLoading(false);
            setStreak(0);
        }
    };

    const handleKeyDown = useCallback((event) => {
        if (event.key === '1') handleDecision(0.0);
        if (event.key === '2') handleDecision(1.0);
    }, [image]);

    useEffect(() => {
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [handleKeyDown]);

    return (
        <div className="flex flex-col h-screen bg-gray-50">
            <Header appName="Workbench" title="Tagger Station" />

{/* Quick Help */}
<div className="border-b border-blue-100 bg-blue-50 px-4 py-2 text-xs text-blue-900 flex items-center gap-2">
    <HelpCircle size={16} className="flex-shrink-0" />
    <span>
        One binary question at a time. Press <span className="font-semibold">1</span> for NO,
        <span className="font-semibold"> 2</span> for YES, or use the mouse buttons. Use <span className="font-semibold">Retry</span> if something looks wrong.
    </span>
</div>
            
            <div className="flex-1 flex overflow-hidden">
                {/* Main Canvas */}
                <main className="flex-1 relative bg-black flex items-center justify-center group">
                    {/* Loading State */}
                    {loading && !image && (
                        <div className="text-white/50 animate-pulse flex flex-col items-center gap-2">
                            <Zap size={32} />
                            <span>Fetching Task...</span>
                        </div>
                    )}
                    
                    {/* Error State */}
                    {error && (
                        <div className="absolute inset-0 bg-gray-900 z-50 flex flex-col items-center justify-center text-red-400">
                            <AlertCircle size={48} />
                            <p className="mt-4 font-bold text-xl">{error}</p>
                            <Button onClick={loadNextImage} variant="secondary" className="mt-6">Retry</Button>
                        </div>
                    )}

                    {/* Active Image */}
                    {image && !loading && (
                        <img 
                            src={image.url} 
                            alt="Tagging Target" 
                            className="max-w-full max-h-full object-contain shadow-2xl transition-transform duration-200"
                        />
                    )}

                    {/* Feedback Animation Overlay */}
                    {lastAction === 'accept' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-green-500/20 pointer-events-none animate-ping">
                            <CheckCircle2 size={128} className="text-green-400" />
                        </div>
                    )}
                    {lastAction === 'reject' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-red-500/20 pointer-events-none animate-ping">
                            <XCircle size={128} className="text-red-400" />
                        </div>
                    )}

                    {/* Floating Stats */}
                    <div className="absolute top-4 right-4 bg-black/50 backdrop-blur px-4 py-2 rounded-full text-white font-mono text-sm border border-white/20">
                        üî• Streak: {streak}
                    </div>
                </main>

                {/* Tool Sidebar */}
                <aside className="w-80 bg-white border-l border-gray-200 flex flex-col shadow-xl z-10">
                    <div className="p-6 border-b border-gray-100">
                        <h2 className="font-bold text-gray-800 text-sm uppercase tracking-wider">Current Task</h2>
                        <p className="text-lg font-medium text-gray-900 mt-2">Is this image "Modern"?</p>
                    </div>
                    
                    <div className="flex-1 p-6 flex flex-col gap-4">
                        <div className="bg-blue-50 p-4 rounded-lg border border-blue-100 text-blue-900 text-sm leading-relaxed">
                            <strong>Instructions:</strong> Look for clean lines, glass curtains, lack of ornament, and industrial materials.
                        </div>
                    </div>

                    <div className="p-6 border-t border-gray-200 bg-gray-50">
                        <div className="grid grid-cols-2 gap-4">
                            <Button onClick={() => handleDecision(0.0)} variant="danger" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">NO</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-black/10 px-2 py-1 rounded">Key: 1</span>
                            </Button>
                            <Button onClick={() => handleDecision(1.0)} variant="primary" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">YES</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-white/20 px-2 py-1 rounded">Key: 2</span>
                            </Button>
                        </div>
                        <div className="mt-4 flex items-center justify-center text-gray-400 text-xs gap-2">
                            <Keyboard size={14} />
                            <span>Shortcuts Active</span>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: archive/v3_2_30_auth_hardening/backend/services/auth.py
----- CONTENT START -----
from typing import Literal, Optional

from fastapi import Depends, Header, HTTPException, status
from pydantic import BaseModel


Role = Literal["tagger", "scientist", "supervisor", "admin"]


class CurrentUser(BaseModel):
    id: str
    role: Role


async def get_current_user(
    x_user_id: Optional[str] = Header(default=None, alias="X-User-Id"),
    x_user_role: Optional[str] = Header(default=None, alias="X-User-Role"),
) -> CurrentUser:
    """
    Minimal, header-based auth for v3.2 dev.

    This is intentionally simple so we can exercise RBAC and the
    different GUIs without a full identity provider. In production,
    replace with JWT / OAuth validation and a real User table lookup.
    """
    user_id = x_user_id or "1"
    role = (x_user_role or "tagger").lower()

    if role not in {"tagger", "scientist", "supervisor", "admin"}:
        # Default to least-privileged role if we get nonsense.
        role = "tagger"

    return CurrentUser(id=user_id, role=role)


def require_tagger(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Allow any authenticated role.

    Tagger, scientist, supervisor, and admin can all see Workbench/Explorer.
    """
    return user


def require_supervisor(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Require supervisor or admin role for monitoring endpoints."""
    if user.role not in {"supervisor", "admin"}:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Supervisor role required",
        )
    return user


def require_admin(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Require admin role for configuration and kill-switch endpoints."""
    if user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Admin role required",
        )
    return user----- CONTENT END -----
----- FILE PATH: archive/v3_2_31_science_composites/backend/science/pipeline.py
----- CONTENT START -----
"""Science pipeline orchestrator for Image Tagger v3.3."""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import numpy as np
from sqlalchemy.orm import Session

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer

logger = logging.getLogger(__name__)


class SciencePipelineConfig:
    """Runtime configuration flags for the science pipeline."""

    def __init__(
        self,
        enable_color: bool = True,
        enable_complexity: bool = True,
        enable_texture: bool = True,
        enable_fractals: bool = True,
        enable_spatial: bool = True,
        enable_cognitive: bool = False,
        image_root: str = "data_store",
    ) -> None:
        self.enable_color = enable_color
        self.enable_complexity = enable_complexity
        self.enable_texture = enable_texture
        self.enable_fractals = enable_fractals
        self.enable_spatial = enable_spatial
        self.enable_cognitive = enable_cognitive
        self.image_root = image_root


class SciencePipeline:
    """Orchestrates science analyzers over Image records."""

    def __init__(self, db: Session, config: Optional[SciencePipelineConfig] = None):
        self.db = db
        self.config = config or SciencePipelineConfig()
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.spatial = DepthAnalyzer()
        self.cognitive = CognitiveStateAnalyzer()

    def process_image(self, image_id: int) -> bool:
        """Run science analyses for a single image_id.

        Returns True on success, False on any fatal error. Partial attribute
        extraction is allowed; we commit whatever was computed.
        """
        image_record = self.db.query(Image).get(image_id)
        if image_record is None:
            logger.warning("SciencePipeline: image %s not found", image_id)
            return False

        rgb = self._load_image(image_record)
        if rgb is None:
            logger.warning("SciencePipeline: could not load pixels for %s", image_id)
            return False

        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            if self.config.enable_color:
                self.color.analyze(frame)
            if self.config.enable_complexity:
                self.complexity.analyze(frame)
            if self.config.enable_texture:
                self.texture.analyze(frame)
            if self.config.enable_fractals:
                self.fractals.analyze(frame)
            if self.config.enable_spatial:
                self.spatial.analyze(frame)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)
        except Exception:
            logger.exception("SciencePipeline: error while analyzing image %s", image_id)

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        """Load an RGB uint8 image for the given record.

        This implementation assumes a local file under config.image_root.
        In future we can replace this with a storage abstraction.
        """
        rel = Path(image_record.storage_path)
        path = Path(self.config.image_root) / rel
        if not path.exists():
            logger.error("SciencePipeline: file does not exist: %s", path)
            return None
        if cv2 is None:
            logger.error("SciencePipeline: cv2 not available, cannot load image")
            return None

        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            return None
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        return rgb

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=value,
                source="science_pipeline_v3.3",
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: archive/v3_2_31_science_composites/backend/science/math/color.py
----- CONTENT START -----
"""Perceptual color analysis in CIELAB space."""

from __future__ import annotations

import numpy as np
from scipy.spatial import ConvexHull
from backend.science.core import AnalysisFrame


class ColorAnalyzer:
    """Extract color descriptors aligned with perceptual space.

    All outputs are normalized to [0, 1] where possible.
    """

    def __init__(self, max_samples: int = 4096, random_seed: int = 42):
        self.max_samples = max_samples
        self.random_seed = random_seed

    def analyze(self, frame: AnalysisFrame) -> None:
        lab = frame.ensure_lab()

        # L* channel in [0, 100]
        L = lab[:, :, 0]
        a = lab[:, :, 1]
        b = lab[:, :, 2]

        # 1. Perceptual lightness (mean L* scaled to [0, 1])
        mean_L = float(np.mean(L))
        lightness = np.clip(mean_L / 100.0, 0.0, 1.0)

        # 2. Lightness contrast (std L* normalized by a nominal max, e.g. 25)
        std_L = float(np.std(L))
        lightness_contrast = np.clip(std_L / 25.0, 0.0, 1.0)

        # 3. Warm‚Äìcool index: map mean a* and b* to [0, 1]
        mean_a = float(np.mean(a))
        mean_b = float(np.mean(b))
        warm_radius = np.sqrt(mean_a ** 2 + mean_b ** 2)
        warm_radius_norm = np.clip(warm_radius / 60.0, 0.0, 1.0)
        warm_sign = 0.0
        denom = abs(mean_a) + abs(mean_b)
        if warm_radius > 1e-6 and denom > 1e-6:
            warm_sign = (mean_a + mean_b) / denom
        warm_sign_norm = (warm_sign + 1.0) / 2.0
        warmth_index = 0.5 * warm_radius_norm + 0.5 * warm_sign_norm

        # 4. Gamut volume in a‚Äìb plane via convex hull on a subsample
        h, w = a.shape
        ab = np.stack([a.reshape(-1), b.reshape(-1)], axis=1)

        if ab.shape[0] > self.max_samples:
            rng = np.random.default_rng(self.random_seed)
            idx = rng.choice(ab.shape[0], size=self.max_samples, replace=False)
            ab_sample = ab[idx]
        else:
            ab_sample = ab

        try:
            hull = ConvexHull(ab_sample)
            raw_area = float(hull.volume)
            gamut_volume = np.clip(raw_area / (200.0 * 200.0), 0.0, 1.0)
        except Exception:
            gamut_volume = 0.0

        frame.set_attributes(
            {
                "color.lightness_mean": lightness,
                "color.lightness_contrast": lightness_contrast,
                "color.warmth_index": warmth_index,
                "color.gamut_volume": gamut_volume,
            }
        )----- CONTENT END -----
----- FILE PATH: archive/v3_2_31_science_composites/backend/science/math/complexity.py
----- CONTENT START -----
"""Image complexity metrics: Shannon entropy, spatial entropy, edge density."""

from __future__ import annotations

import numpy as np
from scipy.stats import entropy
from skimage.feature import greycomatrix
from backend.science.core import AnalysisFrame


class ComplexityAnalyzer:
    """Compute basic complexity metrics on gray image and edges.

    All outputs are normalized roughly into [0, 1] for downstream BN use.
    """

    def __init__(self, glcm_levels: int = 32, glcm_downsample_max: int = 256):
        self.glcm_levels = glcm_levels
        self.glcm_downsample_max = glcm_downsample_max

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        edges = frame.ensure_edges()

        # Normalize gray to 0..1
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray

        # 1. Shannon entropy of gray histogram
        hist, _ = np.histogram(gray_norm, bins=64, range=(0.0, 1.0))
        if hist.sum() > 0:
            hist = hist.astype(np.float32) / float(hist.sum())
            shannon = float(entropy(hist, base=2))
            shannon_norm = np.clip(shannon / np.log2(64.0), 0.0, 1.0)
        else:
            shannon_norm = 0.0

        # 2. Spatial entropy using GLCM on downsampled image
        spatial_entropy_norm = self._spatial_entropy(gray_norm)

        # 3. Edge density
        edge_density = float((edges > 0).sum()) / float(edges.size) if edges.size > 0 else 0.0
        edge_density = float(np.clip(edge_density, 0.0, 1.0))

        frame.set_attributes(
            {
                "complexity.shannon_entropy": shannon_norm,
                "complexity.spatial_entropy": spatial_entropy_norm,
                "complexity.edge_density": edge_density,
            }
        )

    def _spatial_entropy(self, gray_norm: np.ndarray) -> float:
        h, w = gray_norm.shape
        # Downsample if needed to keep GLCM manageable
        factor = max(h, w) / float(self.glcm_downsample_max)
        if factor > 1.0:
            new_h = max(1, int(h / factor))
            new_w = max(1, int(w / factor))
            gray_ds = gray_norm[0:h:new_h, 0:w:new_w]
        else:
            gray_ds = gray_norm

        if gray_ds.size < 4:
            return 0.0

        # Quantize
        bins = self.glcm_levels
        quant = np.clip((gray_ds * (bins - 1)).astype(np.int32), 0, bins - 1)

        glcm = greycomatrix(
            quant,
            distances=[1],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=bins,
            symmetric=True,
            normed=True,
        )

        # Joint distribution over gray-level pairs
        P = glcm[:, :, 0, :]  # shape (bins, bins, angles)
        P = P.mean(axis=-1)    # average over angles
        P_flat = P.reshape(-1)
        P_flat = P_flat[P_flat > 0]
        if P_flat.size == 0:
            return 0.0
        H = float(entropy(P_flat, base=2))
        # maximum possible entropy is log2(bins^2)
        H_max = 2.0 * np.log2(float(bins))
        return float(np.clip(H / H_max, 0.0, 1.0))----- CONTENT END -----
----- FILE PATH: archive/v3_2_31_science_composites/backend/science/math/glcm.py
----- CONTENT START -----
"""Texture metrics using GLCM at multiple scales."""

from __future__ import annotations

import numpy as np
from skimage.feature import greycomatrix, greycoprops
from backend.science.core import AnalysisFrame


class TextureAnalyzer:
    """Compute micro- and macro-texture metrics using GLCM.

    The outputs are aggregated across orientations and normalized for BN use.
    """

    def __init__(self, levels: int = 32, micro_distance: int = 1, macro_distance: int = 5):
        self.levels = levels
        self.micro_distance = micro_distance
        self.macro_distance = macro_distance

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        if gray.size == 0:
            return

        # Normalize gray to 0..1 then quantize
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray
        quant = np.clip((gray_norm * (self.levels - 1)).astype(np.int32), 0, self.levels - 1)

        micro = self._compute_props(quant, self.micro_distance)
        macro = self._compute_props(quant, self.macro_distance)

        frame.set_attributes(
            {
                "texture.micro.contrast": micro["contrast"],
                "texture.micro.homogeneity": micro["homogeneity"],
                "texture.micro.energy": micro["energy"],
                "texture.macro.contrast": macro["contrast"],
                "texture.macro.homogeneity": macro["homogeneity"],
                "texture.macro.energy": macro["energy"],
            }
        )

    def _compute_props(self, quant: np.ndarray, distance: int) -> dict:
        glcm = greycomatrix(
            quant,
            distances=[distance],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=self.levels,
            symmetric=True,
            normed=True,
        )
        props = {}
        for name in ("contrast", "homogeneity", "energy"):
            vals = greycoprops(glcm, name)
            mean_val = float(np.mean(vals))
            # simple normalization heuristics
            if name == "contrast":
                # contrast can be large; squash via 1 - exp(-x / c)
                c = 4.0
                props[name] = float(1.0 - np.exp(-mean_val / c))
            elif name == "energy":
                # energy already in [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
            else:
                # homogeneity is [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
        return props----- CONTENT END -----
----- FILE PATH: archive/v3_2_31_science_composites_fractals/backend/science/math/fractals.py
----- CONTENT START -----
"""Fractal dimension estimates based on box-counting on edge maps."""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame


class FractalAnalyzer:
    """Estimate fractal dimension of edge structure.

    We use a simple box-counting method over dyadic box sizes.
    """

    def __init__(self, min_box_size: int = 2):
        self.min_box_size = min_box_size

    def analyze(self, frame: AnalysisFrame) -> None:
        edges = frame.ensure_edges()
        if edges.size == 0:
            return

        # binary mask
        Z = (edges > 0).astype(np.uint8)
        d_raw = self._box_count_dim(Z)
        # For typical natural scenes, D is in [1, 2]; map linearly to [0, 1]
        if np.isnan(d_raw):
            d_norm = 0.0
        else:
            d_norm = (d_raw - 1.0) / 1.0
            d_norm = float(np.clip(d_norm, 0.0, 1.0))

        frame.set_attribute("fractal.dimension", d_norm)

    def _box_count_dim(self, Z: np.ndarray) -> float:
        # Pad to square
        h, w = Z.shape
        n = max(h, w)
        # Next power of two
        n2 = 1 << (n - 1).bit_length()
        pad_h = n2 - h
        pad_w = n2 - w
        Z_padded = np.pad(Z, ((0, pad_h), (0, pad_w)), mode="constant", constant_values=0)

        sizes = []
        counts = []

        size = n2
        while size >= self.min_box_size:
            num = n2 // size
            if num == 0:
                break
            reshaped = Z_padded.reshape(num, size, num, size)
            reshaped = reshaped.swapaxes(1, 2)
            occupied = reshaped.max(axis=(-1, -2))
            count = int(np.count_nonzero(occupied))
            if count > 0:
                sizes.append(size)
                counts.append(count)
            size //= 2

        if len(sizes) < 2:
            return float("nan")

        sizes = np.array(sizes, dtype=np.float64)
        counts = np.array(counts, dtype=np.float64)

        logs = np.log(sizes)
        log_counts = np.log(counts)
        A = np.vstack([logs, np.ones_like(logs)]).T
        coeffs, *_ = np.linalg.lstsq(A, log_counts, rcond=None)
        slope = coeffs[0]
        return float(-slope)----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_auth_hardening/backend/services/auth.py
----- CONTENT START -----
from typing import Literal, Optional

from fastapi import Depends, Header, HTTPException, status
from pydantic import BaseModel


Role = Literal["tagger", "scientist", "supervisor", "admin"]


class CurrentUser(BaseModel):
    id: str
    role: Role


async def get_current_user(
    x_user_id: Optional[str] = Header(default=None, alias="X-User-Id"),
    x_user_role: Optional[str] = Header(default=None, alias="X-User-Role"),
) -> CurrentUser:
    """
    Minimal, header-based auth for v3.2 dev.

    This is intentionally simple so we can exercise RBAC and the
    different GUIs without a full identity provider. In production,
    replace with JWT / OAuth validation and a real User table lookup.
    """
    user_id = x_user_id or "1"
    role = (x_user_role or "tagger").lower()

    if role not in {"tagger", "scientist", "supervisor", "admin"}:
        # Default to least-privileged role if we get nonsense.
        role = "tagger"

    return CurrentUser(id=user_id, role=role)


def require_tagger(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Allow any authenticated role.

    Tagger, scientist, supervisor, and admin can all see Workbench/Explorer.
    """
    return user


def require_supervisor(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Require supervisor or admin role for monitoring endpoints."""
    if user.role not in {"supervisor", "admin"}:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Supervisor role required",
        )
    return user


def require_admin(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    """Require admin role for configuration and kill-switch endpoints."""
    if user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Admin role required",
        )
    return user----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_science_composites/backend/science/pipeline.py
----- CONTENT START -----
"""Science pipeline orchestrator for Image Tagger v3.3."""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import numpy as np
from sqlalchemy.orm import Session

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer

logger = logging.getLogger(__name__)


class SciencePipelineConfig:
    """Runtime configuration flags for the science pipeline."""

    def __init__(
        self,
        enable_color: bool = True,
        enable_complexity: bool = True,
        enable_texture: bool = True,
        enable_fractals: bool = True,
        enable_spatial: bool = True,
        enable_cognitive: bool = False,
        image_root: str = "data_store",
    ) -> None:
        self.enable_color = enable_color
        self.enable_complexity = enable_complexity
        self.enable_texture = enable_texture
        self.enable_fractals = enable_fractals
        self.enable_spatial = enable_spatial
        self.enable_cognitive = enable_cognitive
        self.image_root = image_root


class SciencePipeline:
    """Orchestrates science analyzers over Image records."""

    def __init__(self, db: Session, config: Optional[SciencePipelineConfig] = None):
        self.db = db
        self.config = config or SciencePipelineConfig()
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.spatial = DepthAnalyzer()
        self.cognitive = CognitiveStateAnalyzer()

    def process_image(self, image_id: int) -> bool:
        """Run science analyses for a single image_id.

        Returns True on success, False on any fatal error. Partial attribute
        extraction is allowed; we commit whatever was computed.
        """
        image_record = self.db.query(Image).get(image_id)
        if image_record is None:
            logger.warning("SciencePipeline: image %s not found", image_id)
            return False

        rgb = self._load_image(image_record)
        if rgb is None:
            logger.warning("SciencePipeline: could not load pixels for %s", image_id)
            return False

        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            if self.config.enable_color:
                self.color.analyze(frame)
            if self.config.enable_complexity:
                self.complexity.analyze(frame)
            if self.config.enable_texture:
                self.texture.analyze(frame)
            if self.config.enable_fractals:
                self.fractals.analyze(frame)
            if self.config.enable_spatial:
                self.spatial.analyze(frame)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)
        except Exception:
            logger.exception("SciencePipeline: error while analyzing image %s", image_id)

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        """Load an RGB uint8 image for the given record.

        This implementation assumes a local file under config.image_root.
        In future we can replace this with a storage abstraction.
        """
        rel = Path(image_record.storage_path)
        path = Path(self.config.image_root) / rel
        if not path.exists():
            logger.error("SciencePipeline: file does not exist: %s", path)
            return None
        if cv2 is None:
            logger.error("SciencePipeline: cv2 not available, cannot load image")
            return None

        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            return None
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        return rgb

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=value,
                source="science_pipeline_v3.3",
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_science_composites/backend/science/math/color.py
----- CONTENT START -----
"""Perceptual color analysis in CIELAB space."""

from __future__ import annotations

import numpy as np
from scipy.spatial import ConvexHull
from backend.science.core import AnalysisFrame


class ColorAnalyzer:
    """Extract color descriptors aligned with perceptual space.

    All outputs are normalized to [0, 1] where possible.
    """

    def __init__(self, max_samples: int = 4096, random_seed: int = 42):
        self.max_samples = max_samples
        self.random_seed = random_seed

    def analyze(self, frame: AnalysisFrame) -> None:
        lab = frame.ensure_lab()

        # L* channel in [0, 100]
        L = lab[:, :, 0]
        a = lab[:, :, 1]
        b = lab[:, :, 2]

        # 1. Perceptual lightness (mean L* scaled to [0, 1])
        mean_L = float(np.mean(L))
        lightness = np.clip(mean_L / 100.0, 0.0, 1.0)

        # 2. Lightness contrast (std L* normalized by a nominal max, e.g. 25)
        std_L = float(np.std(L))
        lightness_contrast = np.clip(std_L / 25.0, 0.0, 1.0)

        # 3. Warm‚Äìcool index: map mean a* and b* to [0, 1]
        mean_a = float(np.mean(a))
        mean_b = float(np.mean(b))
        warm_radius = np.sqrt(mean_a ** 2 + mean_b ** 2)
        warm_radius_norm = np.clip(warm_radius / 60.0, 0.0, 1.0)
        warm_sign = 0.0
        denom = abs(mean_a) + abs(mean_b)
        if warm_radius > 1e-6 and denom > 1e-6:
            warm_sign = (mean_a + mean_b) / denom
        warm_sign_norm = (warm_sign + 1.0) / 2.0
        warmth_index = 0.5 * warm_radius_norm + 0.5 * warm_sign_norm

        # 4. Gamut volume in a‚Äìb plane via convex hull on a subsample
        h, w = a.shape
        ab = np.stack([a.reshape(-1), b.reshape(-1)], axis=1)

        if ab.shape[0] > self.max_samples:
            rng = np.random.default_rng(self.random_seed)
            idx = rng.choice(ab.shape[0], size=self.max_samples, replace=False)
            ab_sample = ab[idx]
        else:
            ab_sample = ab

        try:
            hull = ConvexHull(ab_sample)
            raw_area = float(hull.volume)
            gamut_volume = np.clip(raw_area / (200.0 * 200.0), 0.0, 1.0)
        except Exception:
            gamut_volume = 0.0

        frame.set_attributes(
            {
                "color.lightness_mean": lightness,
                "color.lightness_contrast": lightness_contrast,
                "color.warmth_index": warmth_index,
                "color.gamut_volume": gamut_volume,
            }
        )----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_science_composites/backend/science/math/complexity.py
----- CONTENT START -----
"""Image complexity metrics: Shannon entropy, spatial entropy, edge density."""

from __future__ import annotations

import numpy as np
from scipy.stats import entropy
from skimage.feature import greycomatrix
from backend.science.core import AnalysisFrame


class ComplexityAnalyzer:
    """Compute basic complexity metrics on gray image and edges.

    All outputs are normalized roughly into [0, 1] for downstream BN use.
    """

    def __init__(self, glcm_levels: int = 32, glcm_downsample_max: int = 256):
        self.glcm_levels = glcm_levels
        self.glcm_downsample_max = glcm_downsample_max

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        edges = frame.ensure_edges()

        # Normalize gray to 0..1
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray

        # 1. Shannon entropy of gray histogram
        hist, _ = np.histogram(gray_norm, bins=64, range=(0.0, 1.0))
        if hist.sum() > 0:
            hist = hist.astype(np.float32) / float(hist.sum())
            shannon = float(entropy(hist, base=2))
            shannon_norm = np.clip(shannon / np.log2(64.0), 0.0, 1.0)
        else:
            shannon_norm = 0.0

        # 2. Spatial entropy using GLCM on downsampled image
        spatial_entropy_norm = self._spatial_entropy(gray_norm)

        # 3. Edge density
        edge_density = float((edges > 0).sum()) / float(edges.size) if edges.size > 0 else 0.0
        edge_density = float(np.clip(edge_density, 0.0, 1.0))

        frame.set_attributes(
            {
                "complexity.shannon_entropy": shannon_norm,
                "complexity.spatial_entropy": spatial_entropy_norm,
                "complexity.edge_density": edge_density,
            }
        )

    def _spatial_entropy(self, gray_norm: np.ndarray) -> float:
        h, w = gray_norm.shape
        # Downsample if needed to keep GLCM manageable
        factor = max(h, w) / float(self.glcm_downsample_max)
        if factor > 1.0:
            new_h = max(1, int(h / factor))
            new_w = max(1, int(w / factor))
            gray_ds = gray_norm[0:h:new_h, 0:w:new_w]
        else:
            gray_ds = gray_norm

        if gray_ds.size < 4:
            return 0.0

        # Quantize
        bins = self.glcm_levels
        quant = np.clip((gray_ds * (bins - 1)).astype(np.int32), 0, bins - 1)

        glcm = greycomatrix(
            quant,
            distances=[1],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=bins,
            symmetric=True,
            normed=True,
        )

        # Joint distribution over gray-level pairs
        P = glcm[:, :, 0, :]  # shape (bins, bins, angles)
        P = P.mean(axis=-1)    # average over angles
        P_flat = P.reshape(-1)
        P_flat = P_flat[P_flat > 0]
        if P_flat.size == 0:
            return 0.0
        H = float(entropy(P_flat, base=2))
        # maximum possible entropy is log2(bins^2)
        H_max = 2.0 * np.log2(float(bins))
        return float(np.clip(H / H_max, 0.0, 1.0))----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_science_composites/backend/science/math/glcm.py
----- CONTENT START -----
"""Texture metrics using GLCM at multiple scales."""

from __future__ import annotations

import numpy as np
from skimage.feature import greycomatrix, greycoprops
from backend.science.core import AnalysisFrame


class TextureAnalyzer:
    """Compute micro- and macro-texture metrics using GLCM.

    The outputs are aggregated across orientations and normalized for BN use.
    """

    def __init__(self, levels: int = 32, micro_distance: int = 1, macro_distance: int = 5):
        self.levels = levels
        self.micro_distance = micro_distance
        self.macro_distance = macro_distance

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        if gray.size == 0:
            return

        # Normalize gray to 0..1 then quantize
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray
        quant = np.clip((gray_norm * (self.levels - 1)).astype(np.int32), 0, self.levels - 1)

        micro = self._compute_props(quant, self.micro_distance)
        macro = self._compute_props(quant, self.macro_distance)

        frame.set_attributes(
            {
                "texture.micro.contrast": micro["contrast"],
                "texture.micro.homogeneity": micro["homogeneity"],
                "texture.micro.energy": micro["energy"],
                "texture.macro.contrast": macro["contrast"],
                "texture.macro.homogeneity": macro["homogeneity"],
                "texture.macro.energy": macro["energy"],
            }
        )

    def _compute_props(self, quant: np.ndarray, distance: int) -> dict:
        glcm = greycomatrix(
            quant,
            distances=[distance],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=self.levels,
            symmetric=True,
            normed=True,
        )
        props = {}
        for name in ("contrast", "homogeneity", "energy"):
            vals = greycoprops(glcm, name)
            mean_val = float(np.mean(vals))
            # simple normalization heuristics
            if name == "contrast":
                # contrast can be large; squash via 1 - exp(-x / c)
                c = 4.0
                props[name] = float(1.0 - np.exp(-mean_val / c))
            elif name == "energy":
                # energy already in [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
            else:
                # homogeneity is [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
        return props----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_science_composites_fractals/backend/science/math/fractals.py
----- CONTENT START -----
"""Fractal dimension estimates based on box-counting on edge maps."""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame


class FractalAnalyzer:
    """Estimate fractal dimension of edge structure.

    We use a simple box-counting method over dyadic box sizes.
    """

    def __init__(self, min_box_size: int = 2):
        self.min_box_size = min_box_size

    def analyze(self, frame: AnalysisFrame) -> None:
        edges = frame.ensure_edges()
        if edges.size == 0:
            return

        # binary mask
        Z = (edges > 0).astype(np.uint8)
        d_raw = self._box_count_dim(Z)
        # For typical natural scenes, D is in [1, 2]; map linearly to [0, 1]
        if np.isnan(d_raw):
            d_norm = 0.0
        else:
            d_norm = (d_raw - 1.0) / 1.0
            d_norm = float(np.clip(d_norm, 0.0, 1.0))

        frame.set_attribute("fractal.dimension", d_norm)

    def _box_count_dim(self, Z: np.ndarray) -> float:
        # Pad to square
        h, w = Z.shape
        n = max(h, w)
        # Next power of two
        n2 = 1 << (n - 1).bit_length()
        pad_h = n2 - h
        pad_w = n2 - w
        Z_padded = np.pad(Z, ((0, pad_h), (0, pad_w)), mode="constant", constant_values=0)

        sizes = []
        counts = []

        size = n2
        while size >= self.min_box_size:
            num = n2 // size
            if num == 0:
                break
            reshaped = Z_padded.reshape(num, size, num, size)
            reshaped = reshaped.swapaxes(1, 2)
            occupied = reshaped.max(axis=(-1, -2))
            count = int(np.count_nonzero(occupied))
            if count > 0:
                sizes.append(size)
                counts.append(count)
            size //= 2

        if len(sizes) < 2:
            return float("nan")

        sizes = np.array(sizes, dtype=np.float64)
        counts = np.array(counts, dtype=np.float64)

        logs = np.log(sizes)
        log_counts = np.log(counts)
        A = np.vstack([logs, np.ones_like(logs)]).T
        coeffs, *_ = np.linalg.lstsq(A, log_counts, rcond=None)
        slope = coeffs[0]
        return float(-slope)----- CONTENT END -----
----- FILE PATH: archive/v3_2_32_workbench_responsive/frontend/apps/workbench/src/App.jsx
----- CONTENT START -----
import React, { useEffect, useState, useCallback } from 'react';
import { ApiClient, Button, Header } from '@shared';
import { AlertCircle, Zap, Keyboard, CheckCircle2, XCircle, HelpCircle } from 'lucide-react';

const api = new ApiClient('/api/v1/workbench');

export default function WorkbenchApp() {
    const [image, setImage] = useState(null);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [streak, setStreak] = useState(0);
    const [lastAction, setLastAction] = useState(null); // 'accept' or 'reject' for feedback animation

    useEffect(() => {
        loadNextImage();
    }, []);

    const loadNextImage = async () => {
        setLoading(true);
        setError(null);
        try {
            const data = await api.get('/next');
            setImage(data);
        } catch (err) {
            setError(err.message);
        } finally {
            setLoading(false);
        }
    };

    const handleDecision = async (value) => {
        if (!image) return;
        
        const currentId = image.id;
        const action = value === 1.0 ? 'accept' : 'reject';
        
        // 1. Optimistic UI Update
        setLastAction(action);
        setImage(null); 
        setLoading(true);
        setStreak(s => s + 1);

        // Clear animation trigger after short delay
        setTimeout(() => setLastAction(null), 500);

        try {
            // 2. Fire and Forget (in this simple version)
            await api.post('/validate', {
                image_id: currentId,
                attribute_key: "global.relevance", 
                value: value,
                duration_ms: 1200 
            });
            // 3. Fetch Next
            loadNextImage();
        } catch (err) {
            setError("Failed to save decision: " + err.message);
            setLoading(false);
            setStreak(0);
        }
    };

    const handleKeyDown = useCallback((event) => {
        if (event.key === '1') handleDecision(0.0);
        if (event.key === '2') handleDecision(1.0);
    }, [image]);

    useEffect(() => {
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [handleKeyDown]);

    return (
        <div className="flex flex-col h-screen bg-gray-50">
            <Header appName="Workbench" title="Tagger Station" />

{/* Quick Help */}
<div className="border-b border-blue-100 bg-blue-50 px-4 py-2 text-xs text-blue-900 flex items-center gap-2">
    <HelpCircle size={16} className="flex-shrink-0" />
    <span>
        One binary question at a time. Press <span className="font-semibold">1</span> for NO,
        <span className="font-semibold"> 2</span> for YES, or use the mouse buttons. Use <span className="font-semibold">Retry</span> if something looks wrong.
    </span>
</div>
            
            <div className="flex-1 flex overflow-hidden">
                {/* Main Canvas */}
                <main className="flex-1 relative bg-black flex items-center justify-center group">
                    {/* Loading State */}
                    {loading && !image && (
                        <div className="text-white/50 animate-pulse flex flex-col items-center gap-2">
                            <Zap size={32} />
                            <span>Fetching Task...</span>
                        </div>
                    )}
                    
                    {/* Error State */}
                    {error && (
                        <div className="absolute inset-0 bg-gray-900 z-50 flex flex-col items-center justify-center text-red-400">
                            <AlertCircle size={48} />
                            <p className="mt-4 font-bold text-xl">{error}</p>
                            <Button onClick={loadNextImage} variant="secondary" className="mt-6">Retry</Button>
                        </div>
                    )}

                    {/* Active Image */}
                    {image && !loading && (
                        <img 
                            src={image.url} 
                            alt="Tagging Target" 
                            className="max-w-full max-h-full object-contain shadow-2xl transition-transform duration-200"
                        />
                    )}

                    {/* Feedback Animation Overlay */}
                    {lastAction === 'accept' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-green-500/20 pointer-events-none animate-ping">
                            <CheckCircle2 size={128} className="text-green-400" />
                        </div>
                    )}
                    {lastAction === 'reject' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-red-500/20 pointer-events-none animate-ping">
                            <XCircle size={128} className="text-red-400" />
                        </div>
                    )}

                    {/* Floating Stats */}
                    <div className="absolute top-4 right-4 bg-black/50 backdrop-blur px-4 py-2 rounded-full text-white font-mono text-sm border border-white/20">
                        üî• Streak: {streak}
                    </div>
                </main>

                {/* Tool Sidebar */}
                <aside className="w-80 bg-white border-l border-gray-200 flex flex-col shadow-xl z-10">
                    <div className="p-6 border-b border-gray-100">
                        <h2 className="font-bold text-gray-800 text-sm uppercase tracking-wider">Current Task</h2>
                        <p className="text-lg font-medium text-gray-900 mt-2">Is this image "Modern"?</p>
                    </div>
                    
                    <div className="flex-1 p-6 flex flex-col gap-4">
                        <div className="bg-blue-50 p-4 rounded-lg border border-blue-100 text-blue-900 text-sm leading-relaxed">
                            <strong>Instructions:</strong> Look for clean lines, glass curtains, lack of ornament, and industrial materials.
                        </div>
                    </div>

                    <div className="p-6 border-t border-gray-200 bg-gray-50">
                        <div className="grid grid-cols-2 gap-4">
                            <Button onClick={() => handleDecision(0.0)} variant="danger" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">NO</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-black/10 px-2 py-1 rounded">Key: 1</span>
                            </Button>
                            <Button onClick={() => handleDecision(1.0)} variant="primary" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">YES</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-white/20 px-2 py-1 rounded">Key: 2</span>
                            </Button>
                        </div>
                        <div className="mt-4 flex items-center justify-center text-gray-400 text-xs gap-2">
                            <Keyboard size={14} />
                            <span>Shortcuts Active</span>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/core.py
----- CONTENT START -----
import numpy as np
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

try:
    import cv2
except ImportError:  # pragma: no cover - dependency checked at runtime
    cv2 = None

try:
    from skimage.color import rgb2lab
except ImportError:  # pragma: no cover
    rgb2lab = None


@dataclass
class AnalysisFrame:
    """Central data structure for the v3.3 science pipeline.

    This wraps a single RGB image and caches common derived
    representations used by the analyzers (gray, LAB, edges, depth).
    Additional attributes and metadata are accumulated as the pipeline runs.
    """

    image_id: int
    original_image: np.ndarray  # RGB uint8, shape (H, W, 3)

    gray_image: Optional[np.ndarray] = None
    lab_image: Optional[np.ndarray] = None
    edges: Optional[np.ndarray] = None

    depth_map: Optional[np.ndarray] = None
    semantic_segmentation: Optional[np.ndarray] = None

    attributes: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def ensure_gray(self) -> np.ndarray:
        """Return a grayscale version of the image, computing if necessary."""
        if self.gray_image is None:
            if cv2 is None:
                # simple luminance formula as a fallback
                rgb = self.original_image.astype(np.float32) / 255.0
                r = rgb[:, :, 0]
                g = rgb[:, :, 1]
                b = rgb[:, :, 2]
                self.gray_image = (
                    0.2126 * r
                    + 0.7152 * g
                    + 0.0722 * b
                ).astype(np.float32)
            else:
                self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_RGB2GRAY)
        return self.gray_image

    def ensure_lab(self) -> np.ndarray:
        """Return CIELAB representation, computing if necessary.

        We prefer skimage.color.rgb2lab; if unavailable we approximate
        using a simple linear mapping from RGB to L and zero a/b.
        """
        if self.lab_image is None:
            rgb = self.original_image.astype(np.float32) / 255.0
            if rgb2lab is not None:
                lab = rgb2lab(rgb)
                self.lab_image = lab.astype(np.float32)
            else:
                # Fallback: compute L* as luminance; set a/b to zero.
                gray = self.ensure_gray().astype(np.float32)
                L = gray / 255.0 * 100.0  # pseudo-L*
                a = np.zeros_like(L)
                b = np.zeros_like(L)
                self.lab_image = np.stack([L, a, b], axis=-1)
        return self.lab_image

    def ensure_edges(self) -> np.ndarray:
        """Return an edge map (uint8 0/255), computing if necessary."""
        if self.edges is None:
            gray = self.ensure_gray()
            if cv2 is None:
                # basic gradient-magnitude threshold as fallback
                gy, gx = np.gradient(gray.astype(np.float32))
                mag = np.hypot(gx, gy)
                thresh = float(np.percentile(mag, 75))
                self.edges = (mag > thresh).astype(np.uint8) * 255
            else:
                self.edges = cv2.Canny(gray, threshold1=100, threshold2=200)
        return self.edges

    def set_attribute(self, key: str, value: float) -> None:
        """Store a single scalar attribute, clamped to [0, 1] where sensible."""
        if isinstance(value, (int, float)):
            v = float(value)
            if not np.isfinite(v):
                v = 0.0
            if v < 0.0:
                v = 0.0
            if v > 1.0:
                v = 1.0
            self.attributes[key] = v

    def set_attributes(self, values: Dict[str, float]) -> None:
        for k, v in values.items():
            self.set_attribute(k, v)----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/pipeline.py
----- CONTENT START -----
"""Science pipeline orchestrator for Image Tagger v3.3."""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import numpy as np
from sqlalchemy.orm import Session

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer

logger = logging.getLogger(__name__)


class SciencePipelineConfig:
    """Runtime configuration flags for the science pipeline."""

    def __init__(
        self,
        enable_color: bool = True,
        enable_complexity: bool = True,
        enable_texture: bool = True,
        enable_fractals: bool = True,
        enable_spatial: bool = True,
        enable_cognitive: bool = False,
        image_root: str = "data_store",
    ) -> None:
        self.enable_color = enable_color
        self.enable_complexity = enable_complexity
        self.enable_texture = enable_texture
        self.enable_fractals = enable_fractals
        self.enable_spatial = enable_spatial
        self.enable_cognitive = enable_cognitive
        self.image_root = image_root


class SciencePipeline:
    """Orchestrates science analyzers over Image records."""

    def __init__(self, db: Session, config: Optional[SciencePipelineConfig] = None):
        self.db = db
        self.config = config or SciencePipelineConfig()
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.spatial = DepthAnalyzer()
        self.cognitive = CognitiveStateAnalyzer()

    def process_image(self, image_id: int) -> bool:
        """Run science analyses for a single image_id.

        Returns True on success, False on any fatal error. Partial attribute
        extraction is allowed; we commit whatever was computed.
        """
        image_record = self.db.query(Image).get(image_id)
        if image_record is None:
            logger.warning("SciencePipeline: image %s not found", image_id)
            return False

        rgb = self._load_image(image_record)
        if rgb is None:
            logger.warning("SciencePipeline: could not load pixels for %s", image_id)
            return False

        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            if self.config.enable_color:
                self.color.analyze(frame)
            if self.config.enable_complexity:
                self.complexity.analyze(frame)
            if self.config.enable_texture:
                self.texture.analyze(frame)
            if self.config.enable_fractals:
                self.fractals.analyze(frame)
            if self.config.enable_spatial:
                self.spatial.analyze(frame)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)
        except Exception:
            logger.exception("SciencePipeline: error while analyzing image %s", image_id)

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        """Load an RGB uint8 image for the given record.

        This implementation assumes a local file under config.image_root.
        In future we can replace this with a storage abstraction.
        """
        rel = Path(image_record.storage_path)
        path = Path(self.config.image_root) / rel
        if not path.exists():
            logger.error("SciencePipeline: file does not exist: %s", path)
            return None
        if cv2 is None:
            logger.error("SciencePipeline: cv2 not available, cannot load image")
            return None

        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            return None
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        return rgb

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=value,
                source="science_pipeline_v3.3",
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/context/cognitive.py
----- CONTENT START -----
"""Cognitive state / VLM-based attribute analysis."""

from __future__ import annotations

from typing import Dict, Optional, Protocol
import numpy as np

from backend.science.core import AnalysisFrame


class PerceptionEngine(Protocol):
    """Protocol for a VLM-style perception engine.

    Implementations should accept an RGB image (H, W, 3) uint8 array and
    return a mapping from attribute keys to floats in [0, 1].
    """

    def __call__(self, image: np.ndarray) -> Dict[str, float]:  # pragma: no cover
        raise NotImplementedError


class CognitiveStateAnalyzer:
    """Bridge to VLM-based cognitive/affective estimates.

    For now this is intentionally conservative: if no perception engine
    is configured, we produce neutral 0.5 scores with explicit keys.
    """

    def __init__(self, engine: Optional[PerceptionEngine] = None):
        self.engine = engine

    def analyze(self, frame: AnalysisFrame) -> None:
        if self.engine is None:
            neutral = {
                "cog.tranquility": 0.5,
                "cog.arousal": 0.5,
                "cog.formality": 0.5,
            }
            frame.set_attributes(neutral)
            frame.metadata.setdefault("cognitive", {})["source"] = "neutral_baseline"
            return

        try:
            preds = self.engine(frame.original_image)
        except Exception:
            frame.metadata.setdefault("cognitive", {})["source"] = "engine_error"
            return

        if not isinstance(preds, dict):
            return

        clean: Dict[str, float] = {}
        for k, v in preds.items():
            if isinstance(v, (int, float)) and np.isfinite(v):
                clean[k] = float(v)
        if clean:
            frame.set_attributes(clean)
            frame.metadata.setdefault("cognitive", {})["source"] = "engine"----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/math/color.py
----- CONTENT START -----
"""Perceptual color analysis in CIELAB space."""

from __future__ import annotations

import numpy as np
from scipy.spatial import ConvexHull
from backend.science.core import AnalysisFrame


class ColorAnalyzer:
    """Extract color descriptors aligned with perceptual space.

    All outputs are normalized to [0, 1] where possible.
    """

    def __init__(self, max_samples: int = 4096, random_seed: int = 42):
        self.max_samples = max_samples
        self.random_seed = random_seed

    def analyze(self, frame: AnalysisFrame) -> None:
        lab = frame.ensure_lab()

        # L* channel in [0, 100]
        L = lab[:, :, 0]
        a = lab[:, :, 1]
        b = lab[:, :, 2]

        # 1. Perceptual lightness (mean L* scaled to [0, 1])
        mean_L = float(np.mean(L))
        lightness = np.clip(mean_L / 100.0, 0.0, 1.0)

        # 2. Lightness contrast (std L* normalized by a nominal max, e.g. 25)
        std_L = float(np.std(L))
        lightness_contrast = np.clip(std_L / 25.0, 0.0, 1.0)

        # 3. Warm‚Äìcool index: map mean a* and b* to [0, 1]
        mean_a = float(np.mean(a))
        mean_b = float(np.mean(b))
        warm_radius = np.sqrt(mean_a ** 2 + mean_b ** 2)
        warm_radius_norm = np.clip(warm_radius / 60.0, 0.0, 1.0)
        warm_sign = 0.0
        denom = abs(mean_a) + abs(mean_b)
        if warm_radius > 1e-6 and denom > 1e-6:
            warm_sign = (mean_a + mean_b) / denom
        warm_sign_norm = (warm_sign + 1.0) / 2.0
        warmth_index = 0.5 * warm_radius_norm + 0.5 * warm_sign_norm

        # 4. Gamut volume in a‚Äìb plane via convex hull on a subsample
        h, w = a.shape
        ab = np.stack([a.reshape(-1), b.reshape(-1)], axis=1)

        if ab.shape[0] > self.max_samples:
            rng = np.random.default_rng(self.random_seed)
            idx = rng.choice(ab.shape[0], size=self.max_samples, replace=False)
            ab_sample = ab[idx]
        else:
            ab_sample = ab

        try:
            hull = ConvexHull(ab_sample)
            raw_area = float(hull.volume)
            gamut_volume = np.clip(raw_area / (200.0 * 200.0), 0.0, 1.0)
        except Exception:
            gamut_volume = 0.0

        frame.set_attributes(
            {
                "color.lightness_mean": lightness,
                "color.lightness_contrast": lightness_contrast,
                "color.warmth_index": warmth_index,
                "color.gamut_volume": gamut_volume,
            }
        )----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/math/complexity.py
----- CONTENT START -----
"""Image complexity metrics: Shannon entropy, spatial entropy, edge density."""

from __future__ import annotations

import numpy as np
from scipy.stats import entropy
from skimage.feature import greycomatrix
from backend.science.core import AnalysisFrame


class ComplexityAnalyzer:
    """Compute basic complexity metrics on gray image and edges.

    All outputs are normalized roughly into [0, 1] for downstream BN use.
    """

    def __init__(self, glcm_levels: int = 32, glcm_downsample_max: int = 256):
        self.glcm_levels = glcm_levels
        self.glcm_downsample_max = glcm_downsample_max

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        edges = frame.ensure_edges()

        # Normalize gray to 0..1
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray

        # 1. Shannon entropy of gray histogram
        hist, _ = np.histogram(gray_norm, bins=64, range=(0.0, 1.0))
        if hist.sum() > 0:
            hist = hist.astype(np.float32) / float(hist.sum())
            shannon = float(entropy(hist, base=2))
            shannon_norm = np.clip(shannon / np.log2(64.0), 0.0, 1.0)
        else:
            shannon_norm = 0.0

        # 2. Spatial entropy using GLCM on downsampled image
        spatial_entropy_norm = self._spatial_entropy(gray_norm)

        # 3. Edge density
        edge_density = float((edges > 0).sum()) / float(edges.size) if edges.size > 0 else 0.0
        edge_density = float(np.clip(edge_density, 0.0, 1.0))

        frame.set_attributes(
            {
                "complexity.shannon_entropy": shannon_norm,
                "complexity.spatial_entropy": spatial_entropy_norm,
                "complexity.edge_density": edge_density,
            }
        )

    def _spatial_entropy(self, gray_norm: np.ndarray) -> float:
        h, w = gray_norm.shape
        # Downsample if needed to keep GLCM manageable
        factor = max(h, w) / float(self.glcm_downsample_max)
        if factor > 1.0:
            new_h = max(1, int(h / factor))
            new_w = max(1, int(w / factor))
            gray_ds = gray_norm[0:h:new_h, 0:w:new_w]
        else:
            gray_ds = gray_norm

        if gray_ds.size < 4:
            return 0.0

        # Quantize
        bins = self.glcm_levels
        quant = np.clip((gray_ds * (bins - 1)).astype(np.int32), 0, bins - 1)

        glcm = greycomatrix(
            quant,
            distances=[1],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=bins,
            symmetric=True,
            normed=True,
        )

        # Joint distribution over gray-level pairs
        P = glcm[:, :, 0, :]  # shape (bins, bins, angles)
        P = P.mean(axis=-1)    # average over angles
        P_flat = P.reshape(-1)
        P_flat = P_flat[P_flat > 0]
        if P_flat.size == 0:
            return 0.0
        H = float(entropy(P_flat, base=2))
        # maximum possible entropy is log2(bins^2)
        H_max = 2.0 * np.log2(float(bins))
        return float(np.clip(H / H_max, 0.0, 1.0))----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/math/fractals.py
----- CONTENT START -----
"""Fractal dimension estimates based on box-counting on edge maps."""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame


class FractalAnalyzer:
    """Estimate fractal dimension of edge structure.

    We use a simple box-counting method over dyadic box sizes.
    """

    def __init__(self, min_box_size: int = 2):
        self.min_box_size = min_box_size

    def analyze(self, frame: AnalysisFrame) -> None:
        edges = frame.ensure_edges()
        if edges.size == 0:
            return

        # binary mask
        Z = (edges > 0).astype(np.uint8)
        d_raw = self._box_count_dim(Z)
        # For typical natural scenes, D is in [1, 2]; map linearly to [0, 1]
        if np.isnan(d_raw):
            d_norm = 0.0
        else:
            d_norm = (d_raw - 1.0) / 1.0
            d_norm = float(np.clip(d_norm, 0.0, 1.0))

        frame.set_attribute("fractal.dimension", d_norm)

    def _box_count_dim(self, Z: np.ndarray) -> float:
        # Pad to square
        h, w = Z.shape
        n = max(h, w)
        # Next power of two
        n2 = 1 << (n - 1).bit_length()
        pad_h = n2 - h
        pad_w = n2 - w
        Z_padded = np.pad(Z, ((0, pad_h), (0, pad_w)), mode="constant", constant_values=0)

        sizes = []
        counts = []

        size = n2
        while size >= self.min_box_size:
            num = n2 // size
            if num == 0:
                break
            reshaped = Z_padded.reshape(num, size, num, size)
            reshaped = reshaped.swapaxes(1, 2)
            occupied = reshaped.max(axis=(-1, -2))
            count = int(np.count_nonzero(occupied))
            if count > 0:
                sizes.append(size)
                counts.append(count)
            size //= 2

        if len(sizes) < 2:
            return float("nan")

        sizes = np.array(sizes, dtype=np.float64)
        counts = np.array(counts, dtype=np.float64)

        logs = np.log(sizes)
        log_counts = np.log(counts)
        A = np.vstack([logs, np.ones_like(logs)]).T
        coeffs, *_ = np.linalg.lstsq(A, log_counts, rcond=None)
        slope = coeffs[0]
        return float(-slope)----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/math/glcm.py
----- CONTENT START -----
"""Texture metrics using GLCM at multiple scales."""

from __future__ import annotations

import numpy as np
from skimage.feature import greycomatrix, greycoprops
from backend.science.core import AnalysisFrame


class TextureAnalyzer:
    """Compute micro- and macro-texture metrics using GLCM.

    The outputs are aggregated across orientations and normalized for BN use.
    """

    def __init__(self, levels: int = 32, micro_distance: int = 1, macro_distance: int = 5):
        self.levels = levels
        self.micro_distance = micro_distance
        self.macro_distance = macro_distance

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.ensure_gray().astype(np.float32)
        if gray.size == 0:
            return

        # Normalize gray to 0..1 then quantize
        if gray.max() > 0:
            gray_norm = gray / float(gray.max())
        else:
            gray_norm = gray
        quant = np.clip((gray_norm * (self.levels - 1)).astype(np.int32), 0, self.levels - 1)

        micro = self._compute_props(quant, self.micro_distance)
        macro = self._compute_props(quant, self.macro_distance)

        frame.set_attributes(
            {
                "texture.micro.contrast": micro["contrast"],
                "texture.micro.homogeneity": micro["homogeneity"],
                "texture.micro.energy": micro["energy"],
                "texture.macro.contrast": macro["contrast"],
                "texture.macro.homogeneity": macro["homogeneity"],
                "texture.macro.energy": macro["energy"],
            }
        )

    def _compute_props(self, quant: np.ndarray, distance: int) -> dict:
        glcm = greycomatrix(
            quant,
            distances=[distance],
            angles=[0.0, np.pi / 4, np.pi / 2, 3 * np.pi / 4],
            levels=self.levels,
            symmetric=True,
            normed=True,
        )
        props = {}
        for name in ("contrast", "homogeneity", "energy"):
            vals = greycoprops(glcm, name)
            mean_val = float(np.mean(vals))
            # simple normalization heuristics
            if name == "contrast":
                # contrast can be large; squash via 1 - exp(-x / c)
                c = 4.0
                props[name] = float(1.0 - np.exp(-mean_val / c))
            elif name == "energy":
                # energy already in [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
            else:
                # homogeneity is [0,1]
                props[name] = float(np.clip(mean_val, 0.0, 1.0))
        return props----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_refactor/backend/science/spatial/depth.py
----- CONTENT START -----
"""Spatial structure analyzer (placeholder for depth-based metrics)."""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame


class DepthAnalyzer:
    """Compute simple edge-based spatial structure proxies.

    This is a bridge towards a future depth-based module. It provides:
      - spatial.edge_clutter_sd: SD of edge density across a coarse grid
      - spatial.central_openness: 1 - edge density in central crop
      - spatial.vertical_balance: edge density difference top vs bottom
    """

    def __init__(self, grid_size: int = 8, central_fraction: float = 0.4):
        self.grid_size = max(2, grid_size)
        self.central_fraction = float(np.clip(central_fraction, 0.1, 0.9))

    def analyze(self, frame: AnalysisFrame) -> None:
        edges = frame.ensure_edges()
        h, w = edges.shape
        if h < 2 or w < 2:
            return

        # 1. Edge clutter SD across grid
        gh = min(self.grid_size, h)
        gw = min(self.grid_size, w)
        cell_h = max(1, h // gh)
        cell_w = max(1, w // gw)

        densities = []
        for gy in range(gh):
            y0 = gy * cell_h
            y1 = min(h, (gy + 1) * cell_h)
            if y0 >= h:
                break
            for gx in range(gw):
                x0 = gx * cell_w
                x1 = min(w, (gx + 1) * cell_w)
                if x0 >= w:
                    break
                cell = edges[y0:y1, x0:x1]
                if cell.size == 0:
                    continue
                dens = float((cell > 0).sum()) / float(cell.size)
                densities.append(dens)

        if len(densities) > 0:
            clutter_sd = float(np.std(densities))
            clutter_norm = float(np.clip(clutter_sd / 0.5, 0.0, 1.0))
        else:
            clutter_norm = 0.0

        # 2. Central openness (fewer edges in centre => higher openness)
        cf = self.central_fraction
        cy0 = int((h * (1.0 - cf)) / 2.0)
        cy1 = int((h * (1.0 + cf)) / 2.0)
        cx0 = int((w * (1.0 - cf)) / 2.0)
        cx1 = int((w * (1.0 + cf)) / 2.0)
        centre = edges[cy0:cy1, cx0:cx1]
        if centre.size > 0:
            centre_density = float((centre > 0).sum()) / float(centre.size)
        else:
            centre_density = 0.0
        central_openness = float(np.clip(1.0 - centre_density, 0.0, 1.0))

        # 3. Vertical balance (edges top vs bottom)
        half = h // 2
        top = edges[:half, :]
        bottom = edges[half:, :]
        if top.size > 0:
            top_d = float((top > 0).sum()) / float(top.size)
        else:
            top_d = 0.0
        if bottom.size > 0:
            bot_d = float((bottom > 0).sum()) / float(bottom.size)
        else:
            bot_d = 0.0
        vert_balance = float(np.clip((bot_d - top_d + 1.0) / 2.0, 0.0, 1.0))

        frame.set_attributes(
            {
                "spatial.edge_clutter_sd": clutter_norm,
                "spatial.central_openness": central_openness,
                "spatial.vertical_balance": vert_balance,
            }
        )----- CONTENT END -----
----- FILE PATH: archive/v3_2_33_science_smoke/scripts/smoke_science.py
----- CONTENT START -----
"""DB-backed smoke test for the science pipeline.

This script verifies that:

  * The database is reachable.
  * At least one Image record exists.
  * SciencePipeline.process_image(image_id) runs without crashing.
  * At least one Validation row with source="science_pipeline_v3.3"
    is written for that image.

Exit codes:
  0 = success
  1 = failure (no images, DB error, or no attributes written)

Typical usage inside the API container:

  python -m scripts.smoke_science
"""

from __future__ import annotations

import sys
from typing import Optional

from sqlalchemy.orm import Session

from backend.database.session import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.pipeline import SciencePipeline, SciencePipelineConfig


def _pick_image(db: Session) -> Optional[Image]:
    """Pick an image for the smoketest.

    Preference order:
    1. An image that has *no* science_pipeline_v3.3 validations yet,
       so we exercise a fresh run.
    2. Otherwise, the first Image in the table.
    """
    img = (
        db.query(Image)
        .outerjoin(
            Validation,
            (Validation.image_id == Image.id)
            & (Validation.source == "science_pipeline_v3.3"),
        )
        .filter(Validation.id.is_(None))
        .order_by(Image.id)
        .first()
    )
    if img is not None:
        return img

    return db.query(Image).order_by(Image.id).first()


def main() -> int:
    try:
        db = SessionLocal()
    except Exception as exc:
        print(f"[smoke_science] FAILED: could not create DB session: {exc}")
        return 1

    try:
        image = _pick_image(db)
        if image is None:
            print(
                "[smoke_science] FAILED: no Image rows found in database. "
                "Seed at least one image before running this smoke test."
            )
            return 1

        image_id = image.id
        print(f"[smoke_science] Using image_id={image_id}")

        cfg = SciencePipelineConfig()
        pipeline = SciencePipeline(db=db, config=cfg)

        ok = pipeline.process_image(image_id=image_id)
        if not ok:
            print(
                "[smoke_science] FAILED: SciencePipeline.process_image "
                "returned False"
            )
            return 1

        count = (
            db.query(Validation)
            .filter(
                Validation.image_id == image_id,
                Validation.source == "science_pipeline_v3.3",
            )
            .count()
        )
        if count <= 0:
            print(
                "[smoke_science] FAILED: no Validation rows written for "
                f"image_id={image_id} with source='science_pipeline_v3.3'"
            )
            return 1

        print(
            f"[smoke_science] SUCCESS: {count} validation rows written "
            f"for image_id={image_id}"
        )
        return 0
    finally:
        try:
            db.close()
        except Exception:
            pass


if __name__ == "__main__":
    sys.exit(main())----- CONTENT END -----
----- FILE PATH: archive/v3_2_34_router_gui/backend/main.py
----- CONTENT START -----
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles

from backend.api import v1_annotation, v1_admin, v1_supervision, v1_discovery

# v3 Enterprise Application Entry Point
app = FastAPI(
    title="Image Tagger v3.2.33 (Enterprise)",
    description="Unified API for Tagger Workbench, Supervisor, Admin, and Explorer.",
    version="3.2.33",
    docs_url="/docs",
    redoc_url="/redoc",
)

# API Routers
# All v1 routers are mounted here so that the smoketests and GUIs see a coherent API surface.
app.include_router(v1_annotation.router)
app.include_router(v1_admin.router)
app.include_router(v1_supervision.router)
app.include_router(v1_discovery.router)

# Static files (optional, for serving built frontends if desired)
# In Docker/Nginx this may be handled by the web server instead.
# app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/health")
def health_check():
    """Kubernetes/Docker Health Probe"""
    return {"status": "healthy", "version": "3.2.33"}

@app.get("/")
def root():
    return {
        "message": "Image Tagger v3 API",
        "docs": "/docs",
        "workbench_api": "/v1/workbench/next",
    }----- CONTENT END -----
----- FILE PATH: archive/v3_2_34_router_gui/frontend/apps/admin/src/App.jsx
----- CONTENT START -----
import React, { useState, useEffect } from 'react';
import { Header, Button, Toggle, ApiClient } from '@shared';
import { ShieldAlert, DollarSign, Server, Power, Info, RefreshCcw, Download } from 'lucide-react';

const api = new ApiClient('/api/v1/admin');

export default function AdminApp() {
    const [models, setModels] = useState([]);
    const [budget, setBudget] = useState(null);
    const [killSwitchActive, setKillSwitchActive] = useState(false);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [savingModelId, setSavingModelId] = useState(null);
    const [killBusy, setKillBusy] = useState(false);
    const [exportIds, setExportIds] = useState('');
    const [exportBusy, setExportBusy] = useState(false);
    const [exportMessage, setExportMessage] = useState(null);

    useEffect(() => {
        loadAll();
    }, []);

    async function loadAll() {
        setLoading(true);
        setError(null);
        try {
            const [modelsResp, budgetResp] = await Promise.all([
                api.get('/models'),
                api.get('/budget'),
            ]);
            setModels(Array.isArray(modelsResp) ? modelsResp : []);
            if (budgetResp) {
                setBudget(budgetResp);
                setKillSwitchActive(!!budgetResp.is_kill_switched);
            }
        } catch (err) {
            console.error('Failed to load admin data', err);
            setError(err.message || 'Failed to load admin cockpit');
        } finally {
            setLoading(false);
        }
    }

    async function handleToggle(id) {
        const current = models.find(m => m.id === id);
        if (!current) return;
        setSavingModelId(id);
        setError(null);
        try {
            const updated = await api.patch(`/models/${id}`, {
                is_enabled: !current.is_enabled,
            });
            setModels(models.map(m => (m.id === id ? updated : m)));
        } catch (err) {
            console.error('Failed to update model', err);
            setError(err.message || 'Failed to update model');
        } finally {
            setSavingModelId(null);
        }
    }

    async function handleCostBlur(id, rawValue) {
        const value = parseFloat(rawValue);
        if (Number.isNaN(value)) {
            return;
        }
        const current = models.find(m => m.id === id);
        if (!current || current.cost_per_1k_tokens === value) {
            return;
        }
        setSavingModelId(id);
        setError(null);
        try {
            const updated = await api.patch(`/models/${id}`, {
                cost_per_1k_tokens: value,
            });
            setModels(models.map(m => (m.id === id ? updated : m)));
        } catch (err) {
            console.error('Failed to update cost', err);
            setError(err.message || 'Failed to update model cost');
        } finally {
            setSavingModelId(null);
        }
    }

    async function handleKillSwitch(nextActive) {
        setKillBusy(true);
        setError(null);
        try {
            const resp = await api.post(`/kill-switch?active=${nextActive}
async function handleTrainingExport() {
    setExportBusy(true);
    setExportMessage(null);
    setError(null);
    try {
        const ids = exportIds
            .split(',')
            .map(s => parseInt(s.trim(), 10))
            .filter(n => !Number.isNaN(n));
        const payload = { image_ids: ids, format: 'json' };
        const data = await api.post('/training/export', payload);
        const blob = new Blob([JSON.stringify(data, null, 2)], { type: 'application/json' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'training_export.json';
        a.click();
        URL.revokeObjectURL(url);
        setExportMessage(`Exported ${data.length || 0} rows to training_export.json`);
    } catch (err) {
        console.error('Training export failed', err);
        setError(err.message || 'Training export failed');
    } finally {
        setExportBusy(false);
    }
}
`);
            if (resp) {
                setBudget(resp);
                setKillSwitchActive(!!resp.is_kill_switched);
            }
        } catch (err) {
            console.error('Failed to toggle kill switch', err);
            setError(err.message || 'Failed to toggle kill switch');
        } finally {
            setKillBusy(false);
        }
    }

    const totalModels = models.length;
    const enabledModels = models.filter(m => m.is_enabled).length;

    const totalSpent = budget ? budget.total_spent : 0;
    const hardLimit = budget ? budget.hard_limit : 1;
    const usagePct = hardLimit > 0 ? Math.min(100, (totalSpent / hardLimit) * 100) : 0;

    return (
        <div className="min-h-screen bg-gray-100 pb-10">
            <Header appName="Admin" title="Cost & Governance Cockpit" />

            <div className="p-8 max-w-6xl mx-auto space-y-8">
                <div className="flex items-center justify-between gap-4">
                    <div>
                        <p className="text-sm text-gray-500">
                            Configure which models and tools are allowed to run, and monitor budget risk.
                        </p>
                        {error && (
                            <p className="text-xs text-red-600 mt-1">
                                {error}
                            </p>
                        )}
                    </div>
                    <div className="flex items-center gap-3">
                        {loading && (
                            <span className="text-xs text-gray-500 flex items-center gap-2">
                                <RefreshCcw className="animate-spin" size={14} /> Loading‚Ä¶
                            </span>
                        )}
                        <Button variant="secondary" onClick={loadAll}>
                            <RefreshCcw size={16} className="mr-2" /> Refresh
                        </Button>
                    </div>
                </div>

                <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
                    {/* Models & Tools */}
                    <section className="lg:col-span-2 bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                        <div className="p-4 border-b border-gray-100 flex items-center justify-between">
                            <div className="flex items-center gap-2">
                                <Server className="text-blue-500" size={18} />
                                <h2 className="font-semibold text-gray-900 text-sm">
                                    AI Models
                                </h2>
                            </div>
                            <span className="text-xs text-gray-400">
                                {enabledModels}/{totalModels} models enabled
                            </span>
                        </div>
                        <div className="divide-y divide-gray-100">
                            {models.map(model => (
                                <div
                                    key={model.id}
                                    className="flex items-center justify-between px-4 py-3 hover:bg-gray-50 transition-colors"
                                >
                                    <div>
                                        <p className="text-sm font-medium text-gray-900">
                                            {model.name}
                                        </p>
                                        <p className="text-xs text-gray-500">
                                            Provider: {model.provider || '‚Äî'}
                                        </p>
                                    </div>
                                    <div className="flex items-center gap-4">
                                        <div className="text-right">
                                            <label className="block text-[11px] uppercase tracking-wide text-gray-500 font-semibold mb-1">
                                                Cost / 1K tokens
                                            </label>
                                            <div className="flex items-center gap-1">
                                                <DollarSign size={12} className="text-gray-400" />
                                                <input
                                                    type="number"
                                                    step="0.0001"
                                                    defaultValue={model.cost_per_1k_tokens}
                                                    onBlur={e => handleCostBlur(model.id, e.target.value)}
                                                    className="w-20 px-2 py-1 border border-gray-200 rounded text-xs text-right focus:outline-none focus:ring-1 focus:ring-blue-400"
                                                />
                                            </div>
                                        </div>
                                        <div className="flex flex-col items-end">
                                            <span className="text-[11px] text-gray-500 mb-1">
                                                {model.is_enabled ? 'Enabled' : 'Disabled'}
                                            </span>
                                            <Toggle
                                                checked={model.is_enabled}
                                                disabled={savingModelId === model.id}
                                                onChange={() => handleToggle(model.id)}
                                            />
                                        </div>
                                    </div>
                                </div>
                            ))}
                            {!models.length && !loading && (
                                <div className="p-4 text-xs text-gray-400">
                                    No ToolConfigs found. Run the seed_tool_configs script or insert rows into
                                    the tool_configs table to populate this view.
                                </div>
                            )}
                        </div>
                    </section>

                    {/* Kill Switch & Budget */}
                    <section className="space-y-4">
                        <div className="bg-red-50 border border-red-100 rounded-xl p-4 flex flex-col gap-3">
                            <div className="flex items-center gap-2">
                                <ShieldAlert className="text-red-500" size={18} />
                                <h2 className="font-semibold text-sm text-red-800">
                                    Kill Switch
                                </h2>
                            </div>
                            <p className="text-xs text-red-700">
                                When activated, all paid models (cost_per_1k_tokens &gt; 0) are disabled. This is
                                enforced server-side via the ToolConfig table and checked before tools are used.
                            </p>
                            <div className="flex items-center justify-between mt-2">
                                <div>
                                    <p className="text-[11px] text-red-600 font-semibold uppercase tracking-wide">
                                        Status
                                    </p>
                                    <p className="text-sm font-medium text-red-900">
                                        {killSwitchActive ? 'ACTIVE' : 'Inactive'}
                                    </p>
                                </div>
                                <Button
                                    variant={killSwitchActive ? 'outline' : 'primary'}
                                    size="sm"
                                    disabled={killBusy}
                                    onClick={() => handleKillSwitch(!killSwitchActive)}
                                >
                                    <Power size={14} className="mr-1" />
                                    {killSwitchActive ? 'Disable Kill Switch' : 'Activate Kill Switch'}
                                </Button>
                            </div>
                            <p className="text-[11px] text-red-500 flex items-center gap-1 mt-1">
                                <Info size={12} /> Use this if cost monitoring shows you are approaching budget.
                            </p>
                        </div>

                        <div className="bg-slate-900 rounded-xl p-4 text-slate-50 shadow-sm">
<div className="bg-white rounded-xl border border-gray-200 p-4">
    <div className="flex items-center justify-between mb-2">
        <div className="flex items-center gap-2">
            <Download className="text-gray-600" size={18} />
            <h2 className="font-semibold text-sm text-gray-900">
                Training Export
            </h2>
        </div>
    </div>
    <p className="text-xs text-gray-500 mb-2">
        Export validated tags as JSON for fine-tuning or active learning. Provide a comma-separated
        list of image IDs (or leave blank to export nothing).
    </p>
    <textarea
        className="w-full text-xs border border-gray-200 rounded p-2 mb-2 focus:outline-none focus:ring-1 focus:ring-blue-400"
        rows={2}
        placeholder="e.g. 101, 102, 103"
        value={exportIds}
        onChange={e => setExportIds(e.target.value)}
    />
    <div className="flex items-center justify-between">
        <Button
            size="sm"
            variant="secondary"
            disabled={exportBusy}
            onClick={handleTrainingExport}
        >
            <Download size={14} className="mr-1" />
            Download JSON
        </Button>
        {exportMessage && (
            <span className="text-[11px] text-gray-500">
                {exportMessage}
            </span>
        )}
    </div>
</div>

                            <div className="flex items-center justify-between">
                                <div className="flex items-center gap-2">
                                    <DollarSign className="text-emerald-300" size={18} />
                                    <h2 className="font-semibold text-sm">
                                        Cost Overview
                                    </h2>
                                </div>
                                <span className="text-[11px] text-slate-400">
                                    Prototype estimator
                                </span>
                            </div>
                            <div className="mt-4 space-y-1 text-xs">
                                <p>
                                    <span className="text-slate-400">Estimated spend:</span>{' '}
                                    <span className="font-semibold">${totalSpent.toFixed(2)}</span>
                                </p>
                                <p>
                                    <span className="text-slate-400">Hard limit:</span>{' '}
                                    <span className="font-semibold">${hardLimit.toFixed(2)}</span>
                                </p>
                            </div>
                            <div className="w-full bg-slate-800 h-2 rounded-full mt-4 overflow-hidden">
                                <div
                                    className="bg-emerald-300 h-full"
                                    style={{ width: `${usagePct}%` }}
                                />
                            </div>
                            <p className="text-[11px] text-slate-400 mt-2">
                                {usagePct.toFixed(0)}% of hard limit used.
                            </p>
                        </div>
                    </section>
                </div>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: archive/v3_3_6_admin_upload_patch/CHANGELOG_v3_3_6_admin_upload_patch.txt
----- CONTENT START -----
v3.3.6 admin upload + version bump patch
- Bumped VERSION and metadata files from 3.3.5 to 3.3.6.
- Fixed Admin bulk upload endpoint path to /api/v1/admin/upload.
- Preserved all prior files; this release is a strict superset of v3.3.5.
----- CONTENT END -----
----- FILE PATH: archive/v3_3_7_ruthless_followup/CHANGELOG_v3_3_7_ruthless_followup.txt
----- CONTENT START -----
v3.3.7 Ruthless follow-up patch

- Unified visible version strings to 3.3.7 where appropriate.
- Made scripts/smoke_science.py tolerant of a zero-image database (skip with guidance).
- Added pytest coverage for Admin bulk upload and Monitor Tag Inspector endpoints.
- Improved empty-state messaging in the Monitor and Explorer frontends.
- Added docs: WHATS_NEW_v3_3_x.md, FIRST_DASHBOARD_QUICKSTART.md, BN_EXPORT_EXAMPLE.md.
- Expanded governance_guide.md with a Guardian everyday-work cheat sheet.
- Documented frontend smoketest expectations in devops_quickstart.md and smoke_frontend.py.
----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/VERSION
----- CONTENT START -----
3.4.12
----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/main.py
----- CONTENT START -----
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles

from backend.api import v1_annotation, v1_admin, v1_supervision, v1_discovery, v1_bn_export, v1_debug
from backend.versioning import VERSION

# v3 Enterprise Application Entry Point
app = FastAPI(
    title=f"Image Tagger v3 (v{VERSION})",
    description="Unified API for Tagger Workbench, Supervisor, Admin, and Explorer.",
    version=VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
)

# API Routers
# All v1 routers are mounted here so that the smoketests and GUIs see a coherent API surface.
app.include_router(v1_annotation.router)
app.include_router(v1_admin.router)
app.include_router(v1_supervision.router)
app.include_router(v1_discovery.router)
app.include_router(v1_bn_export.router)
app.include_router(v1_debug.router)

# Frontend-facing mounts (ApiClient uses base '/api')
app.include_router(v1_annotation.router, prefix="/api")
app.include_router(v1_admin.router, prefix="/api")
app.include_router(v1_supervision.router, prefix="/api")
app.include_router(v1_discovery.router, prefix="/api")
app.include_router(v1_debug.router, prefix="/api")

# Static files (optional, for serving built frontends if desired)
# In Docker/Nginx this may be handled by the web server instead.
# app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/health")
def health_check():
    """Kubernetes/Docker Health Probe"""
    return {"status": "healthy", "version": VERSION}

@app.get("/")
def root():
    return {
        "message": "Image Tagger v3 API",
        "docs": "/docs",
        "workbench_api": "/v1/workbench/next",
    }----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/api/v1_admin.py
----- CONTENT START -----
from pydantic import BaseModel
from typing import Dict, List, Optional
from pathlib import Path
from fastapi import UploadFile, File,  APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import select, update

from backend.database.core import get_db
from backend.services.auth import require_admin
from backend.services.vlm import describe_vlm_configuration, update_vlm_config, get_vlm_engine, StubEngine
from backend.models.config import ToolConfig
from backend.schemas.admin import ToolConfigRead, ToolConfigUpdate, BudgetStatus
from backend.schemas.discovery import ExportRequest
from backend.schemas.training import TrainingExample
from backend.services.training_export import TrainingExporter


router = APIRouter(prefix="/v1/admin", tags=["Admin Cockpit"])


@router.get("/models", response_model=list[ToolConfigRead])
async def list_models(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> list[ToolConfigRead]:
    """Return all ToolConfig rows.

    This powers the 'AI Models' section of the Admin Cockpit.
    """
    tools = db.execute(select(ToolConfig)).scalars().all()
    return [ToolConfigRead.model_validate(t) for t in tools]


@router.patch("/models/{tool_id}", response_model=ToolConfigRead)
async def update_model(
    tool_id: int,
    payload: ToolConfigUpdate,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> ToolConfigRead:
    """Update a single model's enabled state and/or cost.

    Frontend calls this when toggling a model or editing its cost.
    """
    tool = db.get(ToolConfig, tool_id)
    if tool is None:
        raise HTTPException(status_code=404, detail="ToolConfig not found")

    if payload.is_enabled is not None:
        tool.is_enabled = payload.is_enabled
    if payload.cost_per_1k_tokens is not None:
        tool.cost_per_1k_tokens = float(payload.cost_per_1k_tokens)

    db.add(tool)
    db.commit()
    db.refresh(tool)
    return ToolConfigRead.model_validate(tool)


@router.get("/budget", response_model=BudgetStatus)
async def get_budget(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> BudgetStatus:
    """Return a simple cost snapshot and kill-switch state.

    In this v3.2 dev build, the numbers are conservative placeholders.
    The kill-switch is derived from whether any paid models (cost_per_1k_tokens > 0)
    remain enabled.
    """
    paid_enabled = db.execute(
        select(ToolConfig).where(
            ToolConfig.cost_per_1k_tokens > 0,
            ToolConfig.is_enabled.is_(True),
        ).limit(1)
    ).first()
    is_kill_switched = paid_enabled is None

    # TODO: integrate with a real CostEstimator / usage ledger.
    total_spent = 14.50
    hard_limit = 15.00

    return BudgetStatus(
        total_spent=total_spent,
        hard_limit=hard_limit,
        is_kill_switched=is_kill_switched,
    )


@router.post("/kill-switch", response_model=BudgetStatus)
async def kill_switch(
    active: bool,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> BudgetStatus:
    """Global kill switch for paid models.

    When activated, all ToolConfigs with cost_per_1k_tokens > 0 are disabled.
    We then return a fresh BudgetStatus so the UI can update seamlessly.
    """
    if active:
        stmt = (
            update(ToolConfig)
            .where(ToolConfig.cost_per_1k_tokens > 0)
            .values(is_enabled=False)
        )
        db.execute(stmt)
        db.commit()

    # Re-use the logic from get_budget to compute state
    paid_enabled = db.execute(
        select(ToolConfig).where(
            ToolConfig.cost_per_1k_tokens > 0,
            ToolConfig.is_enabled.is_(True),
        ).limit(1)
    ).first()
    is_kill_switched = paid_enabled is None

    total_spent = 14.50
    hard_limit = 15.00

    return BudgetStatus(
        total_spent=total_spent,
        hard_limit=hard_limit,
        is_kill_switched=is_kill_switched,
    )
@router.post("/training/export", response_model=list[TrainingExample])
async def admin_export_training(
    request: ExportRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> list[TrainingExample]:
    """
    Admin-facing training export endpoint.

    This mirrors the Explorer export but is scoped to admin RBAC and
    can be extended with richer filters (quality thresholds, time
    windows) in future steps.
    """
    exporter = TrainingExporter(db=db)
    examples = exporter.export_for_images(request.image_ids)
    return [TrainingExample(**e) for e in examples]

@router.get("/export/images")
def export_all_images(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
):
    """Download a zip of all stored image files.

    This is an admin-only convenience endpoint for researchers who
    need local copies of the raw assets. In typical classroom
    usage, datasets are modest in size, so an in-memory zip is
    acceptable.
    """
    import io as _io
    import os as _os
    import zipfile as _zip
    from pathlib import Path as _Path
    from backend.models.assets import Image  # type: ignore

    try:
        from backend.settings import IMAGE_STORAGE_ROOT  # type: ignore
    except Exception:
        IMAGE_STORAGE_ROOT = _os.getenv("IMAGE_STORAGE_ROOT", "data_store")

    images = db.query(Image).all()
    if not images:
        raise HTTPException(status_code=404, detail="No images available for export")

    buf = _io.BytesIO()
    root = _Path(IMAGE_STORAGE_ROOT)

    with _zip.ZipFile(buf, "w", _zip.ZIP_DEFLATED) as zf:
        for img in images:
            path = _Path(img.storage_path)
            if not path.is_absolute():
                path = root / path
            if not path.exists():
                # Skip missing files rather than failing completely.
                continue
            try:
                arcname = path.relative_to(root)
            except Exception:
                arcname = path.name
            zf.write(path, arcname=str(arcname))

    buf.seek(0)
    headers = {
        "Content-Disposition": 'attachment; filename="image_tagger_images_export.zip"'
    }
    return StreamingResponse(buf, media_type="application/zip", headers=headers)
class AdminUploadResult(BaseModel):
    created_count: int
    image_ids: List[int]
    storage_paths: List[str]


@router.post("/upload", response_model=AdminUploadResult)
async def upload_images(
    files: List[UploadFile] = File(...),
    db: Session = Depends(get_db),
    user=Depends(require_admin),
) -> AdminUploadResult:
    """Bulk image upload endpoint for the Admin Cockpit.

    This accepts one or more image files and stores them under the
    configured IMAGE_STORAGE_ROOT. For each stored file, a new Image
    row is created in the database.
    """
    import os as _os
    from pathlib import Path as _Path
    from uuid import uuid4

    try:
        from backend.settings import IMAGE_STORAGE_ROOT  # type: ignore
    except Exception:
        IMAGE_STORAGE_ROOT = _os.getenv("IMAGE_STORAGE_ROOT", "data_store")

    from backend.models.assets import Image  # type: ignore

    root = _Path(IMAGE_STORAGE_ROOT)
    root.mkdir(parents=True, exist_ok=True)

    created_ids: List[int] = []
    storage_paths: List[str] = []

    for f in files:
        suffix = "".join(_Path(f.filename or "uploaded").suffixes)
        if suffix.lower() not in {".jpg", ".jpeg", ".png", ".webp"}:
            # Skip unknown formats; in future we might reject the request instead.
            continue
        unique_name = f"{uuid4().hex}{suffix}"
        dest = root / unique_name
        content = await f.read()
        dest.write_bytes(content)

        image = Image(storage_path=str(dest), source="admin_upload")
        db.add(image)
        db.flush()
        created_ids.append(image.id)
        storage_paths.append(str(dest))

    db.commit()

    return AdminUploadResult(
        created_count=len(created_ids),
        image_ids=created_ids,
        storage_paths=storage_paths,
    )



class VLMConfigRequest(BaseModel):
    provider: str = "auto"
    cognitive_prompt_override: Optional[str] = None
    max_batch_size: Optional[int] = None
    cost_per_1k_images_usd: Optional[float] = None


@router.get("/vlm/config")
def get_vlm_config(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Return the current VLM configuration and detected backends."""
    return describe_vlm_configuration()


@router.post("/vlm/config")
def set_vlm_config(
    payload: VLMConfigRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Persist the chosen VLM settings (provider, prompt override, batch size, cost hint)."""
    return update_vlm_config(
        provider=payload.provider,
        cognitive_prompt_override=payload.cognitive_prompt_override,
        max_batch_size=payload.max_batch_size,
        cost_per_1k_images_usd=payload.cost_per_1k_images_usd,
    )


class VLMTestRequest(BaseModel):
    image_id: int
    prompt: str = "Describe this architectural image in one or two sentences."


@router.post("/vlm/test")
def test_vlm_configuration(
    payload: VLMTestRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Test the currently configured VLM on a single stored image."""
    from backend.models.assets import Image

    img = db.query(Image).get(payload.image_id)
    if not img:
        raise HTTPException(status_code=404, detail="Image not found")

    path = Path(img.storage_path)
    if not path.is_absolute():
        path = Path("data_store") / path
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"File not found at {path}")

    with open(path, "rb") as f:
        image_bytes = f.read()

    engine = get_vlm_engine()
    try:
        result = engine.analyze_image(image_bytes, payload.prompt)
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"VLM error: {exc}")

    return {
        "status": "success",
        "engine": type(engine).__name__,
        "is_stub": isinstance(engine, StubEngine) or bool(result.get("stub")),
        "response": result,
    }
----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/api/v1_annotation.py
----- CONTENT START -----
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Any

from backend.database.core import get_db
from backend.services.annotation import AnnotationService
from backend.services.auth import require_tagger
from backend.schemas.annotation import (
    ValidationRequest, 
    ValidationResponse, 
    RegionCreateRequest, 
    ImageWorkItem
)

# Router Definition
router = APIRouter(
    prefix="/v1/workbench",
    tags=["Tagger Workbench"],
    responses={404: {"description": "Not found"}},
)

@router.get("/next", response_model=ImageWorkItem)
async def get_next_image(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    WORKBENCH CORE: Fetches the next image for the user to annotate.
    Uses the 'Least Validated' queue algorithm.
    """
    service = AnnotationService(db)
    image = service.get_next_image_for_user(current_user["id"])
    
    if not image:
        raise HTTPException(status_code=404, detail="No more images in queue")
    
    # Construct response
    # Note: In production, 'url' would be signed S3 link or static Nginx path
    return ImageWorkItem(
        id=image.id,
        filename=image.filename,
        url=f"/static/{image.storage_path}", 
        meta_data=image.meta_data,
        created_at=image.created_at
    )

@router.post("/validate", response_model=ValidationResponse)
async def submit_validation(
    payload: ValidationRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    HITL: Submit a judgment on an attribute (e.g., "This is Modern: 0.9").
    """
    service = AnnotationService(db)
    result = service.create_validation(current_user["id"], payload)
    
    return ValidationResponse(
        id=result.id,
        created_at=result.created_at,
        status="success"
    )

@router.post("/region", status_code=201)
async def create_region(
    payload: RegionCreateRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    HITL: Submit a new manual segmentation (box/polygon).
    """
    service = AnnotationService(db)
    result = service.create_region(current_user["id"], payload)
    
    return {
        "id": result.id,
        "status": "created",
        "label": result.manual_label
    }----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/api/v1_debug.py
----- CONTENT START -----
from __future__ import annotations

import io
import os
from pathlib import Path
from typing import Optional

import numpy as np

try:
    import cv2  # type: ignore
except Exception:  # pragma: no cover - cv2 may not be available in tiny CI images
    cv2 = None  # type: ignore

from fastapi import APIRouter, Depends, HTTPException, Response, status
from sqlalchemy.orm import Session

from backend.database.core import get_db
from backend.models.assets import Image  # type: ignore
from backend.services.auth import CurrentUser, require_tagger

router = APIRouter(prefix="/v1/debug", tags=["Debug / Science"])

def _resolve_image_path(storage_path: str) -> Path:
    """Resolve the on-disk path for a stored image.

    The storage_path column is expected to contain either an absolute
    path or a path relative to the working directory / IMAGE_STORAGE_ROOT.
    We first try the path as-is; if it does not exist and is relative,
    we fall back to IMAGE_STORAGE_ROOT + storage_path.
    """
    raw = Path(storage_path)
    if raw.is_file():
        return raw

    # Try prefixing with IMAGE_STORAGE_ROOT if provided
    root = os.getenv("IMAGE_STORAGE_ROOT")
    if root:
        candidate = Path(root) / storage_path
        if candidate.is_file():
            return candidate

    return raw  # Best-effort; caller will handle missing file

def _compute_edge_map_bytes(path: Path, t1: int = 50, t2: int = 150, l2: bool = True) -> bytes:
    """Compute a Canny edge map PNG for the given image path.

    This mirrors the logic in backend.science.core.AnalysisFrame.compute_derived,
    but is implemented locally to keep the debug endpoint self-contained.

    To keep things efficient in classroom settings, we maintain a tiny on-disk
    cache keyed by (image, thresholds, L2 flag). If a matching PNG already
    exists, we serve it directly instead of recomputing.
    """
    if cv2 is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="cv2 (OpenCV) is not available; cannot compute edge maps.",
        )

    # Compute cache path
    cache_root = os.getenv("IMAGE_DEBUG_CACHE_ROOT") or os.path.join("backend", "data", "debug_edges")
    cache_root_path = Path(cache_root)
    cache_root_path.mkdir(parents=True, exist_ok=True)

    cache_name = f"{path.stem}_edges_{t1}_{t2}_{1 if l2 else 0}.png"
    cache_path = cache_root_path / cache_name

    if cache_path.is_file():
        try:
            return cache_path.read_bytes()
        except Exception:
            # Fall through to recomputation on any read error
            pass

    img_bgr = cv2.imread(str(path))
    if img_bgr is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Could not read image from storage: {path}",
        )

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    # Allow experimentation with thresholds and the L2gradient flag
    edges = cv2.Canny(gray, t1, t2, L2gradient=l2)

    ok, buf = cv2.imencode(".png", edges)
    if not ok:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to encode edge map as PNG.",
        )

    data = buf.tobytes()
    try:
        cache_path.write_bytes(data)
    except Exception:
        # Cache write failure should not break the endpoint
        pass
    return data


@router.get("/images/{image_id}/edges", summary="Return edge-map debug view for an image")
def get_image_edge_map(
    image_id: int,
    t1: int = 50,
    t2: int = 150,
    l2: bool = True,
    db: Session = Depends(get_db),
    user: CurrentUser = Depends(require_tagger),
) -> Response:
    """Serve a PNG edge map for the requested image.

    This endpoint is intended purely for *debug / teaching* purposes. It
    allows Explorer (and other tools) to show "what the algorithm sees"
    when computing complexity and related metrics.
    """
    image: Optional[Image] = db.query(Image).filter(Image.id == image_id).first()
    if image is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Image not found")

    storage_path = getattr(image, "storage_path", None)
    if not storage_path:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Image has no storage_path configured",
        )

    path = _resolve_image_path(storage_path)
    if not path.is_file():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Image file not found on disk: {path}",
        )

    data = _compute_edge_map_bytes(path, t1=t1, t2=t2, l2=l2)
    return Response(content=data, media_type="image/png")
----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/api/v1_discovery.py
----- CONTENT START -----
"""
Explorer / Discovery API (v1).

This module is the canonical adaptor between the Explorer UI and the
database + training export service. It exposes three main endpoints:

- POST /v1/explorer/search
- POST /v1/explorer/export
- GET  /v1/explorer/attributes

All endpoints are RBAC-protected for taggers (and above).
"""

from __future__ import annotations

from typing import List

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from backend.database.session import get_db
from backend.services.auth import require_tagger
from backend.services.training_export import TrainingExporter, TrainingExample
from backend.schemas.discovery import SearchQuery, ImageSearchResult, ExportRequest, AttributeRead
from backend.models.attribute import Attribute

router = APIRouter(prefix="/v1/explorer", tags=["explorer"])


@router.post("/search", response_model=List[ImageSearchResult])
def search_images(
    payload: SearchQuery,
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[ImageSearchResult]:
    """Search for images matching the query.

    This minimal implementation supports:
      - free-text search over image name / description (if available), or
      - returning a simple paginated list when no filter is provided.

    The exact ranking can be improved later; for now we favour determinism
    and well-formed responses over sophistication.
    """
    # Lazy import to avoid circular dependencies
    from backend.models.assets import Image  # type: ignore

    q = db.query(Image)

    if getattr(payload, "text", None):
        text = f"%{payload.text}%"
        # Try to filter by name / description if those fields exist.
        # We guard each attribute access with hasattr to avoid hard failures
        # across slightly different schemas.
        name_col = getattr(Image, "name", None)
        desc_col = getattr(Image, "description", None)
        if name_col is not None and desc_col is not None:
            q = q.filter((name_col.ilike(text)) | (desc_col.ilike(text)))
        elif name_col is not None:
            q = q.filter(name_col.ilike(text))

    limit = max(1, min(getattr(payload, "limit", 50), 200))
    offset = max(0, getattr(payload, "offset", 0))
    q = q.order_by(getattr(Image, "id")).offset(offset).limit(limit)

    images = q.all()
    results: List[ImageSearchResult] = []

    base_thumb_url = "/static/thumbnails"

    for img in images:
        image_id = getattr(img, "id", None)
        if image_id is None:
            continue
        thumb_name = getattr(img, "thumbnail_path", None) or f"image_{image_id}.jpg"
        thumb_url = f"{base_thumb_url}/{thumb_name}"

        # For now, we do not join attributes here; Explorer primarily needs
        # thumbnails and IDs to drive downstream detail views and exports.
        res = ImageSearchResult(image_id=image_id, thumbnail_url=thumb_url, attributes={})
        results.append(res)

    return results


@router.post("/export", response_model=List[TrainingExample])
def export_training_data(
    payload: ExportRequest,
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[TrainingExample]:
    """Export training examples for the given image IDs.

    This is a thin wrapper around the TrainingExporter service, returning
    a list of TrainingExample objects suitable for downstream model training.
    """
    if not payload.image_ids:
        return []

    exporter = TrainingExporter(db=db)
    try:
        examples = exporter.export_for_images(payload.image_ids)
    except Exception as exc:  # pragma: no cover - defensive logging
        raise HTTPException(status_code=500, detail=f"training export failed: {exc}") from exc

    return examples


@router.get("/attributes", response_model=List[AttributeRead])
def list_attributes(
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[AttributeRead]:
    """Return the attribute registry for Explorer filters.

    We expose all Attribute rows, mapping them into AttributeRead records.
    """
    attrs = db.query(Attribute).order_by(Attribute.key).all()
    results: List[AttributeRead] = []
    for attr in attrs:
        key = getattr(attr, "key", None)
        label = getattr(attr, "label", None) or key
        group = getattr(attr, "group", "default")
        if key is None:
            continue
        results.append(AttributeRead(key=key, label=label, group=group, description=getattr(attr, 'description', None) or getattr(attr, 'notes', None)))
    return results----- CONTENT END -----
----- FILE PATH: archive/v3_4_13_sprint1_20251123/backend/science/pipeline.py
----- CONTENT START -----
"""
Science Pipeline Orchestrator v3.3 (Grand Jury Edition).
Integrates DepthAnalyzer and removed IsovistAnalyzer.
"""
import logging
from typing import Optional
import numpy as np
from sqlalchemy.orm import Session

try:
    import cv2
except ImportError:
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame

# v3.3 Modular Imports
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.math.symmetry import SymmetryAnalyzer
from backend.science.math.naturalness import NaturalnessAnalyzer
from backend.science.math.fluency import FluencyAnalyzer
from backend.science.spatial.depth import DepthAnalyzer # Replaces Isovist
from backend.science.context.cognitive import CognitiveStateAnalyzer

logger = logging.getLogger(__name__)

class SciencePipelineConfig:
    def __init__(self, enable_all: bool = True):
        self.enable_color = enable_all
        self.enable_complexity = enable_all
        self.enable_texture = enable_all
        self.enable_fractals = enable_all
        self.enable_spatial = enable_all
        self.enable_cognitive = False # Expensive, default off

class SciencePipeline:
    def __init__(self, db: Session, config: Optional[SciencePipelineConfig] = None):
        self.db = db
        self.config = config or SciencePipelineConfig()
        
        # Init Analyzers
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.symmetry = SymmetryAnalyzer()
        self.naturalness = NaturalnessAnalyzer()
        self.fluency = FluencyAnalyzer()
        self.spatial = DepthAnalyzer() # The new Spatial Engine
        self.cognitive = CognitiveStateAnalyzer()

    def process_image(self, image_id: int) -> bool:
        image_record = self.db.query(Image).get(image_id)
        if not image_record:
            logger.warning(f"Image {image_id} not found.")
            return False

        # Load Image
        rgb = self._load_image(image_record)
        if rgb is None: return False

        # Init Frame
        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            # L0: Physics & Basic Stats
            if self.config.enable_color: self.color.analyze(frame)
            if self.config.enable_complexity: self.complexity.analyze(frame)
            if self.config.enable_texture: self.texture.analyze(frame)
            if self.config.enable_fractals: self.fractals.analyze(frame)
            if self.config.enable_spatial: 
                self.symmetry.analyze(frame)
                self.naturalness.analyze(frame)
                self.spatial.analyze(frame) # Runs Depth/Clutter

            # L1: Perceptual (Dependent on L0)
            if self.config.enable_spatial:
                self.fluency.analyze(frame)

            # L2: Cognitive (VLM)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)

        except Exception:
            logger.exception(f"Analysis failed for image {image_id}")
            return False

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        # Simple local loader
        import os
        path = f"data_store/{image_record.storage_path}"
        if not os.path.exists(path): return None
        bgr = cv2.imread(path)
        if bgr is None: return None
        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            if value != value: value = 0.0 # NaN check
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=float(value),
                source="science_pipeline_v3.3"
            )
            self.db.add(val)
        self.db.commit()
----- CONTENT END -----
----- FILE PATH: archive/v3_4_14_semantic_depth_20251123/VERSION
----- CONTENT START -----
3.4.13_new
----- CONTENT END -----
----- FILE PATH: archive/v3_4_15_feature_nav_20251123/VERSION
----- CONTENT START -----
3.4.14_semantic_depth
----- CONTENT END -----
----- FILE PATH: backend/__init__.py
----- CONTENT START -----
# backend root package----- CONTENT END -----
----- FILE PATH: backend/main.py
----- CONTENT START -----
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles

from backend.services.storage import get_image_storage_root
from backend.api import (
    v1_annotation,
    v1_admin,
    v1_supervision,
    v1_discovery,
    v1_bn_export,
    v1_debug,
    v1_features,
    v1_vlm_health,
)
from backend.versioning import VERSION

# v3 Enterprise Application Entry Point
app = FastAPI(
    title=f"Image Tagger v3 (v{VERSION})",
    description="Unified API for Tagger Workbench, Supervisor, Admin, and Explorer.",
    version=VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
)

# Router wiring
app.include_router(v1_annotation.router)
app.include_router(v1_admin.router)
app.include_router(v1_supervision.router)
app.include_router(v1_discovery.router)
app.include_router(v1_bn_export.router)
app.include_router(v1_debug.router)
app.include_router(v1_features.router)
app.include_router(v1_vlm_health.router)

# Static file mount for image assets
IMAGE_STORAGE_ROOT = get_image_storage_root()
app.mount("/static", StaticFiles(directory=str(IMAGE_STORAGE_ROOT)), name="static")


@app.get("/health")
def health_check():
    """Kubernetes/Docker Health Probe"""
    return {"status": "healthy", "version": VERSION}


@app.get("/")
def root():
    return {
        "message": "Image Tagger v3 API",
        "docs": "/docs",
        "workbench_api": "/v1/workbench/next",
    }
----- CONTENT END -----
----- FILE PATH: backend/versioning.py
----- CONTENT START -----
"""
Centralized version helper.

The VERSION file at the repository root is the single source of truth for
the Image Tagger version string. Other components should import VERSION
from this module instead of hard-coding a value.
"""

from pathlib import Path


def _read_version() -> str:
    """
    Return the current Image Tagger version string.

    We resolve the repository root by going one directory up from the
    backend/ package and looking for a VERSION file there.

    If anything goes wrong (e.g. the file is missing inside a container),
    we fall back to "dev".
    """
    try:
        root = Path(__file__).resolve().parents[1]
        version_file = root / "VERSION"
        return version_file.read_text(encoding="utf-8").strip()
    except Exception:
        return "dev"


VERSION: str = _read_version()
----- CONTENT END -----
----- FILE PATH: backend/api/__init__.py
----- CONTENT START -----
# backend/api package----- CONTENT END -----
----- FILE PATH: backend/api/v1_admin.py
----- CONTENT START -----
import os
import logging
from pydantic import BaseModel
from typing import Any, Dict, List, Optional
from pathlib import Path
from fastapi import UploadFile, File,  APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import select, update, func

from backend.database.core import get_db
from backend.services.auth import require_admin
from backend.services.storage import get_image_storage_root, to_static_path
from backend.services.costs import get_total_spent, log_vlm_usage
from backend.services.vlm import describe_vlm_configuration, update_vlm_config, get_vlm_engine, StubEngine

logger = logging.getLogger(__name__)
from backend.models.config import ToolConfig
from backend.schemas.admin import ToolConfigRead, ToolConfigUpdate, BudgetStatus
from backend.schemas.discovery import ExportRequest
from backend.schemas.training import TrainingExample
from backend.services.training_export import TrainingExporter


router = APIRouter(prefix="/v1/admin", tags=["Admin Cockpit"])


@router.get("/models", response_model=list[ToolConfigRead])
async def list_models(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> list[ToolConfigRead]:
    """Return all ToolConfig rows.

    This powers the 'AI Models' section of the Admin Cockpit.
    """
    tools = db.execute(select(ToolConfig)).scalars().all()
    return [ToolConfigRead.model_validate(t) for t in tools]


@router.patch("/models/{tool_id}", response_model=ToolConfigRead)
async def update_model(
    tool_id: int,
    payload: ToolConfigUpdate,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> ToolConfigRead:
    """Update a single model's enabled state and/or cost.

    Frontend calls this when toggling a model or editing its cost.
    """
    tool = db.get(ToolConfig, tool_id)
    if tool is None:
        raise HTTPException(status_code=404, detail="ToolConfig not found")

    if payload.is_enabled is not None:
        tool.is_enabled = payload.is_enabled
    if payload.cost_per_1k_tokens is not None:
        tool.cost_per_1k_tokens = float(payload.cost_per_1k_tokens)

    db.add(tool)
    db.commit()
    db.refresh(tool)
    return ToolConfigRead.model_validate(tool)



def _get_budget_hard_limit() -> float:
    """Resolve the hard USD budget limit for VLM usage.

    Priority:
    1. VLM_HARD_LIMIT_USD env var
    2. COST_HARD_LIMIT_USD env var
    3. Conservative default of 15.0 USD
    """ 
    for key in ("VLM_HARD_LIMIT_USD", "COST_HARD_LIMIT_USD"):
        raw = os.getenv(key)
        if raw is None:
            continue
        try:
            value = float(raw)
            if value > 0:
                return value
        except (TypeError, ValueError):
            continue
    return 15.0

@router.get("/budget", response_model=BudgetStatus)
async def get_budget(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> BudgetStatus:
    """Return a simple cost snapshot and kill-switch state.

    Numbers are derived from the ToolUsage ledger
    (see backend.services.costs) and the kill-switch is derived from
    whether any paid models (cost_per_1k_tokens > 0) remain enabled.
    """
    paid_enabled = db.execute(
        select(ToolConfig).where(
            ToolConfig.cost_per_1k_tokens > 0,
            ToolConfig.is_enabled.is_(True),
        ).limit(1)
    ).first()
    is_kill_switched = paid_enabled is None

    # Compute current spend from the ToolUsage ledger.
    total_spent = get_total_spent()
    hard_limit = _get_budget_hard_limit()

    return BudgetStatus(
        total_spent=total_spent,
        hard_limit=hard_limit,
        is_kill_switched=is_kill_switched,
    )



@router.get("/costs/daily")
async def get_daily_costs(
    days: int = 30,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> List[Dict[str, Any]]:
    """Return a simple daily cost time-series for the Admin cost chart.

    The response is a list of objects of the form:
    { "day": "YYYY-MM-DD", "total_cost": float }
    """
    from datetime import datetime, timedelta

    from backend.models.usage import ToolUsage

    if days <= 0:
        days = 1
    if days > 90:
        days = 90

    now = datetime.utcnow()
    start_ts = now - timedelta(days=days - 1)

    stmt = (
        select(
            func.date(ToolUsage.created_at).label("day"),
            func.coalesce(func.sum(ToolUsage.cost_usd), 0.0).label("total_cost"),
        )
        .where(ToolUsage.created_at >= start_ts)
        .group_by(func.date(ToolUsage.created_at))
        .order_by(func.date(ToolUsage.created_at))
    )

    rows = db.execute(stmt).all()
    series: List[Dict[str, Any]] = []
    for row in rows:
        # row may be a Row or tuple depending on SQLAlchemy version.
        day_val = getattr(row, "day", None)
        total_val = getattr(row, "total_cost", None)
        if day_val is None:
            day_val = row[0]
        if total_val is None:
            total_val = row[1]

        # Normalise to a simple ISO date string.
        day_str = getattr(day_val, "isoformat", lambda: str(day_val))()
        series.append(
            {
                "day": day_str[:10],
                "total_cost": float(total_val or 0.0),
            }
        )

    return series

@router.post("/kill-switch", response_model=BudgetStatus)
async def kill_switch(
    active: bool,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> BudgetStatus:
    """Global kill switch for paid models.

    When activated, all ToolConfigs with cost_per_1k_tokens > 0 are disabled.
    We then return a fresh BudgetStatus so the UI can update seamlessly.
    """
    if active:
        stmt = (
            update(ToolConfig)
            .where(ToolConfig.cost_per_1k_tokens > 0)
            .values(is_enabled=False)
        )
        db.execute(stmt)
        db.commit()

    # Re-use the logic from get_budget to compute state
    paid_enabled = db.execute(
        select(ToolConfig).where(
            ToolConfig.cost_per_1k_tokens > 0,
            ToolConfig.is_enabled.is_(True),
        ).limit(1)
    ).first()
    is_kill_switched = paid_enabled is None

    total_spent = get_total_spent()
    hard_limit = _get_budget_hard_limit()

    return BudgetStatus(
        total_spent=total_spent,
        hard_limit=hard_limit,
        is_kill_switched=is_kill_switched,
    )
@router.post("/training/export", response_model=list[TrainingExample])
async def admin_export_training(
    request: ExportRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> list[TrainingExample]:
    """
    Admin-facing training export endpoint.

    This mirrors the Explorer export but is scoped to admin RBAC and
    can be extended with richer filters (quality thresholds, time
    windows) in future steps.
    """
    exporter = TrainingExporter(db=db)
    examples = exporter.export_for_images(request.image_ids)
    return [TrainingExample(**e) for e in examples]

@router.get("/export/images")
def export_all_images(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
):
    """Download a zip of all stored image files.

    This is an admin-only convenience endpoint for researchers who
    need local copies of the raw assets. In typical classroom
    usage, datasets are modest in size, so an in-memory zip is
    acceptable.
    """
    import io as _io
    import os as _os
    import zipfile as _zip
    from pathlib import Path as _Path
    from backend.models.assets import Image  # type: ignore

    try:
        from backend.settings import IMAGE_STORAGE_ROOT  # type: ignore
    except Exception:
        IMAGE_STORAGE_ROOT = _os.getenv("IMAGE_STORAGE_ROOT", "data_store")

    images = db.query(Image).all()
    if not images:
        raise HTTPException(status_code=404, detail="No images available for export")

    buf = _io.BytesIO()
    root = _Path(IMAGE_STORAGE_ROOT)

    with _zip.ZipFile(buf, "w", _zip.ZIP_DEFLATED) as zf:
        for img in images:
            path = _Path(img.storage_path)
            if not path.is_absolute():
                path = root / path
            if not path.exists():
                # Skip missing files rather than failing completely.
                continue
            try:
                arcname = path.relative_to(root)
            except Exception:
                arcname = path.name
            zf.write(path, arcname=str(arcname))

    buf.seek(0)
    headers = {
        "Content-Disposition": 'attachment; filename="image_tagger_images_export.zip"'
    }
    return StreamingResponse(buf, media_type="application/zip", headers=headers)
class AdminUploadResult(BaseModel):
    created_count: int
    image_ids: List[int]
    storage_paths: List[str]

    job_id: Optional[int] = None

# v3.4.36: Explicit constants for upload hardening.
ALLOWED_UPLOAD_SUFFIXES = {".jpg", ".jpeg", ".png", ".webp"}
MAX_UPLOAD_BYTES = 10 * 1024 * 1024  # 10 MiB per file
MAX_UPLOAD_FILES = 200  # Safety guard to avoid browser/HTTP timeouts on huge batches.


@router.post("/upload", response_model=AdminUploadResult, status_code=202)
async def upload_images(
    files: List[UploadFile] = File(...),
    db: Session = Depends(get_db),
    user = Depends(require_admin),
    background_tasks: BackgroundTasks = None,
) -> AdminUploadResult:
    """Upload one or more images and enqueue a background science job.

    This accepts one or more image files and stores them under the
    configured IMAGE_STORAGE_ROOT. For each stored file, a new Image
    row is created in the database.

    v3.4.36: hardened against oversized and non-image uploads; filenames are
    normalised to random UUIDs to avoid path traversal and collisions.

    v3.4.50: additionally creates an UploadJob so the science pipeline
    can be executed asynchronously without holding the HTTP request
    open for large batches.
    """
    import os as _os
    from pathlib import Path as _Path
    from uuid import uuid4

    from backend.models.assets import Image  # local import to avoid cycles

    try:
        from backend.settings import IMAGE_STORAGE_ROOT  # type: ignore[attr-defined]
        storage_root = IMAGE_STORAGE_ROOT
    except Exception:
        storage_root = _os.getenv("IMAGE_STORAGE_ROOT", "data_store")

    root = _Path(storage_root)
    root.mkdir(parents=True, exist_ok=True)

    created_ids: List[int] = []
    storage_paths: List[str] = []
    original_names: List[str] = []

    if not files:
        raise HTTPException(status_code=400, detail="No files provided for upload.")

    if len(files) > MAX_UPLOAD_FILES:
        raise HTTPException(
            status_code=400,
            detail=(
                f"Too many files in one batch ({len(files)} > {MAX_UPLOAD_FILES}); "
                "please upload in smaller chunks."
            ),
        )

    for f in files:
        original_name = f.filename or "uploaded"
        suffix = "".join(_Path(original_name).suffixes).lower() or ".jpg"

        if suffix not in ALLOWED_UPLOAD_SUFFIXES:
            raise HTTPException(
                status_code=400,
                detail=(
                    f"Unsupported file type for '{original_name}'. "
                    f"Allowed: {sorted(ALLOWED_UPLOAD_SUFFIXES)}"
                ),
            )

        content = await f.read()
        if not content:
            # Skip empty files rather than failing the whole batch.
            continue

        if len(content) > MAX_UPLOAD_BYTES:
            raise HTTPException(
                status_code=400,
                detail=(
                    f"File '{original_name}' is too large; "
                    f"max allowed size is {MAX_UPLOAD_BYTES // (1024 * 1024)} MiB."
                ),
            )

        unique_name = f"{uuid4().hex}{suffix}"
        dest = root / unique_name
        dest.write_bytes(content)

        image = Image(
            filename=original_name,
            storage_path=unique_name,
            meta_data={},
            source="admin_upload",
        )
        db.add(image)
        db.flush()
        created_ids.append(image.id)
        storage_paths.append(str(dest))
        original_names.append(original_name)

    if not created_ids:
        raise HTTPException(
            status_code=400,
            detail="No valid image files were uploaded.",
        )

    db.commit()

    job_id: Optional[int] = None

    # Enqueue a science job in the background. Failures here should not
    # cause the upload itself to be reported as failed.
    try:
        from backend.services.upload_jobs import (
            create_upload_job_for_images,
            run_upload_job,
        )

        records = list(zip(created_ids, storage_paths, original_names))
        job = create_upload_job_for_images(
            db=db,
            user_id=getattr(user, "id", None),
            records=records,
        )
        job_id = job.id
        if background_tasks is not None:
            background_tasks.add_task(run_upload_job, job.id)
        else:
            logger.warning(
                "upload_images called without BackgroundTasks; job %s "
                "will not be executed automatically.",
                job.id,
            )
    except Exception as exc:  # pragma: no cover - defensive
        logger.exception("Failed to enqueue upload job: %s", exc)

    return AdminUploadResult(
        created_count=len(created_ids),
        image_ids=created_ids,
        storage_paths=storage_paths,
        job_id=job_id,
    )



@router.get("/upload/jobs")
async def list_upload_jobs(
    limit: int = 20,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> List[Dict[str, Any]]:
    """List recent UploadJobs for monitoring asynchronous uploads."""
    from backend.models.jobs import UploadJob

    if limit <= 0:
        limit = 1
    if limit > 100:
        limit = 100

    stmt = (
        select(UploadJob)
        .order_by(UploadJob.created_at.desc())
        .limit(limit)
    )
    jobs = db.execute(stmt).scalars().all()

    results: List[Dict[str, Any]] = []
    for job in jobs:
        results.append(
            {
                "id": job.id,
                "status": job.status,
                "total_items": job.total_items,
                "completed_items": job.completed_items,
                "failed_items": job.failed_items,
                "created_at": job.created_at.isoformat()
                if getattr(job, "created_at", None)
                else None,
                "error_summary": job.error_summary,
            }
        )
    return results


@router.get("/upload/jobs/{job_id}")
async def get_upload_job(
    job_id: int,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Return details for a single UploadJob, including item statuses."""
    from backend.models.jobs import UploadJob

    job = db.get(UploadJob, job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="UploadJob not found.")

    items_payload: List[Dict[str, Any]] = []
    for item in job.items:
        items_payload.append(
            {
                "id": item.id,
                "image_id": item.image_id,
                "filename": item.filename,
                "storage_path": item.storage_path,
                "status": item.status,
                "error_message": item.error_message,
                "created_at": item.created_at.isoformat()
                if getattr(item, "created_at", None)
                else None,
            }
        )

    return {
        "id": job.id,
        "status": job.status,
        "total_items": job.total_items,
        "completed_items": job.completed_items,
        "failed_items": job.failed_items,
        "created_at": job.created_at.isoformat()
        if getattr(job, "created_at", None)
        else None,
        "error_summary": job.error_summary,
        "items": items_payload,
    }

class VLMConfigRequest(BaseModel):
    provider: str = "auto"
    cognitive_prompt_override: Optional[str] = None
    max_batch_size: Optional[int] = None
    cost_per_1k_images_usd: Optional[float] = None


@router.get("/vlm/config")
def get_vlm_config(
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Return the current VLM configuration and detected backends."""
    return describe_vlm_configuration()


@router.post("/vlm/config")
def set_vlm_config(
    payload: VLMConfigRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Persist the chosen VLM settings (provider, prompt override, batch size, cost hint)."""
    return update_vlm_config(
        provider=payload.provider,
        cognitive_prompt_override=payload.cognitive_prompt_override,
        max_batch_size=payload.max_batch_size,
        cost_per_1k_images_usd=payload.cost_per_1k_images_usd,
    )


class VLMTestRequest(BaseModel):
    image_id: int
    prompt: str = "Describe this architectural image in one or two sentences."


@router.post("/vlm/test")
def test_vlm_configuration(
    payload: VLMTestRequest,
    db: Session = Depends(get_db),
    user = Depends(require_admin),
) -> Dict[str, Any]:
    """Test the currently configured VLM on a single stored image."""
    from backend.models.assets import Image

    img = db.query(Image).get(payload.image_id)
    if not img:
        raise HTTPException(status_code=404, detail="Image not found")

    path = Path(img.storage_path)
    if not path.is_absolute():
        path = Path("data_store") / path
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"File not found at {path}")

    with open(path, "rb") as f:
        image_bytes = f.read()

    engine = get_vlm_engine()
    try:
        result = engine.analyze_image(image_bytes, payload.prompt)
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"VLM error: {exc}")

    return {
        "status": "success",
        "engine": type(engine).__name__,
        "is_stub": isinstance(engine, StubEngine) or bool(result.get("stub")),
        "response": result,
    }----- CONTENT END -----
----- FILE PATH: backend/api/v1_annotation.py
----- CONTENT START -----
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Any

from backend.database.core import get_db
from backend.services.storage import to_static_path
from backend.services.annotation import AnnotationService
from backend.services.auth import require_tagger
from backend.schemas.annotation import (
    ValidationRequest, 
    ValidationResponse, 
    RegionCreateRequest, 
    ImageWorkItem
)

# Router Definition
router = APIRouter(
    prefix="/v1/workbench",
    tags=["Tagger Workbench"],
    responses={404: {"description": "Not found"}},
)

@router.get("/next", response_model=ImageWorkItem)
async def get_next_image(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    WORKBENCH CORE: Fetches the next image for the user to annotate.
    Uses the 'Least Validated' queue algorithm.
    """
    service = AnnotationService(db)
    image = service.get_next_image_for_user(current_user["id"])
    
    if not image:
        raise HTTPException(status_code=404, detail="No more images in queue")
    
    # Construct response
    # Note: In production, 'url' would be signed S3 link or static Nginx path
    return ImageWorkItem(
        id=image.id,
        filename=image.filename,
        url=f"/static/{to_static_path(image.storage_path)}", 
        meta_data=image.meta_data,
        created_at=image.created_at
    )

@router.post("/validate", response_model=ValidationResponse)
async def submit_validation(
    payload: ValidationRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    HITL: Submit a judgment on an attribute (e.g., "This is Modern: 0.9").
    """
    service = AnnotationService(db)
    result = service.create_validation(current_user["id"], payload)
    
    return ValidationResponse(
        id=result.id,
        created_at=result.created_at,
        status="success"
    )

@router.post("/region", status_code=201)
async def create_region(
    payload: RegionCreateRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_tagger)
):
    """
    HITL: Submit a new manual segmentation (box/polygon).
    """
    service = AnnotationService(db)
    result = service.create_region(current_user["id"], payload)
    
    return {
        "id": result.id,
        "status": "created",
        "label": result.manual_label
    }----- CONTENT END -----
----- FILE PATH: backend/api/v1_bn_export.py
----- CONTENT START -----
"""BN export API for Image Tagger v3.

This router exposes a minimal endpoint to export BN-ready rows that contain
science indices and their bins for each image, based on the Validation table.
"""

from __future__ import annotations

from typing import Dict, List, Optional

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from backend.database.core import get_db
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.schemas.bn_export import BNRow, BNCodebook, BNVariable, BNValidationRow
from backend.science.index_catalog import get_candidate_bn_keys, get_index_metadata

router = APIRouter(tags=["bn_export"], prefix="/v1/export")


BIN_LABELS = {0.0: "low", 1.0: "mid", 2.0: "high"}
# Also support integer keys just in case
BIN_LABELS.update({0: "low", 1: "mid", 2: "high"})


def _collect_indices_for_image(
    session: Session,
    image_id: int,
    keys: List[str],
) -> Dict[str, Optional[float]]:
    """Collect continuous index values for the given image from Validation.

    We treat the latest Validation per (image_id, attribute_key, source) as the
    canonical value, restricted to entries produced by the science pipeline.
    """
    indices: Dict[str, Optional[float]] = {k: None for k in keys}

    q = (
        session.query(Validation)
        .filter(Validation.image_id == image_id)
        .filter(Validation.attribute_key.in_(keys))
        .filter(Validation.source.like("science_pipeline%"))
    )

    for row in q:
        key = row.attribute_key
        if key in indices and row.value is not None:
            # Last write wins; this keeps the implementation simple.
            indices[key] = float(row.value)

    return indices


def _collect_bins_for_image(session: Session, image_id: int) -> Dict[str, Optional[str]]:
    """Collect bin labels for composite indices from Validation.

    The underlying science pipeline currently stores bins as numeric codes
    (0, 1, 2). We map these to string labels (low, mid, high) for BN and UX.
    """
    bins: Dict[str, Optional[str]] = {}

    metadata = get_index_metadata()
    bin_keys: List[str] = []
    for _, info in metadata.items():
        binspec = info.get("bins")
        if not binspec:
            continue
        field = binspec.get("field")
        if field:
            bin_keys.append(field)

    if not bin_keys:
        return bins

    q = (
        session.query(Validation)
        .filter(Validation.image_id == image_id)
        .filter(Validation.attribute_key.in_(bin_keys))
        .filter(Validation.source.like("science_pipeline%"))
    )

    for row in q:
        key = row.attribute_key
        raw = row.value
        label: Optional[str] = None
        if raw is not None:
            label = BIN_LABELS.get(raw)
        bins[key] = label

    # Ensure all expected bin keys are present in the dict
    for k in bin_keys:
        bins.setdefault(k, None)

    return bins




def _compute_irr_for_image(session: Session, image_id: int) -> Optional[float]:
    """Compute a simple IRR score for a given image across all attributes.

    This mirrors the pairwise-agreement logic used in the Supervisor's /irr
    endpoint, but without any time-window filter. For each attribute with
    at least two validations, we compute the fraction of agreeing pairs of
    ratings, then average across attributes.

    Returns None if there is insufficient overlapping data.
    """
    # Fetch all validations for this image (human + pipeline). We do not
    # filter by source here; downstream BN tools can decide how to treat
    # low-IRR images.
    from collections import defaultdict

    rows = session.query(
        Validation.attribute_key,
        Validation.value,
    ).filter(Validation.image_id == image_id).all()

    if not rows:
        return None

    grouped: Dict[str, list] = defaultdict(list)
    for attr_key, value in rows:
        grouped[attr_key].append(value)

    scores = []
    for _attr_key, values in grouped.items():
        n = len(values)
        if n < 2:
            continue
        total_pairs = 0
        agree_pairs = 0
        for i in range(n):
            vi = values[i]
            for j in range(i + 1, n):
                vj = values[j]
                total_pairs += 1
                if vi == vj:
                    agree_pairs += 1

        if total_pairs == 0:
            continue

        scores.append(agree_pairs / total_pairs)

    if not scores:
        return None

    return float(sum(scores) / len(scores))


def _bin_irr(score: Optional[float]) -> Optional[str]:
    """Map an IRR score into a coarse bin.

    This is intentionally coarse so BN tools can quickly filter to
    "high-consensus" images (e.g., irr_bin == "high").
    """
    if score is None:
        return None
    if score < 0.4:
        return "low"
    if score < 0.7:
        return "medium"
    return "high"


@router.get("/bn-snapshot", response_model=List[BNRow])
def export_bn_snapshot(db: Session = Depends(get_db)) -> List[BNRow]:
    """Export a BN-ready snapshot of science indices and bins for each image.

    This endpoint reads from the Validation table, restricted to entries whose
    `source` starts with "science_pipeline". It is intended as a stable,
    inspectable contract for downstream BN tools, not as a full-featured data
    warehouse API.
    """
    candidate_keys = get_candidate_bn_keys()

    # Collect all image IDs
    image_ids: List[int] = [row.id for row in db.query(Image.id)]

    rows: List[BNRow] = []
    for image_id in image_ids:
        indices = _collect_indices_for_image(db, image_id, candidate_keys)
        bins = _collect_bins_for_image(db, image_id)
        irr = _compute_irr_for_image(db, image_id)
        irr_bin = _bin_irr(irr)

        rows.append(
            BNRow(
                image_id=image_id,
                source="image_tagger_v3.4.65",
                indices=indices,
                bins=bins,
                agreement_score=irr,
                irr_bin=irr_bin,
            )
        )

    return rows




@router.get("/bn-validations", response_model=List[BNValidationRow])
def export_bn_validations(db: Session = Depends(get_db)) -> List[BNValidationRow]:
    """Export one row per Validation record for hierarchical models.

    This endpoint exposes a flattened view over the Validation table
    for science-pipeline and manual sources, allowing downstream
    tools to model individual tagger bias, learning curves, and
    multi-level structures.
    """
    rows: List[BNValidationRow] = []

    q = (
        db.query(
            Validation.image_id,
            Validation.user_id,
            Validation.attribute_key,
            Validation.value,
            Validation.source,
            Validation.duration_ms,
        )
        .filter(
            Validation.source.like("science_pipeline%")
            | (Validation.source == "manual")
        )
    )

    for image_id, user_id, key, value, source, duration_ms in q:
        if value is None:
            continue
        rows.append(
            BNValidationRow(
                image_id=image_id,
                user_id=user_id,
                attribute_key=key,
                value=float(value),
                source=source,
                duration_ms=duration_ms or 0,
            )
        )

    return rows


@router.get("/bn-codebook", response_model=BNCodebook)
def get_bn_codebook() -> BNCodebook:
    """Return a codebook describing the BN export variables.

    This endpoint exposes the domain of each variable emitted by the
    `/v1/export/bn-snapshot` endpoint so that downstream BN /
    probabilistic programming tools do not need to guess at data types
    or valid states.
    """
    index_metadata = get_index_metadata()
    candidate_keys = get_candidate_bn_keys()

    variables: List[BNVariable] = []

    # Continuous index variables (science indices used as BN inputs)
    for key in candidate_keys:
        info = index_metadata.get(key, {})
        label = info.get("label", key)
        description = info.get("description", "")
        variables.append(
            BNVariable(
                name=key,
                label=label,
                description=description,
                role="index",
                var_type="continuous",
                states=None,
            )
        )

    # Bin variables derived from index metadata
    bin_fields: Dict[str, List[str]] = {}
    for _key, info in index_metadata.items():
        binspec = info.get("bins")
        if not binspec:
            continue
        field = binspec.get("field")
        if not field:
            continue
        values = binspec.get("values") or ["low", "mid", "high"]
        bin_fields[field] = list(values)

    for field, values in sorted(bin_fields.items()):
        variables.append(
            BNVariable(
                name=field,
                label=field,
                description=f"Bin for {field}",
                role="bin",
                var_type="ordinal",
                states=values,
            )
        )

    # Inter-rater reliability variables
    variables.append(
        BNVariable(
            name="agreement_score",
            label="agreement_score",
            description=(
                "Average inter-rater agreement for this image (0‚Äì1), "
                "computed as mean pairwise agreement across attributes."
            ),
            role="meta",
            var_type="continuous",
            states=None,
        )
    )
    variables.append(
        BNVariable(
            name="irr_bin",
            label="irr_bin",
            description=(
                "Coarse IRR bin derived from agreement_score: "
                "low / medium / high."
            ),
            role="meta",
            var_type="ordinal",
            states=["low", "medium", "high"],
        )
    )

    # Identifier + provenance variables
    variables.append(
        BNVariable(
            name="image_id",
            label="image_id",
            description="Numeric identifier of the image.",
            role="id",
            var_type="discrete",
            states=None,
        )
    )
    variables.append(
        BNVariable(
            name="source",
            label="source",
            description=(
                "Origin of this BN row (typically an Image Tagger "
                "version string)."
            ),
            role="meta",
            var_type="discrete",
            states=None,
        )
    )

    return BNCodebook(variables=variables)

----- CONTENT END -----
----- FILE PATH: backend/api/v1_debug.py
----- CONTENT START -----
from __future__ import annotations

import io
import os
from pathlib import Path
from typing import Optional

import numpy as np

try:
    import cv2  # type: ignore
except Exception:  # pragma: no cover - cv2 may not be available in tiny CI images
    cv2 = None  # type: ignore

from fastapi import APIRouter, Depends, HTTPException, Response, status
from backend.science import pipeline as science_pipeline
from backend.science.core import AnalysisFrame
from backend.science.spatial.depth import DepthAnalyzer
from sqlalchemy.orm import Session

from backend.database.core import get_db
from backend.models.assets import Image  # type: ignore
from backend.services.auth import CurrentUser, require_tagger

router = APIRouter(prefix="/v1/debug", tags=["Debug / Science"])

def _resolve_image_path(storage_path: str) -> Path:
    """Resolve the on-disk path for a stored image.

    The storage_path column is expected to contain either an absolute
    path or a path relative to the working directory / IMAGE_STORAGE_ROOT.
    We first try the path as-is; if it does not exist and is relative,
    we fall back to IMAGE_STORAGE_ROOT + storage_path.
    """
    raw = Path(storage_path)
    if raw.is_file():
        return raw

    # Try prefixing with IMAGE_STORAGE_ROOT if provided
    root = os.getenv("IMAGE_STORAGE_ROOT")
    if root:
        candidate = Path(root) / storage_path
        if candidate.is_file():
            return candidate

    return raw  # Best-effort; caller will handle missing file

def _compute_edge_map_bytes(path: Path, t1: int = 50, t2: int = 150, l2: bool = True) -> bytes:
    """Compute a Canny edge map PNG for the given image path.

    This mirrors the logic in backend.science.core.AnalysisFrame.compute_derived,
    but is implemented locally to keep the debug endpoint self-contained.

    To keep things efficient in classroom settings, we maintain a tiny on-disk
    cache keyed by (image, thresholds, L2 flag). If a matching PNG already
    exists, we serve it directly instead of recomputing.
    """
    if cv2 is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="cv2 (OpenCV) is not available; cannot compute edge maps.",
        )

    # Compute cache path
    cache_root = os.getenv("IMAGE_DEBUG_CACHE_ROOT") or os.path.join("backend", "data", "debug_edges")
    cache_root_path = Path(cache_root)
    cache_root_path.mkdir(parents=True, exist_ok=True)

    cache_name = f"{path.stem}_edges_{t1}_{t2}_{1 if l2 else 0}.png"
    cache_path = cache_root_path / cache_name

    if cache_path.is_file():
        try:
            return cache_path.read_bytes()
        except Exception:
            # Fall through to recomputation on any read error
            pass

    img_bgr = cv2.imread(str(path))
    if img_bgr is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Could not read image from storage: {path}",
        )

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    # Allow experimentation with thresholds and the L2gradient flag
    edges = cv2.Canny(gray, t1, t2, L2gradient=l2)

    ok, buf = cv2.imencode(".png", edges)
    if not ok:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to encode edge map as PNG.",
        )

    data = buf.tobytes()
    try:
        cache_path.write_bytes(data)
    except Exception:
        # Cache write failure should not break the endpoint
        pass
    return data



def _compute_depth_map_bytes(path: Path) -> bytes:
    """Compute a depth-map PNG for the given image path.

    This uses the DepthAnalyzer's monocular depth model if it is configured
    (via DEPTH_ANYTHING_ONNX_PATH and onnxruntime). If depth inference is
    not available, we surface a 503 so that the frontend can show a clear
    maintenance overlay rather than silently failing.

    The returned PNG is a single-channel grayscale image where lighter
    pixels correspond to *farther* regions and darker pixels to nearer
    regions, after a simple per-image normalisation.
    """
    if cv2 is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="cv2 (OpenCV) is not available; cannot compute depth maps.",
        )

    # Compute cache path
    cache_root = os.getenv("IMAGE_DEPTH_DEBUG_CACHE_ROOT") or os.path.join("backend", "data", "debug_depth")
    cache_root_path = Path(cache_root)
    cache_root_path.mkdir(parents=True, exist_ok=True)

    cache_name = f"{path.stem}_depth.png"
    cache_path = cache_root_path / cache_name

    if cache_path.is_file():
        try:
            return cache_path.read_bytes()
        except Exception:
            # Fall through to recomputation on any read error
            pass

    img_bgr = cv2.imread(str(path))
    if img_bgr is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Could not read image from storage: {path}",
        )

    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

    # Minimal AnalysisFrame: we only need original_image and a dummy id.
    frame = AnalysisFrame(image_id=-1, original_image=img_rgb)

    depth = DepthAnalyzer._compute_depth_map(frame)
    if depth is None:
        # Surface as a 503 so clients know depth debug is temporarily
        # unavailable (e.g. missing model weights or onnxruntime).
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=(
                "Depth debug is not configured. Ensure DEPTH_ANYTHING_ONNX_PATH "
                "is set and onnxruntime is installed."
            ),
        )

    import numpy as _np  # local alias to keep debug module dependency-light

    arr = _np.asarray(depth, dtype="float32")
    if arr.ndim == 3:
        arr = arr[..., 0]
    if arr.ndim != 2 or arr.size == 0:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Depth model returned an invalid depth map.",
        )

    # Normalise depth to [0, 1] per-image to maximise visual contrast.
    d_min = float(_np.nanmin(arr))
    d_max = float(_np.nanmax(arr))
    if not _np.isfinite(d_min) or not _np.isfinite(d_max):
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Depth map contained only NaN / Inf values.",
        )

    if d_max > d_min:
        norm = (arr - d_min) / (d_max - d_min)
    else:
        norm = _np.zeros_like(arr, dtype="float32")

    norm = _np.clip(norm, 0.0, 1.0)
    depth_uint8 = (norm * 255.0).astype("uint8")

    ok, buf = cv2.imencode(".png", depth_uint8)
    if not ok:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to encode depth map as PNG.",
        )

    data = buf.tobytes()
    try:
        cache_path.write_bytes(data)
    except Exception:
        # Cache write failure should not break the endpoint
        pass
    return data


@router.get("/images/{image_id}/edges", summary="Return edge-map debug view for an image")
def get_image_edge_map(
    image_id: int,
    t1: int = 50,
    t2: int = 150,
    l2: bool = True,
    db: Session = Depends(get_db),
    user: CurrentUser = Depends(require_tagger),
) -> Response:
    """Serve a PNG edge map for the requested image.

    This endpoint is intended purely for *debug / teaching* purposes. It
    allows Explorer (and other tools) to show "what the algorithm sees"
    when computing complexity and related metrics.
    """
    image: Optional[Image] = db.query(Image).filter(Image.id == image_id).first()
    if image is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Image not found")

    storage_path = getattr(image, "storage_path", None)
    if not storage_path:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Image has no storage_path configured",
        )

    path = _resolve_image_path(storage_path)
    if not path.is_file():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Image file not found on disk: {path}",
        )

    data = _compute_edge_map_bytes(path, t1=t1, t2=t2, l2=l2)
    return Response(content=data, media_type="image/png")


@router.get("/images/{image_id}/depth", summary="Return depth-map debug view for an image")
def get_image_depth_map(
    image_id: int,
    db: Session = Depends(get_db),
    user: CurrentUser = Depends(require_tagger),
) -> Response:
    """Serve a PNG depth map for the requested image.

    This endpoint is intended purely for *debug / teaching* purposes. It
    exposes the monocular depth prediction used by the spatial metrics so
    that students can see "what the model thinks is near vs far".

    If the depth model is not configured, the endpoint returns HTTP 503 so
    that the frontend can surface a clear maintenance overlay instead of
    a generic network error.
    """
    from backend.models.assets import Image  # local import to avoid circularity

    session: Session = db
    image = session.query(Image).filter(Image.id == image_id).one_or_none()
    if image is None or not image.storage_path:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Image {image_id} not found or has no storage_path.",
        )

    path = _resolve_image_path(image.storage_path)

    data = _compute_depth_map_bytes(path)
    return Response(content=data, media_type="image/png")
@router.get("/pipeline_health")
def pipeline_health() -> dict:
    """Return a lightweight view of the science pipeline health.

    This avoids hitting the database and instead instantiates the
    configured analyzers directly, grouping them by tier and
    reporting their requires/provides contracts.
    """
    summary: dict = {
        "import_ok": True,
        "cv2_available": getattr(science_pipeline, "cv2", None) is not None,
        "analyzers_by_tier": {},
        "warnings": [],
    }

    analyzer_class_names = [
        "ColorAnalyzer",
        "ComplexityAnalyzer",
        "TextureAnalyzer",
        "FractalAnalyzer",
        "SymmetryAnalyzer",
        "NaturalnessAnalyzer",
        "DepthAnalyzer",
        "CognitiveStateAnalyzer",
    ]

    analyzer_classes = []
    for name in analyzer_class_names:
        cls = getattr(science_pipeline, name, None)
        if cls is None:
            summary["warnings"].append(f"Analyzer class {name} missing from pipeline module.")
            continue
        analyzer_classes.append(cls)

    for cls in analyzer_classes:
        try:
            inst = cls()
        except Exception as exc:  # pragma: no cover - defensive
            summary.setdefault("analyzer_errors", []).append(
                {"analyzer": cls.__name__, "error": repr(exc)}
            )
            continue
        tier = getattr(inst, "tier", "unknown")
        requires = list(getattr(inst, "requires", []))
        provides = list(getattr(inst, "provides", []))
        entry = {
            "name": getattr(inst, "name", cls.__name__),
            "tier": tier,
            "requires": requires,
            "provides": provides,
        }
        summary["analyzers_by_tier"].setdefault(tier, []).append(entry)

    return summary
----- CONTENT END -----
----- FILE PATH: backend/api/v1_discovery.py
----- CONTENT START -----
"""
Explorer / Discovery API (v1).

This module is the canonical adaptor between the Explorer UI and the
database + training export service. It exposes three main endpoints:

- POST /v1/explorer/search
- POST /v1/explorer/export
- GET  /v1/explorer/attributes

All endpoints are RBAC-protected for taggers (and above).
"""

from __future__ import annotations

from typing import List

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from backend.database.session import get_db
from backend.services.auth import require_tagger
from backend.services.training_export import TrainingExporter, TrainingExample
from backend.schemas.discovery import SearchQuery, ImageSearchResult, ExportRequest, AttributeRead
from backend.models.attribute import Attribute

router = APIRouter(prefix="/v1/explorer", tags=["explorer"])


@router.post("/search", response_model=List[ImageSearchResult])
def search_images(
    payload: SearchQuery,
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[ImageSearchResult]:
    """Search for images matching the query.

    This minimal implementation supports:
      - free-text search over image name / description (if available), or
      - returning a simple paginated list when no filter is provided.

    The exact ranking can be improved later; for now we favour determinism
    and well-formed responses over sophistication.
    """
    # Lazy import to avoid circular dependencies
    from backend.models.assets import Image  # type: ignore

    q = db.query(Image)

    if getattr(payload, "text", None):
        text = f"%{payload.text}%"
        # Try to filter by name / description if those fields exist.
        # We guard each attribute access with hasattr to avoid hard failures
        # across slightly different schemas.
        name_col = getattr(Image, "name", None)
        desc_col = getattr(Image, "description", None)
        if name_col is not None and desc_col is not None:
            q = q.filter((name_col.ilike(text)) | (desc_col.ilike(text)))
        elif name_col is not None:
            q = q.filter(name_col.ilike(text))

    limit = max(1, min(getattr(payload, "limit", 50), 200))
    offset = max(0, getattr(payload, "offset", 0))
    q = q.order_by(getattr(Image, "id")).offset(offset).limit(limit)

    images = q.all()
    results: List[ImageSearchResult] = []

    base_thumb_url = "/static/thumbnails"

    for img in images:
        image_id = getattr(img, "id", None)
        if image_id is None:
            continue
        thumb_name = getattr(img, "thumbnail_path", None) or f"image_{image_id}.jpg"
        thumb_url = f"{base_thumb_url}/{thumb_name}"

        # For now, we do not join attributes here; Explorer primarily needs
        # thumbnails and IDs to drive downstream detail views and exports.
        res = ImageSearchResult(image_id=image_id, thumbnail_url=thumb_url, attributes={})
        results.append(res)

    return results


@router.post("/export", response_model=List[TrainingExample])
def export_training_data(
    payload: ExportRequest,
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[TrainingExample]:
    """Export training examples for the given image IDs.

    This is a thin wrapper around the TrainingExporter service, returning
    a list of TrainingExample objects suitable for downstream model training.
    """
    if not payload.image_ids:
        return []

    exporter = TrainingExporter(db=db)
    try:
        examples = exporter.export_for_images(payload.image_ids)
    except Exception as exc:  # pragma: no cover - defensive logging
        raise HTTPException(status_code=500, detail=f"training export failed: {exc}") from exc

    return examples


@router.get("/attributes", response_model=List[AttributeRead])
def list_attributes(
    db: Session = Depends(get_db),
    user=Depends(require_tagger),
) -> List[AttributeRead]:
    """Return the attribute registry for Explorer filters.

    We expose all Attribute rows, mapping them into AttributeRead records.
    """
    attrs = db.query(Attribute).order_by(Attribute.key).all()
    results: List[AttributeRead] = []
    for attr in attrs:
        key = getattr(attr, "key", None)
        label = getattr(attr, "label", None) or key
        group = getattr(attr, "group", "default")
        if key is None:
            continue
        results.append(AttributeRead(key=key, label=label, group=group, description=getattr(attr, 'description', None) or getattr(attr, 'notes', None)))
    return results----- CONTENT END -----
----- FILE PATH: backend/api/v1_features.py
----- CONTENT START -----
"""
Feature Navigator API (v1).

Read-only API for browsing the CNfA feature/attribute ontology.
"""

from __future__ import annotations

from typing import List, Optional

from fastapi import APIRouter, Depends

from backend.services.auth import require_admin_or_supervisor, require_tagger
from backend.science.features_registry import FeatureDefinition, list_features, get_feature

router = APIRouter(prefix="/v1/features", tags=["features"])


# Pydantic response model is re-declared to avoid coupling to dataclasses.
from pydantic import BaseModel


class FeatureRead(BaseModel):
    key: str
    category: str
    tier: str
    label: str
    status: str
    type: str
    group: Optional[str] = None
    description: Optional[str] = None
    cfa_relevance: Optional[str] = None
    source: Optional[str] = None
    scale: Optional[dict] = None
    methods: Optional[list] = None


@router.get("/", response_model=List[FeatureRead])
def list_all_features(
    tier: Optional[str] = None,
    category: Optional[str] = None,
    status: Optional[str] = "active",
    user=Depends(require_tagger),
) -> List[FeatureRead]:
    """Return the feature registry for Feature Navigator.

    This is intentionally read-only for now. Admin mutations are handled
    via a separate workflow.
    """
    feats = list_features(tier=tier, category=category, status=status)
    return [
        FeatureRead(
            key=f.key,
            category=f.category,
            tier=f.tier,
            label=f.label,
            status=f.status,
            type=f.type,
            group=f.group,
            description=f.description,
            cfa_relevance=f.cfa_relevance,
            source=f.source,
            scale=f.scale,
            methods=f.methods,
        )
        for f in feats
    ]


@router.get("/{key}", response_model=FeatureRead)
def get_feature_detail(
    key: str,
    user=Depends(require_tagger),
) -> FeatureRead:
    feat = get_feature(key)
    if feat is None:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="Feature not found")
    return FeatureRead(
        key=feat.key,
        category=feat.category,
        tier=feat.tier,
        label=feat.label,
        status=feat.status,
        type=feat.type,
        group=feat.group,
        description=feat.description,
        cfa_relevance=feat.cfa_relevance,
        source=feat.source,
        scale=feat.scale,
        methods=feat.methods,
    )
----- CONTENT END -----
----- FILE PATH: backend/api/v1_supervision.py
----- CONTENT START -----
from __future__ import annotations

from datetime import datetime, timedelta
from typing import List, Dict, Optional

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy import func
from sqlalchemy.orm import Session

from backend.database.core import get_db
from backend.services.auth import require_admin
from backend.models.users import User
from backend.models.annotation import Validation
from backend.models.assets import Image
from backend.schemas.supervision import TaggerPerformance, IRRStat, ValidationDetail
from backend.services.storage import to_static_path
from backend.science.index_catalog import get_index_metadata
from backend.api import v1_bn_export


router = APIRouter(prefix="/v1/monitor", tags=["Supervisor Dashboard"])

def _build_restorativeness_heuristic_node(features: List[Dict[str, object]]):
    """Build a simple restorativeness heuristic node for Tag Inspector.

    This is an early, explicitly *heuristic* rule family (H1) that tries to
    combine a few CNfA-aligned visual features into a coarse 3-level judgment
    about how *restorative* an interior might feel.

    Currently we combine:

    - cnfa.biophilic.natural_material_ratio      (more natural material -> more restorative)
    - cnfa.fluency.visual_entropy_spatial        (mid-level visual entropy preferred)
    - cnfa.fluency.clutter_density_count         (less clutter -> more restorative)
    - cnfa.fluency.processing_load_proxy         (lower processing load -> more restorative)

    The computation is intentionally simple and transparent; it should evolve
    as we gather human ratings and fit real models.

    Returns:
        (node, tag) where:
            - node is a BN-like dict for the Tag Inspector `bn.nodes` array
            - tag is a tag dict for the Tag Inspector `tags` array
        Either may be None if we do not have enough evidence.
    """
    # Map feature key -> numeric value for quick lookup.
    feature_values: Dict[str, Optional[float]] = {}
    for row in features:
        key = row.get("key")
        value = row.get("value")
        if not isinstance(key, str):
            continue
        if value is None:
            feature_values[key] = None
            continue
        try:
            feature_values[key] = float(value)
        except (TypeError, ValueError):
            feature_values[key] = None

    nat = feature_values.get("cnfa.biophilic.natural_material_ratio")
    entropy = feature_values.get("cnfa.fluency.visual_entropy_spatial")
    clutter = feature_values.get("cnfa.fluency.clutter_density_count")
    load = feature_values.get("cnfa.fluency.processing_load_proxy")

    evidence = {
        "cnfa.biophilic.natural_material_ratio": nat,
        "cnfa.fluency.visual_entropy_spatial": entropy,
        "cnfa.fluency.clutter_density_count": clutter,
        "cnfa.fluency.processing_load_proxy": load,
    }

    available = {k: v for k, v in evidence.items() if v is not None}
    if len(available) < 2:
        # Not enough data to make even a coarse call.
        return None, None

    def _clamp01(x: float) -> float:
        if x < 0.0:
            return 0.0
        if x > 1.0:
            return 1.0
        return x

    # Weights are heuristic and explicitly marked as such.
    weights = {}
    if nat is not None:
        weights["nat"] = 0.4
    if entropy is not None:
        weights["entropy"] = 0.3
    if clutter is not None:
        weights["clutter"] = 0.15
    if load is not None:
        weights["load"] = 0.15

    total_w = sum(weights.values())
    if total_w <= 0.0:
        return None, None

    # Natural materials: higher is better.
    nat_term = _clamp01(nat) if nat is not None else 0.5

    # Entropy: we tentatively prefer mid-level values.
    if entropy is not None:
        e = _clamp01(entropy)
        entropy_term = max(0.0, 1.0 - 2.0 * abs(e - 0.5))  # 1 at 0.5, 0 at 0 or 1
    else:
        entropy_term = 0.5

    # Clutter / processing load: lower is better.
    clutter_term = 1.0 - _clamp01(clutter) if clutter is not None else 0.5
    load_term = 1.0 - _clamp01(load) if load is not None else 0.5

    num = 0.0
    num += weights.get("nat", 0.0) * nat_term
    num += weights.get("entropy", 0.0) * entropy_term
    num += weights.get("clutter", 0.0) * clutter_term
    num += weights.get("load", 0.0) * load_term

    rest_score = num / total_w

    # Map score into a coarse 3-bin label.
    if rest_score < 0.33:
        bin_label = "low"
    elif rest_score < 0.66:
        bin_label = "mid"
    else:
        bin_label = "high"

    # Build a short explanation string.
    parts = []
    if nat is not None:
        parts.append(f"natural_material_ratio={nat:.2f}")
    if entropy is not None:
        parts.append(f"visual_entropy_spatial={entropy:.2f}")
    if clutter is not None:
        parts.append(f"clutter_density_count={clutter:.2f}")
    if load is not None:
        parts.append(f"processing_load_proxy={load:.2f}")
    evidence_str = ", ".join(parts) if parts else "no CNfA fluency features available"

    notes = (
        "H1 restorativeness heuristic combining biophilic material ratio and CNfA fluency features. "
        "Higher natural material ratio, mid-level visual entropy, and lower clutter/processing load "
        "push the score toward 'high'. This rule is uncalibrated and should be refined with human "
        "ratings and cultural baselines. Evidence: " + evidence_str
    )

    node_name = "affect.restorative_h1"
    label = "Restorativeness (H1 heuristic)"

    posterior = {bin_label: 1.0} if bin_label else None

    node = {
        "name": node_name,
        "label": label,
        "posterior": posterior,
        "prior": None,
        "notes": notes,
    }

    tag = {
        "key": node_name,
        "label": label,
        "description": notes,
        "value": bin_label,
        "raw_value": rest_score,
        "bin": bin_label,
        "status": "derived",
        "bn_node": node_name,
    }

    return node, tag


@router.get("/velocity", response_model=List[TaggerPerformance])
def get_velocity(
    window_hours: int = 24,
    db: Session = Depends(get_db),
    _: User = Depends(require_admin),
) -> List[TaggerPerformance]:
    """Aggregate tagger velocity over a recent time window.

    The default window is the last 24 hours. The query aggregates:

      * number of distinct images validated per tagger
      * average dwell time (duration_ms) for those validations

    This endpoint drives the team velocity table in the Supervisor GUI.
    """
    if window_hours <= 0:
        window_hours = 24

    cutoff = datetime.utcnow() - timedelta(hours=window_hours)

    rows = (
        db.query(
            Validation.user_id.label("user_id"),
            func.coalesce(User.username, "unknown").label("username"),
            func.count(func.distinct(Validation.image_id)).label("images_validated"),
            func.coalesce(func.avg(Validation.duration_ms), 0).label("avg_duration_ms"),
            func.count(Validation.id).label("validations_count"),
        )
        .outerjoin(User, User.id == Validation.user_id)
        .filter(Validation.created_at >= cutoff)
        .group_by(Validation.user_id, User.username)
        .all()
    )

    results: List[TaggerPerformance] = []
    for row in rows:
        avg_duration_ms = int(row.avg_duration_ms or 0)
        status = "active" if row.validations_count > 0 else "inactive"
        results.append(
            TaggerPerformance(
                user_id=row.user_id or 0,
                username=row.username or "unknown",
                images_validated=row.images_validated,
                avg_duration_ms=avg_duration_ms,
                status=status,
            )
        )

    return results


@router.get("/irr", response_model=List[IRRStat])
def get_irr(
    window_hours: int = 72,
    db: Session = Depends(get_db),
    _: User = Depends(require_admin),
) -> List[IRRStat]:
    """Compute a simple IRR metric from overlapping validations.

    For each (image_id, attribute_key) pair with 2+ validations in the
    given time window, we compute:

      * pairwise agreement ratio between raters (exact value match)
      * conflict_count = number of disagreeing pairs

    This is intentionally conservative and easy to interpret, rather than
    a full Fleiss' Œ∫ implementation.
    """
    if window_hours <= 0:
        window_hours = 72

    cutoff = datetime.utcnow() - timedelta(hours=window_hours)

    rows = (
        db.query(
            Validation.image_id,
            Validation.attribute_key,
            Validation.value,
            Validation.user_id,
            func.coalesce(User.username, "unknown").label("username"),
            Image.filename,
        )
        .join(Image, Image.id == Validation.image_id)
        .outerjoin(User, User.id == Validation.user_id)
        .filter(Validation.created_at >= cutoff)
        .all()
    )

    from collections import defaultdict

    grouped = defaultdict(list)
    for row in rows:
        key = (row.image_id, row.filename, row.attribute_key)
        grouped[key].append((row.value, row.username))

    results: List[IRRStat] = []
    for (image_id, filename, attribute_key), values in grouped.items():
        n = len(values)
        if n < 2:
            continue

        total_pairs = 0
        agree_pairs = 0
        for i in range(n):
            vi, _ui = values[i]
            for j in range(i + 1, n):
                vj, _uj = values[j]
                total_pairs += 1
                if vi == vj:
                    agree_pairs += 1

        if total_pairs == 0:
            continue

        agreement_score = agree_pairs / total_pairs
        conflict_count = total_pairs - agree_pairs
        raters = sorted({u for _v, u in values})

        results.append(
            IRRStat(
                image_id=image_id,
                filename=filename,
                attribute_key=attribute_key,
                agreement_score=agreement_score,
                conflict_count=conflict_count,
                raters=raters,
            )
        )

    return results


@router.get("/image/{image_id}/validations", response_model=List[ValidationDetail])
def get_image_validations(
    image_id: int,
    db: Session = Depends(get_db),
    _: User = Depends(require_admin),
) -> List[ValidationDetail]:
    """Return per-user validations for a specific image.

    This powers the Tag Inspector drawer in the Supervisor dashboard.
    """
    rows = (
        db.query(
            Validation.id,
            Validation.user_id,
            func.coalesce(User.username, "unknown").label("username"),
            Validation.image_id,
            Validation.attribute_key,
            Validation.value,
            Validation.duration_ms,
            Validation.created_at,
        )
        .outerjoin(User, User.id == Validation.user_id)
        .filter(Validation.image_id == image_id)
        .order_by(Validation.created_at.asc())
        .all()
    )

    return [
        ValidationDetail(
            id=row.id,
            user_id=row.user_id,
            username=row.username,
            image_id=row.image_id,
            attribute_key=row.attribute_key,
            value=row.value,
            duration_ms=row.duration_ms,
            created_at=row.created_at,
        )
        for row in rows
    ]


@router.get("/image/{image_id}/inspector")
def get_image_inspector(
    image_id: int,
    db: Session = Depends(get_db),
    _: User = Depends(require_admin),
) -> Dict[str, object]:
    """Aggregate science pipeline outputs, BN indices, and validations for Tag Inspector.

    This endpoint is intentionally read-only and conservative. It surfaces:

    - basic image metadata and a /static URL,
    - raw science_pipeline_* Validation rows as numeric features,
    - composite indices + bins from the BN export helpers,
    - a simple BN-like node summary for each index,
    - per-user validations for the image (for convenience).
    """

    image = db.query(Image).get(image_id)
    if image is None:
        raise HTTPException(status_code=404, detail="Image not found")

    # Construct a stable /static URL for the underlying image asset.
    storage_path = getattr(image, "storage_path", None) or getattr(image, "filename", "")
    image_url: Optional[str] = None
    if storage_path:
        rel = to_static_path(storage_path)
        image_url = f"/static/{rel}" if rel else None

    # Raw science pipeline attributes (continuous features).
    sci_rows = (
        db.query(Validation)
        .filter(
            Validation.image_id == image_id,
            Validation.source.like("science_pipeline%"),
        )
        .order_by(Validation.attribute_key)
        .all()
    )

    features = [
        {
            "key": row.attribute_key,
            "value": float(row.value) if row.value is not None else None,
            "source": row.source,
        }
        for row in sci_rows
    ]

    # Composite indices and bins reused from the BN export helpers.
    index_meta = get_index_metadata()
    index_keys = list(index_meta.keys())

    indices = v1_bn_export._collect_indices_for_image(db, image_id, index_keys)
    bins = v1_bn_export._collect_bins_for_image(db, image_id)
    irr = v1_bn_export._compute_irr_for_image(db, image_id)

    tags = []
    nodes = []
    for key in index_keys:
        meta = index_meta.get(key, {})
        value = indices.get(key)
        bin_label = bins.get(key)

        if value is None and bin_label is None:
            continue

        tags.append(
            {
                "key": key,
                "label": meta.get("label", key),
                "description": meta.get("description"),
                "value": bin_label if bin_label is not None else value,
                "raw_value": value,
                "bin": bin_label,
                "status": "machine",
                "bn_node": key,
            }
        )

        posterior = {}
        if bin_label:
            posterior[bin_label] = 1.0

        nodes.append(
            {
                "name": key,
                "label": meta.get("label", key),
                "posterior": posterior or None,
                "prior": None,
                "notes": meta.get("description"),
            }
        )

    # H1: add a derived restorativeness heuristic node based on CNfA fluency + biophilia features.
    rest_node, rest_tag = _build_restorativeness_heuristic_node(features)
    if rest_node is not None:
        nodes.append(rest_node)
    if rest_tag is not None:
        tags.append(rest_tag)

    # Also surface per-user validations as part of the inspector payload.
    validations_rows = (
        db.query(
            Validation.id,
            Validation.user_id,
            User.username,
            Validation.image_id,
            Validation.attribute_key,
            Validation.value,
            Validation.duration_ms,
            Validation.created_at,
        )
        .join(User, Validation.user_id == User.id, isouter=True)
        .filter(Validation.image_id == image_id)
        .order_by(Validation.created_at.desc())
        .all()
    )

    validations = [
        {
            "id": row.id,
            "user_id": row.user_id,
            "username": row.username,
            "image_id": row.image_id,
            "attribute_key": row.attribute_key,
            "value": row.value,
            "duration_ms": row.duration_ms,
            "created_at": row.created_at.isoformat() if row.created_at else None,
        }
        for row in validations_rows
    ]

    return {
        "image": {
            "image_id": image.id,
            "filename": image.filename,
            "url": image_url,
        },
        "pipeline": {
            # We do not yet track per-analyzer run state per image; this will evolve.
            "overall_status": "unknown",
            "analyzers_run": [],
        },
        "features": features,
        "tags": tags,
        "bn": {
            "nodes": nodes,
            "irr": irr,
        },
        "validations": validations,
    }----- CONTENT END -----
----- FILE PATH: backend/api/v1_vlm_health.py
----- CONTENT START -----
"""VLM health and Turing-test API endpoints.

This router exposes a minimal read-only view over the VLM health reports
produced by the CLI/SOP in docs/ops/VLM_Health_SOP.md.

It allows the Admin frontend to:
- List available health runs under reports/vlm_health/
- Download the variance-audit CSV for a run
- Download the Turing summary text file for a run
"""

from __future__ import annotations

import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel

from backend.services.auth import require_admin

router = APIRouter(prefix="/v1/vlm-health", tags=["vlm-health"])

# Root directory where CLI scripts write health reports.
# Layout:
#   reports/vlm_health/
#       RUN_ID/
#           raw/
#           derived/
#           log.md
VLM_HEALTH_ROOT = Path(os.getenv("VLM_HEALTH_ROOT", "reports/vlm_health")).resolve()


class VLMHealthRun(BaseModel):
    """Summary metadata for a single VLM health run."""

    run_id: str
    created_at: Optional[datetime]
    has_variance_audit: bool
    has_turing_summary: bool


def _get_runs_root() -> Path:
    return VLM_HEALTH_ROOT


def _safe_file(run_id: str, relative: str) -> Path:
    """Resolve a file inside a run folder and guard against traversal."""
    root = _get_runs_root()
    run_dir = (root / run_id).resolve()
    root_resolved = root.resolve()

    if not str(run_dir).startswith(str(root_resolved)):
        raise HTTPException(status_code=400, detail="Invalid run identifier")

    target = (run_dir / relative).resolve()
    if not str(target).startswith(str(run_dir)):
        raise HTTPException(status_code=400, detail="Invalid file path")

    if not target.exists() or not target.is_file():
        raise HTTPException(status_code=404, detail="File not found")

    return target


@router.get("/runs", response_model=List[VLMHealthRun])
def list_runs(user = Depends(require_admin)) -> List[VLMHealthRun]:
    """List all known VLM health runs, newest first.

    This inspects the reports/vlm_health directory and surfaces a small
    metadata record per RUN_ID. It does not read any of the large CSV
    contents; the Admin UI can fetch those lazily on demand.
    """
    root = _get_runs_root()
    if not root.exists() or not root.is_dir():
        return []

    items: List[VLMHealthRun] = []

    for entry in root.iterdir():
        if not entry.is_dir():
            continue

        run_id = entry.name
        derived_dir = entry / "derived"
        variance_path = derived_dir / "vlm_variance_audit.csv"
        summary_path = derived_dir / "vlm_turing_summary.txt"

        try:
            stat = entry.stat()
            created_at: Optional[datetime] = datetime.fromtimestamp(stat.st_mtime)
        except Exception:
            created_at = None

        items.append(
            VLMHealthRun(
                run_id=run_id,
                created_at=created_at,
                has_variance_audit=variance_path.exists(),
                has_turing_summary=summary_path.exists(),
            )
        )

    # Sort newest first by created_at (fallback to name)
    items.sort(
        key=lambda r: (
            r.created_at or datetime.min,
            r.run_id,
        ),
        reverse=True,
    )
    return items


@router.get("/runs/{run_id}/variance-audit")
def download_variance_audit(run_id: str, user = Depends(require_admin)):
    """Download the variance audit CSV for a given run."""
    path = _safe_file(run_id, "derived/vlm_variance_audit.csv")
    filename = f"{run_id}_vlm_variance_audit.csv"
    return FileResponse(
        path,
        media_type="text/csv",
        filename=filename,
    )


@router.get("/runs/{run_id}/turing-summary")
def download_turing_summary(run_id: str, user = Depends(require_admin)):
    """Download the Turing-test summary text file for a given run."""
    path = _safe_file(run_id, "derived/vlm_turing_summary.txt")
    filename = f"{run_id}_vlm_turing_summary.txt"
    return FileResponse(
        path,
        media_type="text/plain",
        filename=filename,
    )
----- CONTENT END -----
----- FILE PATH: backend/data/canonical_tree_v1.json
----- CONTENT START -----
{
  "I. Spatial Configuration": [
    "spatial.prospect_depth",
    "spatial.refuge_availability",
    "spatial.edge_definition",
    "spatial.ceiling_height_variation",
    "spatial.visual_escape_routes"
  ],
  "II. Fractals & Pattern": [
    "fractals.dimension_range",
    "pattern.self_similarity_index",
    "rhythm.repetition_regularity",
    "pattern.hierarchical_nesting"
  ],
  "III. Biophilia": [
    "biophilia.natural_material_diversity",
    "biophilia.biomimetic_forms",
    "plants.biomass_estimate",
    "plants.foliage_density_layers"
  ],
  "IV. Color & Light": [
    "color.palette_diversity",
    "color.harmony_measure",
    "lighting.shadow_softness",
    "lighting.glare_probability"
  ],
  "V. Material & Texture": [
    "materials.tactile_diversity",
    "materials.softness_presence",
    "materials.natural_weathering"
  ],
  "VI. Social Affordances": [
    "social.seating_arrangements",
    "social.interpersonal_distance_support",
    "density.people_capacity_ratio"
  ],
  "VII. Temporal & Dynamic": [
    "temporal.wear_evidence",
    "dynamics.movement_elements",
    "dynamics.discovery_potential"
  ],
  "VIII. Cognitive Load": [
    "navigation.visual_access_extent",
    "navigation.landmark_distinctiveness",
    "cognitive.visual_information_density",
    "cognitive.novelty_vs_familiarity"
  ],
  "IX. Restorative Qualities": [
    "restoration.fascination_elements",
    "restoration.being_away_quality",
    "stress.threatening_elements"
  ],
  "X. Cultural & Semantic": [
    "archetype.cave_like_quality",
    "archetype.cathedral_quality",
    "semantics.function_clarity",
    "semantics.status_markers"
  ],
  "XI. Control & Agency": [
    "control.adjustable_element_density",
    "control.modification_affordances",
    "control.microclimate_zones",
    "agency.responsive_feedback",
    "agency.furniture_mobility_afforded",
    "agency.light_zoning_control"
  ],
  "XII. Info Gradient & Surprise": [
    "information.conditional_entropy",
    "surprise.expectation_violation",
    "mystery.progressive_disclosure"
  ],
  "XIII. Topology & Network": [
    "topology.integration_value",
    "topology.choice_coefficient",
    "network.visual_graph_connectivity"
  ],
  "XIV. Proportion & Scale": [
    "scale.body_part_resonance",
    "proportion.golden_ratio_proximity",
    "proportion.harmonic_intervals",
    "scale.magnitude_range"
  ],
  "XV. Affordances & Activity": [
    "activity.primary_script_clarity",
    "activity.functional_density",
    "activity.object_constellation_completeness",
    "activity.transition_impedance",
    "motor.postural_variety",
    "motor.body_clearance_min",
    "affordance.graspability_index",
    "affordance.view_adjustment_potential"
  ],
  "XVI. Privacy & Territory": [
    "privacy.visual_exposure_to_entry",
    "privacy.gaze_vulnerability_outward",
    "territory.personalization_density",
    "territory.boundary_softness",
    "territory.primary_vantage_point"
  ],
  "XVII. Anomalies & Illusions": [
    "schema.functional_mismatch",
    "schema.material_contradiction",
    "material.mimicry_fidelity",
    "surface.trompe_l_oeil_presence",
    "structure.physics_defiance_proxy",
    "scale.gulliver_effect"
  ]
}----- CONTENT END -----
----- FILE PATH: backend/data/goldilocks_attributes.csv
----- CONTENT START -----
Feature_Name,Human_Readable_Label,Feature_Category,Low_End_State (Boring),High_End_State (Chaotic),Optimal_State (Just Right),Computational_Method
cnfa.fluency.processing_load_proxy,Visual Information Density,Cognitive Load,"Sterile, under-stimulating","Chaotic, overwhelming, high-load","Engaging, rich, ordered complexity","JPEG file size / total_pixels"
cnfa.fluency.clutter_density_count,Clutter Density (Objects),Cognitive Load,"Impersonal, unlived-in","Stressful, overwhelming","Personalized, cozy, organized density","count(objects)/area(room_floor)"
cnfa.fluency.symmetry_score,Symmetry,Fluency,"Chaotic, Unbalanced","Static, Rigid","Balanced Asymmetry","SSIM(image, flipped)"
cnfa.spatial.prospect,Prospect,Spatial,"Confined, Claustrophobic","Exposed, Vulnerable","Sheltered View","Isovist Raycast > 5m"----- CONTENT END -----
----- FILE PATH: backend/database/__init__.py
----- CONTENT START -----
# backend/database package----- CONTENT END -----
----- FILE PATH: backend/database/core.py
----- CONTENT START -----
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, DeclarativeBase
from typing import Generator
import os

# Configuration
# Defaulting to localhost postgres. In Docker, this will be overridden by ENV vars.
# NOTE: v3 Architecture explicitly drops support for SQLite to prevent locking in multi-user mode.
DATABASE_URL = os.getenv(
    "DATABASE_URL", 
    "postgresql://user:password@localhost:5432/image_tagger_v3"
)

# Engine Setup
# pool_pre_ping=True handles DB connection drops gracefully
engine = create_engine(DATABASE_URL, pool_pre_ping=True)

# Session Factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

class Base(DeclarativeBase):
    """
    Unified SQLAlchemy Base for v3 Enterprise Refactor.
    All models must inherit from this class.
    """
    pass

def get_db() -> Generator:
    """
    FastAPI Dependency for database sessions.
    Ensures connections are closed after request lifecycle.
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()----- CONTENT END -----
----- FILE PATH: backend/database/mixins.py
----- CONTENT START -----
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy import DateTime, func
from datetime import datetime

class TimestampMixin:
    """
    Standard Audit Mixin.
    Ensures every table in the system tracks creation and modification times
    crucial for the 'Supervisor Dashboard' velocity tracking.
    """
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(), 
        nullable=False
    )
    
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        onupdate=func.now(), 
        nullable=True
    )----- CONTENT END -----
----- FILE PATH: backend/models/__init__.py
----- CONTENT START -----
# backend/models/__init__.py
# This file aggregates all models so Alembic can discover them for migrations.

from backend.database.core import Base
from backend.models.users import User
from backend.models.config import ToolConfig
from backend.models.assets import Image, Region
from backend.models.usage import ToolUsage
from backend.models.annotation import Validation

__all__ = ["Base", "User", "ToolConfig", "Image", "Region", "Validation"]
from backend.models.attribute import Attribute

__all__ = [
    "Base",
    "User",
    "ToolConfig",
    "Image",
    "Region",
    "Validation",
    "Attribute",
    "ToolUsage",
]

from backend.models.jobs import UploadJob, UploadJobItem

__all__ = list(dict.fromkeys(__all__ + ['UploadJob', 'UploadJobItem']))
----- CONTENT END -----
----- FILE PATH: backend/models/annotation.py
----- CONTENT START -----
from __future__ import annotations

from sqlalchemy import String, Float, ForeignKey, Integer
from sqlalchemy.orm import Mapped, mapped_column, relationship

from backend.database.core import Base
from backend.database.mixins import TimestampMixin


class Validation(Base, TimestampMixin):
    """Human or pipeline validation record.

    This table is the central audit trail for:
      * Tagger Workbench decisions (human HITL).
      * Science pipeline auto-attributes (source = "science_pipeline_v3.3").
      * Supervisor Dashboard velocity and IRR calculations.

    Design notes
    ------------
    - ``user_id`` is nullable so that automated science jobs can emit
      validations without a concrete user.
    - ``attribute_key`` is enforced at the DB level to reference the
      canonical Attribute registry (``attributes.key``).
    - ``duration_ms`` provides the raw material for velocity / quality
      checks (e.g., detecting "spam clicking").
    """

    __tablename__ = "validations"

    id: Mapped[int] = mapped_column(primary_key=True)
    user_id: Mapped[int | None] = mapped_column(
        ForeignKey("users.id"),
        nullable=True,
        index=True,
    )
    image_id: Mapped[int] = mapped_column(
        ForeignKey("images.id"),
        index=True,
    )

    # The attribute being validated (e.g., "spatial.prospect").
    # In v3.4.63+ this is enforced at the DB level via a foreign key to
    # the canonical Attribute registry.
    attribute_key: Mapped[str] = mapped_column(
        String,
        ForeignKey("attributes.key"),
        index=True,
    )

    # The value assigned (0.0 - 1.0 for continuous, or categorical encoded
    # as a float for now).
    value: Mapped[float] = mapped_column(Float)

    # Optional: link to a specific region if this is a local attribute.
    region_id: Mapped[int | None] = mapped_column(Integer, nullable=True)

    # Origin of this validation: "manual", "science_pipeline_v3.3", etc.
    source: Mapped[str] = mapped_column(String, default="manual")

    # Velocity tracking: how long did the user look before clicking?
    # Critical for detecting "spam clicking" by tired taggers.
    duration_ms: Mapped[int] = mapped_column(Integer, default=0)

    # Relationships
    user = relationship("User", back_populates="validations")
    image = relationship("Image", back_populates="validations")
    attribute = relationship("Attribute", back_populates="validations")
----- CONTENT END -----
----- FILE PATH: backend/models/assets.py
----- CONTENT START -----
from sqlalchemy import String, Integer, ForeignKey, JSON
from sqlalchemy.orm import Mapped, mapped_column, relationship
from typing import List
from backend.database.core import Base
from backend.database.mixins import TimestampMixin

class Image(Base, TimestampMixin):
    """
    Represents the raw asset being analyzed.
    """
    __tablename__ = "images"

    id: Mapped[int] = mapped_column(primary_key=True)
    filename: Mapped[str] = mapped_column(String)
    
    # Path on disk or S3 key. 
    # v3 requirement: Do not store binary data in DB.
    storage_path: Mapped[str] = mapped_column(String)
    
    # Metadata for search (width, height, format, exif data)
    # This powers the 'Research Explorer' filters
    meta_data: Mapped[dict] = mapped_column(JSON, default={})
    
    # Tracking batch imports
    upload_batch_id: Mapped[str] = mapped_column(String, nullable=True, index=True)
    
    regions = relationship("Region", back_populates="image", cascade="all, delete-orphan")
    validations = relationship("Validation", back_populates="image")

class Region(Base, TimestampMixin):
    """
    A specific area of interest within an image (bounding box or polygon).
    Generated by SAM (Auto) or drawn by Tagger (Manual).
    """
    __tablename__ = "regions"

    id: Mapped[int] = mapped_column(primary_key=True)
    image_id: Mapped[int] = mapped_column(ForeignKey("images.id"))
    
    # Geometry: The polygon or bounding box coordinates
    # Stored as JSON to allow flexible shapes (Box vs Polygon)
    geometry: Mapped[dict] = mapped_column(JSON) 
    
    # AI Pre-label (e.g., from VLM) - The "Suggestion"
    auto_label: Mapped[str] = mapped_column(String, nullable=True)
    auto_confidence: Mapped[float] = mapped_column(Float, nullable=True)
    
    # Final Human Label - The "Ground Truth"
    manual_label: Mapped[str] = mapped_column(String, nullable=True)
    
    image = relationship("Image", back_populates="regions")----- CONTENT END -----
----- FILE PATH: backend/models/attribute.py
----- CONTENT START -----
from sqlalchemy import String, Boolean, Text
from sqlalchemy.orm import Mapped, mapped_column, relationship
from typing import Optional

from backend.database.core import Base


class Attribute(Base):
    """
    Attribute taxonomy entry.

    Ported from v2.6.3 contracts/attributes.yml into a proper SQLAlchemy model
    so downstream tools (Explorer, Tag Inspector, science pipeline) can reason
    over a shared attribute registry.
    """

    __tablename__ = "attributes"

    id: Mapped[int] = mapped_column(primary_key=True, index=True)
    key: Mapped[str] = mapped_column(String(255), unique=True, index=True)
    name: Mapped[str] = mapped_column(String(255))
    category: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    level: Mapped[Optional[str]] = mapped_column(String(64), nullable=True)
    range: Mapped[Optional[str]] = mapped_column(String(64), nullable=True)
    sources: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    notes: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    is_active: Mapped[bool] = mapped_column(Boolean, default=True)
    source_version: Mapped[Optional[str]] = mapped_column(String(64), nullable=True)

    # Relationships
    validations = relationship("Validation", back_populates="attribute")
----- CONTENT END -----
----- FILE PATH: backend/models/config.py
----- CONTENT START -----
from sqlalchemy import String, Float, Boolean, JSON
from sqlalchemy.orm import Mapped, mapped_column
from backend.database.core import Base
from backend.database.mixins import TimestampMixin

class ToolConfig(Base, TimestampMixin):
    """
    System Configuration Table.
    Controls which AI models are active and tracks their pricing.
    This is the source of truth for the 'Cost & Governance Cockpit'.
    """
    __tablename__ = "tool_configs"

    id: Mapped[int] = mapped_column(primary_key=True)
    
    # e.g., "gpt-4v-preview", "sam-vit-h"
    name: Mapped[str] = mapped_column(String, unique=True, index=True)
    
    # e.g., "openai", "anthropic", "local"
    provider: Mapped[str] = mapped_column(String)
    
    # Cost Control
    # Used by CostEstimator service to calculate pre-run budget checks
    cost_per_1k_tokens: Mapped[float] = mapped_column(Float, default=0.0)
    cost_per_image: Mapped[float] = mapped_column(Float, default=0.0)
    
    # The "Kill Switch" - if False, the ModelLoader service will refuse to instantiate
    is_enabled: Mapped[bool] = mapped_column(Boolean, default=True)
    
    # Technical config (max tokens, temperature, local file paths)
    settings: Mapped[dict] = mapped_column(JSON, default={})----- CONTENT END -----
----- FILE PATH: backend/models/jobs.py
----- CONTENT START -----
from __future__ import annotations

from typing import List, Optional

from sqlalchemy import String, Integer, ForeignKey, Text
from sqlalchemy.orm import Mapped, mapped_column, relationship

from backend.database.core import Base
from backend.database.mixins import TimestampMixin


class UploadJob(Base, TimestampMixin):
    """Represents a batch upload + science run.

    This is intentionally lightweight: it tracks only aggregate progress and
    a short error summary. Per-image details live in UploadJobItem.
    """

    __tablename__ = "upload_jobs"

    id: Mapped[int] = mapped_column(primary_key=True)

    # Optional link back to the admin user who initiated the job.
    created_by_id: Mapped[Optional[int]] = mapped_column(
        Integer, ForeignKey("users.id"), nullable=True
    )

    status: Mapped[str] = mapped_column(String(32), default="PENDING", index=True)
    total_items: Mapped[int] = mapped_column(Integer, default=0)
    completed_items: Mapped[int] = mapped_column(Integer, default=0)
    failed_items: Mapped[int] = mapped_column(Integer, default=0)

    error_summary: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

    items: Mapped[List["UploadJobItem"]] = relationship(
        "UploadJobItem", back_populates="job", cascade="all, delete-orphan"
    )


class UploadJobItem(Base, TimestampMixin):
    """Individual item within an UploadJob.

    Each item corresponds to a single Image row and tracks the status of
    science-pipeline processing for that image.
    """

    __tablename__ = "upload_job_items"

    id: Mapped[int] = mapped_column(primary_key=True)
    job_id: Mapped[int] = mapped_column(
        Integer, ForeignKey("upload_jobs.id"), index=True
    )

    image_id: Mapped[Optional[int]] = mapped_column(
        Integer, ForeignKey("images.id"), nullable=True
    )

    filename: Mapped[str] = mapped_column(String(255))
    storage_path: Mapped[str] = mapped_column(String(512))

    status: Mapped[str] = mapped_column(String(32), default="PENDING", index=True)
    error_message: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

    job: Mapped["UploadJob"] = relationship("UploadJob", back_populates="items")
----- CONTENT END -----
----- FILE PATH: backend/models/usage.py
----- CONTENT START -----
from sqlalchemy import String, Float, JSON
from sqlalchemy.orm import Mapped, mapped_column

from backend.database.core import Base
from backend.database.mixins import TimestampMixin


class ToolUsage(Base, TimestampMixin):
    """Ledger of external tool usage (e.g., VLM calls).

    This table is intentionally generic so future tools can also log here.
    For Sprint 1, it is used primarily to track VLM image analyses and
    provide a truthful cost aggregate to the Admin budget dashboard.
    """

    __tablename__ = "tool_usage"

    id: Mapped[int] = mapped_column(primary_key=True)

    # High‚Äëlevel tool identifier, e.g. "vlm_analyze_image"
    tool_name: Mapped[str] = mapped_column(String, index=True)

    # Provider and model are kept separate so we can aggregate by either.
    provider: Mapped[str] = mapped_column(String, index=True)
    model_name: Mapped[str] = mapped_column(String, index=True)

    # Estimated cost for this call, in USD.
    cost_usd: Mapped[float] = mapped_column(Float, default=0.0)

    # Optional JSON metadata (e.g. raw usage, batch size, notes).
    meta: Mapped[dict] = mapped_column(JSON, default={})
----- CONTENT END -----
----- FILE PATH: backend/models/users.py
----- CONTENT START -----
from sqlalchemy import String, Boolean
from sqlalchemy.orm import Mapped, mapped_column, relationship
from typing import List
from backend.database.core import Base
from backend.database.mixins import TimestampMixin

class User(Base, TimestampMixin):
    """
    RBAC User Model.
    Roles:
      - 'admin': Full access, Cost Cockpit access.
      - 'scientist': Research Explorer access, Export access.
      - 'tagger': Tagger Workbench access only.
    """
    __tablename__ = "users"

    id: Mapped[int] = mapped_column(primary_key=True, index=True)
    email: Mapped[str] = mapped_column(String, unique=True, index=True)
    username: Mapped[str] = mapped_column(String, unique=True, index=True)
    hashed_password: Mapped[str] = mapped_column(String)
    full_name: Mapped[str] = mapped_column(String, nullable=True)
    
    # Role Based Access Control
    role: Mapped[str] = mapped_column(String, default="tagger")
    
    # Soft delete / Access revocation
    is_active: Mapped[bool] = mapped_column(Boolean, default=True)

    # Relationships
    # Using string reference "Validation" to avoid circular imports
    validations = relationship("Validation", back_populates="user")----- CONTENT END -----
----- FILE PATH: backend/schemas/__init__.py
----- CONTENT START -----
# backend/schemas package----- CONTENT END -----
----- FILE PATH: backend/schemas/admin.py
----- CONTENT START -----
from pydantic import BaseModel, ConfigDict
from typing import Optional

class ToolConfigUpdate(BaseModel):
    """Contract for modifying AI Model settings"""
    is_enabled: Optional[bool] = None
    cost_per_1k_tokens: Optional[float] = None
    
class ToolConfigRead(BaseModel):
    """Contract for reading AI Model state"""
    id: int
    name: str
    provider: str
    cost_per_1k_tokens: float
    is_enabled: bool
    
    model_config = ConfigDict(from_attributes=True)

class BudgetStatus(BaseModel):
    """Contract for Cost Dashboard"""
    total_spent: float
    hard_limit: float
    is_kill_switched: bool----- CONTENT END -----
----- FILE PATH: backend/schemas/annotation.py
----- CONTENT START -----
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional, Dict, Any
from backend.schemas.common import TimestampSchema

# --- INPUT SCHEMAS (Frontend -> Backend) ---

class ValidationRequest(BaseModel):
    """
    Payload sent when a Tagger presses 'Confirm' or uses a keyboard shortcut.
    """
    image_id: int
    attribute_key: str = Field(description="e.g. 'spatial.prospect'")
    value: float = Field(ge=0.0, le=1.0, description="Normalized value 0-1")
    duration_ms: int = Field(ge=0, description="Time spent looking at image (velocity tracking)")
    
class RegionCreateRequest(BaseModel):
    """
    Payload sent when a Tagger draws a box or polygon.
    """
    image_id: int
    geometry: Dict[str, Any] = Field(description="GeoJSON or {x,y,w,h}")
    manual_label: str = Field(description="Human assigned class")

# --- OUTPUT SCHEMAS (Backend -> Frontend) ---

class ImageWorkItem(TimestampSchema):
    """
    The 'Next Image' payload. Optimized for the Workbench Canvas.
    """
    id: int
    filename: str
    # Pre-signed URL or local path served via Nginx
    url: str 
    # Context for the tagger (e.g. "Is this Modern?")
    meta_data: Dict[str, Any] = {} 

class ValidationResponse(TimestampSchema):
    id: int
    status: str = "success"
    agreement_score: Optional[float] = None # Calculated async if other validators exist----- CONTENT END -----
----- FILE PATH: backend/schemas/bn_export.py
----- CONTENT START -----
"""Pydantic schemas for BN-ready export rows."""

from __future__ import annotations
from typing import Dict, Optional
from pydantic import BaseModel


class BNRow(BaseModel):
    image_id: int
    source: str
    indices: Dict[str, Optional[float]]
    bins: Dict[str, Optional[str]]
    # Optional inter-rater reliability summary for this image.
    # If present, this is typically an average agreement metric across
    # attributes, following the same pairwise-agreement logic used in
    # the Supervisor IRR endpoint.
    agreement_score: Optional[float] = None
    irr_bin: Optional[str] = None

from typing import List, Literal


class BNVariable(BaseModel):
    """Codebook entry describing a single BN variable.

    This is designed to be easily serialised to JSON and consumed by
    downstream BN / probabilistic programming tools.
    """

    name: str
    # Optional human-readable label; when omitted, `name` can be used.
    label: Optional[str] = None
    # Short description of the variable's meaning.
    description: Optional[str] = None
    # Coarse role of the variable in the export.
    # - "index": continuous science index (e.g. fractal_d, complexity)
    # - "bin": ordinal / categorical bin (e.g. low / mid / high)
    # - "meta": derived metadata (e.g. irr_bin)
    # - "id": identifier fields (e.g. image_id)
    role: Literal["index", "bin", "meta", "id"] = "index"
    # Variable type from the perspective of BN tools.
    var_type: Literal["continuous", "discrete", "ordinal"] = "continuous"
    # Optional enumerated states. For continuous variables this is typically None.
    states: Optional[List[str]] = None


class BNCodebook(BaseModel):
    """Container for the BN export codebook.

    The `variables` list fully describes the domain of each column in the
    `/v1/export/bn-snapshot` endpoint so that downstream tools do not need
    to guess at data types or valid states.
    """

    variables: List[BNVariable]

class BNValidationRow(BaseModel):
    """Flattened per-validation row for hierarchical / multi-level models.

    Each row corresponds to a single Validation record, restricted to
    science-pipeline and manual sources. Downstream tools can group by
    `image_id`, `user_id`, or `attribute_key` as needed.
    """

    image_id: int
    user_id: Optional[int]
    attribute_key: str
    value: float
    source: str
    duration_ms: int
----- CONTENT END -----
----- FILE PATH: backend/schemas/common.py
----- CONTENT START -----
from pydantic import BaseModel, ConfigDict
from datetime import datetime
from typing import Optional

class TimestampSchema(BaseModel):
    """Standard response mixin for timestamps"""
    created_at: datetime
    updated_at: Optional[datetime] = None

    model_config = ConfigDict(from_attributes=True)----- CONTENT END -----
----- FILE PATH: backend/schemas/discovery.py
----- CONTENT START -----
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

class SearchQuery(BaseModel):
    """Contract for Complex Search"""
    query_string: str = ""
    filters: Dict[str, Any] = {}
    page: int = 1
    page_size: int = 20

class ImageSearchResult(BaseModel):
    """Contract for Masonry Grid Items"""
    id: int
    url: str
    tags: List[str]
    meta_data: Dict[str, Any]
    
class ExportRequest(BaseModel):
    """Contract for Dataset Export"""
    image_ids: List[int]
    format: str = "json"
class AttributeRead(BaseModel):
    id: int
    key: str
    name: str
    category: Optional[str] = None
    level: Optional[str] = None
    description: Optional[str] = None
    range: Optional[str] = None
    sources: Optional[str] = None
    notes: Optional[str] = None

    class Config:
        orm_mode = True----- CONTENT END -----
----- FILE PATH: backend/schemas/supervision.py
----- CONTENT START -----
from __future__ import annotations

from datetime import datetime
from typing import List, Optional

from pydantic import BaseModel, ConfigDict


class TaggerPerformance(BaseModel):
    """Aggregate tagger stats for the Supervisor velocity view."""

    user_id: int
    username: str
    images_validated: int
    avg_duration_ms: int
    status: str = "active"

    model_config = ConfigDict(from_attributes=True)


class IRRStat(BaseModel):
    """Inter-rater reliability summary for a given image/attribute pair.

    This is intentionally simple and numerical so it can drive:
      * A heatmap in the Supervisor dashboard.
      * Drill-down into the Tag Inspector.
    """

    image_id: int
    filename: str
    attribute_key: str
    agreement_score: float
    conflict_count: int
    raters: List[str]

    model_config = ConfigDict(from_attributes=True)


class ValidationDetail(BaseModel):
    """Per-validation record for the Tag Inspector drawer."""

    id: int
    user_id: Optional[int] = None
    username: Optional[str] = None
    image_id: int
    attribute_key: str
    value: float
    duration_ms: Optional[int] = None
    created_at: Optional[datetime] = None

    model_config = ConfigDict(from_attributes=True)----- CONTENT END -----
----- FILE PATH: backend/schemas/training.py
----- CONTENT START -----
from datetime import datetime
from pydantic import BaseModel

class TrainingExample(BaseModel):
    """Single row of exported training data.

    This mirrors the core fields produced by TrainingExporter and can be
    consumed by downstream tools or written to JSON/JSONL.
    """
    image_id: int
    image_filename: str
    attribute_key: str
    value: float
    user_id: int
    region_id: int | None = None
    duration_ms: int | None = None
    created_at: datetime | None = None
    source: str | None = None----- CONTENT END -----
----- FILE PATH: backend/science/__init__.py
----- CONTENT START -----
# backend/science package----- CONTENT END -----
----- FILE PATH: backend/science/bn_naming_guard.py
----- CONTENT START -----
"""Lightweight BN naming guard for v3.4.36.

This script performs a best-effort scan of Bayesian Network configuration
files under ``backend/science`` and prints a summary of node names and
potential naming issues (e.g., whitespace in node identifiers).

It is deliberately *non-fatal*: it never exits with a non-zero status so it
is safe to run inside GO checks and ``install.sh`` without blocking student
workflows.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, Iterable, List, Set


ROOT = Path(__file__).resolve().parent


def _iter_bn_files() -> Iterable[Path]:
    """Yield BN-related config files under ``backend/science``.

    We look for JSON / YAML files whose names contain ``bn`` or ``rest``.
    """
    patterns = ("**/*.json", "**/*.yml", "**/*.yaml")
    for pattern in patterns:
        for path in ROOT.glob(pattern):
            name = path.name.lower()
            if "bn" in name or "rest" in name:
                yield path


def _extract_node_names(path: Path) -> List[str]:
    try:
        text = path.read_text(encoding="utf-8")
    except Exception:
        return []

    data = None
    if path.suffix.lower() == ".json":
        try:
            data = json.loads(text)
        except Exception:
            return []
    else:
        try:
            import yaml  # type: ignore
        except Exception:
            return []
        try:
            data = yaml.safe_load(text)
        except Exception:
            return []

    names: List[str] = []
    if isinstance(data, dict):
        if isinstance(data.get("nodes"), list):
            for node in data["nodes"]:
                if isinstance(node, dict) and "name" in node:
                    names.append(str(node["name"]))
        if isinstance(data.get("variables"), dict):
            names.extend(str(k) for k in data["variables"].keys())
    return names


def main() -> None:
    all_names: Dict[Path, List[str]] = {}
    for path in sorted(set(_iter_bn_files())):
        names = _extract_node_names(path)
        if names:
            all_names[path] = sorted(set(names))

    if not all_names:
        print("[bn_naming_guard] No BN config files detected; nothing to validate.")
        return

    print("[bn_naming_guard] Summary of BN node names:")
    problematic: Dict[Path, List[str]] = {}
    for path, names in all_names.items():
        bad = [n for n in names if any(c.isspace() for c in n)]
        print(f"  - {path.relative_to(ROOT)}: {len(names)} nodes")
        if bad:
            problematic[path] = bad

    if problematic:
        print("\n[bn_naming_guard] Potential naming issues (whitespace in names):")
        for path, bad in problematic.items():
            rel = path.relative_to(ROOT)
            joined = ", ".join(sorted(bad))
            print(f"  - {rel}: {joined}")
    else:
        print("\n[bn_naming_guard] No obvious naming issues detected.")

    print("\n[bn_naming_guard] Completed without fatal errors.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: backend/science/contracts.py
----- CONTENT START -----
"""
Contracts and helper utilities for science analyzers.

This module defines a minimal Analyzer protocol and helper methods to keep
the growing family of analyzers consistent and forward-compatible.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional, Protocol, List

from .core import AnalysisFrame


class Analyzer(Protocol):
    """Minimal protocol every analyzer should follow."""

    name: str
    tier: str  # e.g. "L0", "L1", "L2", "L3", "L4", "L5"
    requires: List[str]
    provides: List[str]

    def analyze(self, frame: AnalysisFrame) -> None:
        ...


@dataclass
class AnalysisError:
    analyzer: str
    reason: str
    detail: Optional[Dict[str, Any]] = None


def safe_get(frame: AnalysisFrame, key: str) -> Any:
    """Small wrapper in case we later move away from direct attributes dict."""
    return frame.attributes.get(key)


def safe_set(frame: AnalysisFrame, key: str, value: Any, confidence: float = 1.0, provenance: Optional[Dict[str, Any]] = None) -> None:
    frame.add_attribute(key, value, confidence=confidence)
    if provenance is not None:
        meta = frame.metadata.get(key, {})
        meta.update(provenance)
        frame.metadata[key] = meta


def fail(frame: AnalysisFrame, analyzer_name: str, reason: str) -> None:
    """Record a failure as metadata without poisoning numeric priors."""
    key = f"science_error.{analyzer_name}"
    frame.metadata[key] = {"reason": reason}


class AnalyzerRegistry:
    """Forward-looking registry for analyzers and their tiers/dependencies."""

    def __init__(self) -> None:
        self._analyzers: Dict[str, Analyzer] = {}

    def register(self, analyzer: Analyzer) -> None:
        self._analyzers[analyzer.name] = analyzer

    def get_all(self) -> Dict[str, Analyzer]:
        return dict(self._analyzers)

----- CONTENT END -----
----- FILE PATH: backend/science/core.py
----- CONTENT START -----
import numpy as np
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

@dataclass
class AnalysisFrame:
    """
    Standard unit of analysis for the v3 pipeline.
    Now extended to support Depth Maps and Semantic Segmentation for higher-order science.
    """
    image_id: int
    original_image: np.ndarray  # RGB, uint8
    
    # Derived data (populated by pipeline)
    gray_image: Optional[np.ndarray] = None
    lab_image: Optional[np.ndarray] = None  # CIELAB for perceptual color
    edges: Optional[np.ndarray] = None
    
    # Future-proofing for Phase 3.2 (Depth)
    depth_map: Optional[np.ndarray] = None 
    
    # Results
    attributes: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        # Lazy load opencv only when needed
        import cv2
        from skimage import color
        
        if self.gray_image is None:
            self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_RGB2GRAY)
        
        if self.edges is None:
            # L2gradient=True provides more accurate edge magnitude for architecture
            self.edges = cv2.Canny(self.gray_image, 50, 150, L2gradient=True)

        if self.lab_image is None:
            # Convert to LAB for scientifically valid color analysis
            # We use skimage because cv2's LAB scaling is non-standard/confusing
            self.lab_image = color.rgb2lab(self.original_image)

    def add_attribute(self, key: str, value: float, confidence: float = 1.0):
        """
        Add a computed attribute to the frame.
        Value should generally be normalized 0.0 - 1.0 where possible.
        """
        self.attributes[key] = float(value)
        self.metadata[key] = {"confidence": confidence}----- CONTENT END -----
----- FILE PATH: backend/science/feature_stubs.py
----- CONTENT START -----
"""
Registry of feature keys that are intentionally *stubbed*.

Any key listed here is allowed to appear in the canonical feature registry
without having a concrete compute implementation yet. This file is used by
tests to ensure there are no *accidental* dangling keys.
"""

from __future__ import annotations

from typing import Set


STUB_FEATURE_KEYS: Set[str] = {
    'N/A (Computed)',
    'arch.pattern.axial_circulation_clear',
    'arch.pattern.bay_window',
    'arch.pattern.central_hearth',
    'arch.pattern.circulation_maze_like',
    'arch.pattern.colonnade',
    'arch.pattern.corner_window',
    'arch.pattern.gallery_edge',
    'arch.pattern.loft_mezzanine',
    'arch.pattern.long_view_corridor',
    'arch.pattern.perimeter_seating',
    'arch.pattern.refuge_nook',
    'arch.pattern.skylight_dominant',
    'arch.pattern.staircase_sculptural',
    'arch.pattern.threshold_emphasized',
    'arch.pattern.window_seat_niche',
    'cnfa.biophilic.natural_material_ratio',
    'cnfa.cognitive.activity_zones_count',
    'cnfa.cognitive.landmark_salience',
    'cnfa.cognitive.legibility_score',
    'cnfa.dynamic.optic_flow_magnitude',
    'cnfa.dynamic.path_glare_max',
    'cnfa.dynamic.reflection_flow',
    'cnfa.dynamic.revelation_rate',
    'cnfa.dynamic.texture_gradient',
    'cnfa.fluency.anomaly_count',
    'cnfa.fluency.clutter_density_count',
    'cnfa.fluency.color_palette_entropy',
    'cnfa.fluency.edge_clarity_mean',
    'cnfa.fluency.figure_ground_clarity',
    'cnfa.fluency.hierarchy_depth',
    'cnfa.fluency.pattern_rhythm_regularity',
    'cnfa.fluency.processing_load_proxy',
    'cnfa.fluency.symmetry_score_horizontal',
    'cnfa.fluency.visual_entropy_spatial',
    'cnfa.fluency.zoning_clarity',
    'cnfa.fractal_dimension',
    'cnfa.haptic.soft_surface_ratio',
    'cnfa.haptic.texture_variation_index',
    'cnfa.light.brightness_variance',
    'cnfa.light.diffuse_vs_direct_ratio',
    'cnfa.light.vertical_illuminance_proxy',
    'cnfa.light.warm_vs_cool_ratio',
    'cnfa.spatial.ceiling_height_avg',
    'cnfa.spatial.enclosure_index',
    'cnfa.spatial.prospect_to_refuge_ratio',
    'component.bathroom.fixture.material.brass',
    'component.bathroom.fixture.material.matte_black',
    'component.bathroom.fixture.type.rain_showerhead',
    'component.ceiling.coffered',
    'component.ceiling.cove_lighting',
    'component.ceiling.exposed_beam',
    'component.ceiling.tray',
    'component.ceiling.vaulted',
    'component.kitchen.hardware.material.brass',
    'component.kitchen.hardware.material.chrome',
    'component.kitchen.hardware.material.matte_black',
    'component.kitchen.hardware.type.bar_pull',
    'component.kitchen.hardware.type.handleless',
    'component.kitchen.hardware.type.knob',
    'component.wall.material.brick_exposed',
    'component.wall.material.stone_wall',
    'component.wall.material.wood_slat',
    'component.wall.treatment.wainscoting',
    'component.wall.treatment.wallpaper_patterned',
    'provenance.architect_kengo_kuma',
    'provenance.architect_le_corbusier',
    'provenance.iconic_chair_barcelona',
    'provenance.iconic_chair_eames',
}
----- CONTENT END -----
----- FILE PATH: backend/science/features_canonical.jsonl
----- CONTENT START -----
{"key": "style.mid_century_modern", "category": "Style", "tier": "L4", "label": "Mid-Century Modern", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, ArchDaily, Unsplash API", "description": "\"mid-century modern\", \"MCM\""}, {"tool": "VLM (Gemini), CLIP", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Mid-Century Modern? (yes/no)\"`"}], "cfa_relevance": "Cues for 'Eames' schema, 'warm' woods, 'classic' design, low 'ornament'."}\n{"key": "style.japandi", "category": "Style", "tier": "L4", "label": "Japandi", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"japandi\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Japandi? (yes/no)\"`"}], "cfa_relevance": "Cues for 'minimalism', 'biophilia' (wood/rattan), 'calm', 'wabi-sabi' (imperfection)."}\n{"key": "style.scandinavian", "category": "Style", "tier": "L4", "label": "Scandinavian", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Unsplash API", "description": "\"scandinavian\", \"scandi\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Scandinavian? (yes/no)\"`"}], "cfa_relevance": "Cues for 'lightness' (light woods, white walls), 'minimalism', 'coziness' (hygge), 'functionality'."}\n{"key": "style.industrial", "category": "Style", "tier": "L4", "label": "Industrial", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Unsplash API", "description": "\"industrial interior\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Industrial? (yes/no)\"`"}], "cfa_relevance": "Cues for 'rawness' (exposed brick/ducts), 'authenticity', 'cold' materials (metal, concrete)."}\n{"key": "style.farmhouse", "category": "Style", "tier": "L4", "label": "Farmhouse / Modern Farmhouse", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"farmhouse\", \"modern farmhouse\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Farmhouse or Modern Farmhouse? (yes/no)\"`"}], "cfa_relevance": "Cues for 'comfort', 'nostalgia', 'rustic' materials (shiplap, barn doors), 'hearth' (social center)."}\n{"key": "style.traditional", "category": "Style", "tier": "L4", "label": "Traditional", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"traditional interior\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Traditional? (yes/no)\"`"}], "cfa_relevance": "Cues for 'formality', 'symmetry', 'ornament', 'heirloom' (status, history)."}\n{"key": "style.minimalist", "category": "Style", "tier": "L4", "label": "Minimalist", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Unsplash API", "description": "\"minimalist interior\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Minimalist? (yes/no)\"`"}], "cfa_relevance": "Low 'clutter', 'low complexity', 'low cognitive load'. Can be 'calming' or 'sterile'."}\n{"key": "style.rustic", "category": "Style", "tier": "L4", "label": "Rustic", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"rustic interior\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Rustic? (yes/no)\"`"}], "cfa_relevance": "Cues for 'refuge' (cabin), 'biophilia' (raw wood, stone), 'warmth', 'escape from technology'."}\n{"key": "style.bohemian", "category": "Style", "tier": "L4", "label": "Bohemian / Boho", "status": "active", "type": "categorical", "scale": null, "group": "style", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Unsplash API", "description": "\"bohemian\", \"boho interior\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is the primary style of this interior Bohemian? (yes/no)\"`"}], "cfa_relevance": "High 'clutter_density', 'personalization', 'soft_goods_ratio' (haptics), 'informal', 'creative'."}\n{"key": "provenance.architect_le_corbusier", "category": "Provenance", "tier": "L4", "label": "Architect: Le Corbusier", "status": "active", "type": "categorical", "scale": null, "group": "provenance", "source": "feature_set_v7.xlsx", "methods": [{"tool": "ArchDaily, Dezeen, Academic Archives", "description": "\"architect: Le Corbusier\""}, {"tool": "VLM (Gemini), CLIP", "description": "(Vector Search) `cosine_similarity(image_vector, known_corbusier_vector) > 0.8` (VLM Query) `Q: \"Does this interior strongly resemble the work of Le Corbusier? (yes/no)\"`"}], "cfa_relevance": "High 'schema' activation for 'Modernism'. Cues for 'concrete', 'primary colors', 'machine for living'."}\n{"key": "provenance.architect_kengo_kuma", "category": "Provenance", "tier": "L4", "label": "Architect: Kengo Kuma", "status": "active", "type": "categorical", "scale": null, "group": "provenance", "source": "feature_set_v7.xlsx", "methods": [{"tool": "ArchDaily, Dezeen", "description": "\"architect: Kengo Kuma\""}, {"tool": "VLM (Gemini), CLIP", "description": "(Vector Search) `cosine_similarity(image_vector, known_kuma_vector) > 0.8` (VLM Query) `Q: \"Does this interior strongly resemble the work of Kengo Kuma? (yes/no)\"`"}], "cfa_relevance": "High 'biophilia' cue, 'wood' (slats), 'lightness', 'dematerialization', 'craft'."}\n{"key": "provenance.iconic_chair_eames", "category": "Provenance", "tier": "L4", "label": "Iconic Chair: Eames Lounge Chair", "status": "active", "type": "categorical", "scale": null, "group": "provenance", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz (Visual Match), Pinterest, Design Museums", "description": "\"Eames Lounge Chair\""}, {"tool": "VLM (Gemini), Google Cloud Vision API", "description": "(Object Detection) `GCV` has a 'mid' for 'Eames Lounge Chair'. (VLM Query) `Q: \"Is there an Eames Lounge Chair in this image? (yes/no)\"`"}], "cfa_relevance": "'Mid-Century Modern' schema. 'High-status' cue. 'Perceived comfort' (affordance)."}\n{"key": "provenance.iconic_chair_barcelona", "category": "Provenance", "tier": "L4", "label": "Iconic Chair: Barcelona Chair", "status": "active", "type": "categorical", "scale": null, "group": "provenance", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz (Visual Match), Pinterest, Design Museums", "description": "\"Barcelona Chair\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is there a Barcelona Chair in this image? (yes/no)\"`"}], "cfa_relevance": "'Mies van der Rohe' schema. 'Modernist' cue. 'Formal' seating (less 'lounge' than Eames)."}\n{"key": "spatial.room_function.living_room", "category": "Spatial & Geometric", "tier": "L4", "label": "Room Function: Living Room", "status": "active", "type": "categorical", "scale": null, "group": "spatial & geometric", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, ArchDaily, ADE20K", "description": "\"Living Room\""}, {"tool": "VLM (Gemini), ADE20K Dataset", "description": "(VLM Query - Classification) `Q: \"What room is this?\"` -> `(label == 'living_room')`"}], "cfa_relevance": "Primary 'social' and 'relaxation' space. High 'sociopetal' potential."}\n{"key": "spatial.room_function.kitchen", "category": "Spatial & Geometric", "tier": "L4", "label": "Room Function: Kitchen", "status": "active", "type": "categorical", "scale": null, "group": "spatial & geometric", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, ArchDaily, ADE20K", "description": "\"Kitchen\""}, {"tool": "VLM (Gemini), ADE20K Dataset", "description": "(VLM Query - Classification) `Q: \"What room is this?\"` -> `(label == 'kitchen')`"}], "cfa_relevance": "'Task-oriented' (focus). 'Social' (hearth). 'Olfactory' cues (food)."}\n{"key": "spatial.room_function.bedroom", "category": "Spatial & Geometric", "tier": "L4", "label": "Room Function: Bedroom", "status": "active", "type": "categorical", "scale": null, "group": "spatial & geometric", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, ArchDaily, ADE20K", "description": "\"Bedroom\""}, {"tool": "VLM (Gemini), ADE20K Dataset", "description": "(VLM Query - Classification) `Q: \"What room is this?\"` -> `(label == 'bedroom')`"}], "cfa_relevance": "Primary 'refuge' and 'rest' space. Low-arousal, low-light, high 'softness' desired."}\n{"key": "spatial.room_function.home_office", "category": "Spatial & Geometric", "tier": "L4", "label": "Room Function: Home Office", "status": "active", "type": "categorical", "scale": null, "group": "spatial & geometric", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, ArchDaily, ADE20K", "description": "\"Home Office\""}, {"tool": "VLM (Gemini), ADE20K Dataset", "description": "(VLM Query - Classification) `Q: \"What room is this?\"` -> `(label == 'home_office')`"}], "cfa_relevance": "'Task-oriented' (focus). Requires 'ergonomic' affordances, 'low-distraction', 'good task lighting'."}\n{"key": "spatial.room_function.bathroom", "category": "Spatial & Geometric", "tier": "L4", "label": "Room Function: Bathroom", "status": "active", "type": "categorical", "scale": null, "group": "spatial & geometric", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, ArchDaily, ADE20K", "description": "\"Bathroom\""}, {"tool": "VLM (Gemini), ADE20K Dataset", "description": "(VLM Query - Classification) `Q: \"What room is this?\"` -> `(label == 'bathroom')`"}], "cfa_relevance": "'Private' space. 'Spa-like' (restorative) or 'functional' (task-based). 'Haptic' (water, steam)."}\n{"key": "component.ceiling.exposed_beam", "category": "Component: Ceiling", "tier": "L4", "label": "Ceiling: Exposed Beam", "status": "active", "type": "categorical", "scale": null, "group": "component: ceiling", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"exposed beam ceiling\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Are there exposed structural beams on the ceiling? (yes/no)\"`"}], "cfa_relevance": "Cues for 'rustic', 'craft', 'structure', 'shelter'. Can increase 'visual complexity'."}\n{"key": "component.ceiling.coffered", "category": "Component: Ceiling", "tier": "L4", "label": "Ceiling: Coffered", "status": "active", "type": "categorical", "scale": null, "group": "component: ceiling", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"coffered ceiling\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is this a coffered (sunken panel) ceiling? (yes/no)\"`"}], "cfa_relevance": "Cues for 'traditional', 'formal', 'high-status', 'high-complexity', 'rhythm'."}\n{"key": "component.ceiling.vaulted", "category": "Component: Ceiling", "tier": "L4", "label": "Ceiling: Vaulted", "status": "active", "type": "categorical", "scale": null, "group": "component: ceiling", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"vaulted ceiling\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is this a vaulted or cathedral ceiling? (yes/no)\"`"}], "cfa_relevance": "'Cathedral Effect'. Primes 'abstract thought', 'awe', 'openness'. Can feel 'less cozy'."}\n{"key": "component.ceiling.tray", "category": "Component: Ceiling", "tier": "L4", "label": "Ceiling: Tray", "status": "active", "type": "categorical", "scale": null, "group": "component: ceiling", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz", "description": "\"tray ceiling\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is this a tray ceiling (recessed center)? (yes/no)\"`"}], "cfa_relevance": "'Formal' cue (often in dining/bedrooms). 'Softly' defines a zone. Good for 'cove lighting'."}\n{"key": "component.ceiling.cove_lighting", "category": "Component: Ceiling", "tier": "L4", "label": "Ceiling: Cove Lighting", "status": "active", "type": "categorical", "scale": null, "group": "component: ceiling", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Lighting Catalogs", "description": "\"cove lighting\""}, {"tool": "VLM (Gemini)", "description": "(VLM Query - Binary Classification) `Q: \"Is indirect cove lighting visible at the ceiling edge? (yes/no)\"`"}], "cfa_relevance": "'Indirect light' = low glare, 'soft' shadows, 'calming' atmosphere. Reduces 'visual hot spots'."}\n{"key": "component.wall.material.brick_exposed", "category": "Component: Wall", "tier": "L4", "label": "Wall: Exposed Brick", "status": "active", "type": "categorical", "scale": null, "group": "component: wall", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, ArchDaily", "description": "\"exposed brick wall\", \"brick wall interior\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `wall` segment (SAM). 2. Pass to VLM: `Q: \"Is this wall exposed brick? (yes/no)\"`"}], "cfa_relevance": "'Industrial' or 'historic' schema. 'Authenticity'. 'Rough' texture. 'Warm' color. High 'fractal_dimension'."}\n{"key": "component.wall.material.stone_wall", "category": "Component: Wall", "tier": "L4", "label": "Wall: Stone Wall", "status": "active", "type": "categorical", "scale": null, "group": "component: wall", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, ArchDaily", "description": "\"stone wall interior\", \"stacked stone wall\" (User's 'stone walls')"}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `wall` segment (SAM). 2. Pass to VLM: `Q: \"Is this wall made of stacked stone? (yes/no)\"`"}], "cfa_relevance": "'Refuge' cue (cave-like, solid). 'Biophilia' (natural material). 'High-mass' thermal cue. 'Rustic' schema."}\n{"key": "component.wall.material.wood_slat", "category": "Component: Wall", "tier": "L4", "label": "Wall: Wood Slat / Acoustic Wood", "status": "active", "type": "categorical", "scale": null, "group": "component: wall", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, ArchDaily (Products)", "description": "\"wood slat wall\", \"acoustic wood panel\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `wall` segment (SAM). 2. Pass to VLM: `Q: \"Is this a wood slat wall? (yes/no)\"`"}], "cfa_relevance": "'Biophilia'. High 'rhythm'. 'Modern' style. Implies 'acoustic damping' (CNfA)."}\n{"key": "component.wall.treatment.wallpaper_patterned", "category": "Component: Wall", "tier": "L4", "label": "Wall: Patterned Wallpaper", "status": "active", "type": "categorical", "scale": null, "group": "component: wall", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Wallpaper Catalogs", "description": "\"patterned wallpaper\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `wall` segment (SAM). 2. Pass to VLM: `Q: \"Is this wall covered in patterned wallpaper? (yes/no)\"`"}], "cfa_relevance": "High 'visual complexity'. 'Biophilic' (if floral/natural). 'Formal' or 'playful' style. Can be 'high-load' or 'fascinating'."}\n{"key": "component.wall.treatment.wainscoting", "category": "Component: Wall", "tier": "L4", "label": "Wall: Wainscoting", "status": "active", "type": "categorical", "scale": null, "group": "component: wall", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest", "description": "\"wainscoting\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `wall` segment (SAM). 2. Pass to VLM: `Q: \"Does this wall have wainscoting (lower-half paneling)? (yes/no)\"`"}], "cfa_relevance": "'Traditional' or 'Formal' schema. Creates a 'horizontal datum line'. 'Human-scale' element."}\n{"key": "component.kitchen.hardware.material.brass", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Brass", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Hardware Catalogs (e.g., Rejuvenation)", "description": "\"brass hardware\", \"brass cabinet pulls\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet` segments (SAM). 2. Find small anomalies (hardware). 3. Pass hardware segment to VLM: `Q: \"Is this hardware brass? (yes/no)\"`"}], "cfa_relevance": "'Warm' metal. 'Classic' or 'high-status' cue. 'Haptic' affordance."}\n{"key": "component.kitchen.hardware.material.matte_black", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Matte Black", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Hardware Catalogs", "description": "\"matte black hardware\", \"matte black faucet\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet` segments (SAM). 2. Find hardware. 3. Pass hardware segment to VLM: `Q: \"Is this hardware matte black? (yes/no)\"`"}], "cfa_relevance": "'Modern', 'Industrial', or 'Farmhouse' style. 'High-contrast' graphic element."}\n{"key": "component.kitchen.hardware.material.chrome", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Chrome", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Hardware Catalogs", "description": "\"chrome hardware\", \"chrome faucet\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet` segments (SAM). 2. Find hardware. 3. Pass hardware segment to VLM: `Q: \"Is this hardware chrome/polished nickel? (yes/no)\"`"}], "cfa_relevance": "'Standard', 'clean', 'functional' cue. 'Cool' metal. Can create 'glare' (visual hot spots)."}\n{"key": "component.kitchen.hardware.type.bar_pull", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Bar Pull", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Hardware Catalogs", "description": "\"bar pull\", \"cabinet bar pull\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet hardware` segments. 2. Pass segment to VLM: `Q: \"Is this hardware a 'bar pull' or a 'knob'?\"` -> `(label == 'bar_pull')`"}], "cfa_relevance": "'Modern', 'minimal', 'rectilinear' cue. 'Clear' haptic affordance."}\n{"key": "component.kitchen.hardware.type.knob", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Knob", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Hardware Catalots", "description": "\"cabinet knob\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet hardware` segments. 2. Pass segment to VLM: `Q: \"Is this hardware a 'bar pull' or a 'knob'?\"` -> `(label == 'knob')`"}], "cfa_relevance": "'Traditional' or 'vintage' cue. 'Small-scale' haptic affordance."}\n{"key": "component.kitchen.hardware.type.handleless", "category": "Component: Kitchen", "tier": "L4", "label": "Hardware: Handleless / Integrated", "status": "active", "type": "categorical", "scale": null, "group": "component: kitchen", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Pinterest, Kitchen Catalogs", "description": "\"handleless cabinets\", \"push-to-open\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `cabinet` segments. 2. `count(hardware segments) == 0`. (VLM Query) `Q: \"Are these cabinets handleless? (yes/no)\"`"}], "cfa_relevance": "'Minimalist' schema. Low 'visual complexity'. 'Ambiguous' haptic affordance (less obvious)."}\n{"key": "component.bathroom.fixture.material.brass", "category": "Component: Bathroom", "tier": "L4", "label": "Fixture: Brass", "status": "active", "type": "categorical", "scale": null, "group": "component: bathroom", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Plumbing Catalogs (e.g., Kohler)", "description": "\"brass faucet\", \"brass showerhead\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `faucet` or `shower` segment. 2. Pass to VLM: `Q: \"Is this fixture's material brass? (yes/no)\"`"}], "cfa_relevance": "'Warm' metal. 'Classic' or 'high-status'. Cues 'wabi-sabi' if 'unlaquered brass' (patina)."}\n{"key": "component.bathroom.fixture.material.matte_black", "category": "Component: Bathroom", "tier": "L4", "label": "Fixture: Matte Black", "status": "active", "type": "categorical", "scale": null, "group": "component: bathroom", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Plumbing Catalogs", "description": "\"matte black faucet\", \"matte black showerhead\""}, {"tool": "VLM (Gemini), SAM", "description": "1. Get `faucet` or `shower` segment. 2. Pass to VLM: `Q: \"Is this fixture's material matte black? (yes/no)\"`"}], "cfa_relevance": "'Modern', 'graphic', 'high-contrast'. Can be 'calming' (low-reflection) or 'severe'."}\n{"key": "component.bathroom.fixture.type.rain_showerhead", "category": "Component: Bathroom", "tier": "L4", "label": "Fixture: Rain Showerhead", "status": "active", "type": "categorical", "scale": null, "group": "component: bathroom", "source": "feature_set_v7.xlsx", "methods": [{"tool": "Houzz, Plumbing Catalogs", "description": "\"rain showerhead\""}, {"tool": "VLM (Gemini)", "description": "1. Get `showerhead` bounding box. 2. Pass to VLM: `Q: \"Is this a rain showerhead (ceiling-mounted or large-diameter)? (yes/no)\"`"}], "cfa_relevance": "'Spa-like' affordance. Cues 'relaxation', 'luxury', 'enveloping' (full-body haptic)."}\n{"key": "cnfa.light.diffuse_vs_direct_ratio", "category": "Computed CNfA: Light", "tier": "L4", "label": "Quantifying 'softness' of light.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: light", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'Diffuse' (soft shadows) = 'calming', 'low-load', 'even' illumination. 'Direct' (hard shadows) = 'dramatic', 'high-contrast', 'focal', can be 'harsh'.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "OpenCV, VLM (Gemini)"}], "cfa_relevance": "(CV Method) 1. Find all `shadow` segments. 2. Calculate the 'entropy' of the shadow edge gradient (penumbra). High entropy = blurry/soft. Low entropy = sharp/hard. (VLM Query) `Q: \"Are the shadows in this room 'soft and diffuse' or 'hard and sharp'?\"`"}\n{"key": "cnfa.light.warm_vs_cool_ratio", "category": "Computed CNfA: Light", "tier": "L4", "label": "Quantifying overall light 'temperature'.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: light", "source": "feature_set_v7.xlsx", "methods": [{"tool": "(Circadian) 'Cool' light (>5000K) = 'alertness'. 'Warm' light (<3000K) = 'relaxation'.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "OpenCV, SAM"}], "cfa_relevance": "1. Get `light_source` (window, lamp) and `illuminated_surface` (white wall) segments (SAM). 2. Calculate the average BGR color of these pixels. 3. Convert BGR to Color Temperature (CCT) in Kelvin. 4. `Ratio = (pixels > 4000K) / (pixels < 4000K)`."}\n{"key": "cnfa.light.vertical_illuminance_proxy", "category": "Computed CNfA: Light", "tier": "L4", "label": "Quantifying light on vertical surfaces (walls).", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: light", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'Vertical illuminance' (lit walls) is key to making a space feel 'bright' and 'open', more so than just floor/ceiling light.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "SAM, OpenCV"}], "cfa_relevance": "1. Get `wall` segments (SAM). 2. Calculate the `mean(pixel_brightness)` of *only* the wall segments. 3. Compare `brightness(wall)` to `brightness(floor)`."}\n{"key": "cnfa.spatial.prospect_to_refuge_ratio", "category": "Computed CNfA: Spatial", "tier": "L4", "label": "Quantifying the 'Prospect-Refuge' balance (Appleton).", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: spatial", "source": "feature_set_v7.xlsx", "methods": [{"tool": "The 'ideal' state. High prospect ('I can see') + High refuge ('I cannot be seen'). The 'captain's chair' or 'cozy nook with a view'.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "Apple RoomPlan API, 3D Mesh Library"}], "cfa_relevance": "(Complex Calculation) 1. Get 3D mesh. 2. For each `seat` (refuge spot): 3. Calculate `prospect_score` (isovist area from seat). 4. Calculate `refuge_score` (1 - visibility *from* main paths). 5. `Ratio = max(prospect_score * refuge_score)`."}\n{"key": "cnfa.spatial.enclosure_index", "category": "Computed CNfA: Spatial", "tier": "L4", "label": "Quantifying the 'sense of enclosure'. (User's 'privacy.enclosure')", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: spatial", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Enclosure' = 'refuge', 'privacy', 'cozy', 'intimate' (can be 'claustrophobic'). 'Low' = 'open', 'social', 'public' (can be 'exposed').", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "Apple RoomPlan API, VLM (Gemini)"}], "cfa_relevance": "(Strong Method) 1. Get 3D mesh (RoomPlan). 2. `Index = 1 - (area(openings) / area(total_wall_surface))`. (VLM Query) `Q: \"On a scale of 1-10, how 'enclosed' or 'cozy' does this space feel?\"`"}\n{"key": "cnfa.spatial.ceiling_height_avg", "category": "Computed CNfA: Spatial", "tier": "L4", "label": "Quantifying the 'Cathedral Effect'.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: spatial", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Ceiling' (>3m) = primes 'abstract' thought, 'awe'. 'Low Ceiling' (<2.5m) = primes 'concrete' thought, 'focus', 'coziness'.", "description": "Apple RoomPlan API"}, {"tool": "N/A (Computed)", "description": "Apple RoomPlan API"}], "cfa_relevance": "(Strong Method) This is a *direct output* of the `RoomPlan` API, `room.height`."}\n{"key": "cnfa.cognitive.legibility_score", "category": "Computed CNfA: Cognitive", "tier": "L4", "label": "Quantifying 'wayfinding' ease (Lynch).", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: cognitive", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Legibility' = 'low cognitive load', 'easy to map', 'comfortable'. 'Low' (ambiguous paths) = 'stress', 'confusion', 'mystery'.", "description": ""}, {"tool": "", "description": ""}], "cfa_relevance": ""}\n{"key": "N/A (Computed)", "category": "N/A (Computed)", "tier": "L4", "label": "VLM (Gemini), Apple RoomPlan API", "status": "active", "type": "categorical", "scale": null, "group": "n/a (computed)", "source": "feature_set_v7.xlsx", "methods": [{"tool": "(Strong Method) 1. Get 2D floor plan (RoomPlan). 2. Run 'Space Syntax' analysis (calculate 'integration' and 'intelligibility' of the graph). (VLM Query) `Q: \"Does this space look 'easy' or 'confusing' to navigate?\"`", "description": ""}, {"tool": "", "description": ""}], "cfa_relevance": ""}\n{"key": "cnfa.cognitive.landmark_salience", "category": "Computed CNfA: Cognitive", "tier": "L4", "label": "Quantifying 'landmarks' for navigation (Lynch).", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: cognitive", "source": "feature_set_v7.xlsx", "methods": [{"tool": "A 'salient landmark' (e.g., fireplace, sculpture, unique window) 'anchors' the mental map, reducing cognitive load.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "Salience Models (FASA, etc.), VLM (Gemini)"}], "cfa_relevance": "(CV Method) 1. Generate a 'visual salience map'. 2. `Score = max(salience_peak_value)`. A high peak = a clear landmark. (VLM Query) `Q: \"What is the single most memorable object or feature in this room?\"`"}\n{"key": "cnfa.cognitive.activity_zones_count", "category": "Computed CNfA: Cognitive", "tier": "L4", "label": "Quantifying 'functional density'.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: cognitive", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Count' (many zones) = 'complex', 'high-function' (e.g., studio apt). 'Low Count' = 'simple', 'focused' (e.g., bedroom).", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "VLM (Gemini)"}], "cfa_relevance": "(VLM Query - Count) `Q: \"How many distinct activity zones can you see (e.g., a 'reading zone', a 'dining zone', a 'TV zone')? List them.\"` -> `count(list)`"}\n{"key": "cnfa.haptic.soft_surface_ratio", "category": "Computed CNfA: Haptic", "tier": "L4", "label": "Quantifying 'haptic comfort' and 'acoustic softness'.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: haptic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Ratio' = 'soft haptics', 'acoustic damping', 'cozy', 'low-reverb'. 'Low' (e.g., all hard surfaces) = 'cold haptics', 'high-reverb', 'loud'.", "description": "N/A (Computed)"}, {"tool": "N/A (Computed)", "description": "VLM (Gemini), SAM"}], "cfa_relevance": "1. Use SAM to segment all major surfaces. 2. For each, ask VLM: `Q: \"Is this surface 'soft' (e.g., rug, curtain, sofa, plant) or 'hard' (e.g., wood, concrete, glass, plaster)?\"` 3. `Ratio = area(Soft) / area(Total)`."}\n{"key": "cnfa.haptic.texture_variation_index", "category": "Computed CNfA: Haptic", "tier": "L4", "label": "Quantifying 'haptic richness'.", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: haptic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "'High Variation' (e.g., wood, stone, wool, glass) = 'rich sensory' experience (Barsalou), 'high-interest', 'craft'. 'Low' (e.g., all drywall) = 'simple', 'low-load', 'sterile'.", "description": ""}, {"tool": "", "description": ""}], "cfa_relevance": ""}\n{"key": "N/A (Computed)", "category": "N/A (Computed)", "tier": "L4", "label": "VLM (Gemini), SAM, OpenCV", "status": "active", "type": "categorical", "scale": null, "group": "n/a (computed)", "source": "feature_set_v7.xlsx", "methods": [{"tool": "(CV Method) 1. Get all surfaces (SAM). 2. For each, calculate a 'texture vector' (Gabor, GLCM). 3. `Index = variance(all_texture_vectors)`. (VLM) `Q: \"Count the number of different textures you can see (e.g., 'smooth glass', 'rough wood', 'soft fabric').\"`", "description": ""}, {"tool": "", "description": ""}], "cfa_relevance": ""}\n{"key": "cnfa.dynamic.optic_flow_magnitude", "category": "Computed CNfA: Dynamic", "tier": "L4", "label": "Optic Flow Magnitude (Avg)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: dynamic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "3D Model (RoomPlan) + 3D Engine (three.js) OR Video (OpenCV)", "description": "(3D Model) 1. Simulate camera move from 'entry' to 'focal_point'. 2. Run OpenCV's 'calcOpticalFlowFarneback' on the rendered frames. 3. Average the vector magnitudes. (Video) 1. Run 'calcOpticalFlowFarneback' directly on the video. 2. Average magnitudes."}], "cfa_relevance": "Gibson's 'optic flow'. High magnitude (e.g., 'tunneling') increases arousal/stress. Low magnitude ('open lobby') is calmer. This is the brain's primary cue for 'speed' and 'space'."}\n{"key": "cnfa.dynamic.revelation_rate", "category": "Computed CNfA: Dynamic", "tier": "L4", "label": "Revelation Rate (Mystery)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: dynamic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "3D Model (RoomPlan) + 3D Engine (three.js)", "description": "(3D Model) 1. Simulate camera move along 'primary_path'. 2. At each step(t), calculate `total_visible_surface_area`. 3. `Rate = d(Area) / d(t)`. A high, sustained rate = high 'Mystery'."}], "cfa_relevance": "Kaplan's 'Mystery'. The rate at which new information is revealed by moving. 'High' rate is 'engaging' and 'fascinating'. 'Zero' rate is 'boring'. 'Infinite' (e.g., a corner) is 'surprising'."}\n{"key": "cnfa.dynamic.texture_gradient", "category": "Computed CNfA: Dynamic", "tier": "L4", "label": "Dynamic Texture Gradient (slat example)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: dynamic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "3D Model (RoomPlan) + 3D Engine + OpenCV OR VLM (Depth Map) + OpenCV", "description": "(3D Model) 1. Simulate camera *approaching* a textured wall. 2. For each frame, run FFT on the texture to get `dominant_spatial_frequency`. 3. `Gradient = d(Frequency) / d(distance)`. (Weak Method) 1. Get 2.5D depth map. 2. Calculate FFT on texture at 'point A' vs. 'point B'. 3. Get gradient."}], "cfa_relevance": "Gibson's 'texture gradient'. This is a primary cue for 'scale', 'distance', and 'speed'. A 'steep' gradient (e.g., a fine-grained texture) provides a powerful cue for your own motion."}\n{"key": "cnfa.dynamic.path_glare_max", "category": "Computed CNfA: Dynamic", "tier": "L4", "label": "Path-Based Glare (Max)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: dynamic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "3D Model (RoomPlan) + 3D Engine (Unity, Unreal)", "description": "(3D Model) 1. Simulate camera move along 'primary_path' *with a light source (sun)*. 2. Use ray-tracing to render reflections. 3. At each frame, run the *static* 'cnfa.light.glare_probability' algorithm. 4. `Result = max(glare_prob_along_path)`."}], "cfa_relevance": "Measures *experiential* visual discomfort. A static photo might miss the blinding glare that only happens when you walk past a window. This is a key 'stressor' that static analysis fails to capture."}\n{"key": "cnfa.dynamic.reflection_flow", "category": "Computed CNfT: Dynamic", "tier": "L4", "label": "Reflection Flow (Shimmer)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnft: dynamic", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "3D Model (RoomPlan) + 3D Engine OR Video (OpenCV)", "description": "(Video/3D Render) 1. Identify 'glossy' surfaces (e.g., polished floor, water). 2. Run 'Optical Flow' *only* on those surfaces. 3. `Flow = avg_magnitude(vectors_on_glossy_surface)`."}], "cfa_relevance": "The 'shimmer' of light on water or a polished floor. Can be a 'Soft Fascination' (restorative, biophilic) or a 'Distraction' (high-load, annoying)."}\n{"key": "cnfa.fluency.edge_clarity_mean", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Edge Clarity / Predictability", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "OpenCV", "description": "(CV Method) 1. Run Canny or Sobel edge detection. 2. Calculate the *average gradient (sharpness)* of all detected edge pixels. High avg = 'crisp', 'high-contrast', 'easy-to-process' edges. Low avg = 'blurry', 'ambiguous' edges."}], "cfa_relevance": "'Predictive Coding'. Our brain 'predicts' edges. Sharp, clear edges fulfill these predictions easily, leading to 'high fluency' (feels good, low-load). Ambiguous edges create 'prediction error' (higher load, 'uneasy' feeling)."}\n{"key": "cnfa.fluency.pattern_rhythm_regularity", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Pattern Rhythm / Regularity (User's 'slats')", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "OpenCV, SAM", "description": "(CV Method) 1. Segment a repeating pattern (e.g., 'wood slats', 'tile grid') using SAM. 2. Take a 1D slice across the pattern. 3. Run a Fast Fourier Transform (FFT) on this signal. A single, sharp peak = high regularity. A noisy, broad signal = low regularity."}], "cfa_relevance": "'Easy to Gestalt'. A regular rhythm (like wood slats) is perceptually 'simple' because the brain can model it with one simple rule (e.g., 'repeat every 5cm'). This is 'high fluency' and can be 'calming' or 'hypnotic'."}\n{"key": "cnfa.fluency.symmetry_score_horizontal", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Horizontal Symmetry Score", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "OpenCV", "description": "(CV Method) 1. `image_flipped = cv2.flip(image, 1)` (horizontal flip). 2. Calculate the 'Structural Similarity Index' (SSIM) between `image` and `image_flipped`. 3. `Symmetry Score = SSIM_result`."}], "cfa_relevance": "'High Fluency' / 'Pr√§gnanz'. Symmetry is the 'simplest' visual form. It is processed *very* quickly and easily by the brain. Often associated with 'formality', 'stability', and 'beauty', but also 'static' or 'boring'."}\n{"key": "cnfa.fluency.visual_entropy_spatial", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Visual Entropy (Spatial Disorder)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "Google Cloud Vision API, OpenCV", "description": "(CV Method) 1. Get bounding boxes for all objects (GCV). 2. Create a 2D spatial histogram (grid) of the object centers. 3. Calculate the Shannon entropy of this distribution. High entropy = high disorder (objects scattered randomly)."}], "cfa_relevance": "This is 'disfluency'. 'High Entropy' = 'chaotic', 'disorganized', 'high-load'. 'Low Entropy' (e.g., everything on a grid) = 'orderly', 'low-load', 'legible'."}\n{"key": "cnfa.fluency.clutter_density_count", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Clutter Density (Object Count)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "Google Cloud Vision API", "description": "(CV Method) 1. Use GCV `objectLocalization` to get a list of *all* objects. 2. `Density = count(objects) / area(room_floor)`. 3. Normalize."}], "cfa_relevance": "'High Cognitive Load'. This is a *count* of items the brain must process. Differs from 'entropy' (which is 'disorder'). A full bookshelf is 'dense' but not 'entropic'. High density = high load."}\n{"key": "cnfa.fluency.color_palette_entropy", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Color Palette Entropy (Complexity)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "Google Cloud Vision API, OpenCV", "description": "(CV Method) 1. Get the `dominant_palette` (from GCV or OpenCV k-means clustering). 2. Calculate the Shannon entropy of the palette's pixel-fraction distribution. Low entropy = monochromatic (fluent). High entropy = many colors (complex)."}], "cfa_relevance": "'High Fluency' (low entropy) = 'calming', 'serene', 'monochromatic'. 'Disfluency' (high entropy) = 'vibrant', 'chaotic', 'playful', 'high-arousal'."}\n{"key": "cnfa.fluency.figure_ground_clarity", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Figure-Ground Clarity", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "VLM (Gemini), SAM", "description": "(VLM Query) `Q: \"On a scale of 1-10, how easy is it to distinguish objects from their background in this image?\"` (CV Method) 1. Get `object` segments (SAM). 2. Get `wall/floor` segments (SAM). 3. Calculate the avg. color/texture *contrast* between object edges and their background."}], "cfa_relevance": "'Gestalt - Figure/Ground'. 'High Clarity' = 'legible', 'low-load'. 'Low Clarity' (e.g., a beige chair against a beige wall) = 'ambiguous', 'high-load', but can also feel 'serene' or 'blended'."}\n{"key": "cnfa.fluency.processing_load_proxy", "category": "Computed CNfA: Perceptual Fluency", "tier": "L4", "label": "Processing Load Proxy (Compression)", "status": "active", "type": "categorical", "scale": null, "group": "computed cnfa: perceptual fluency", "source": "feature_set_v7.xlsx", "methods": [{"tool": "N/A (Computed)", "description": "N/A (Computed)"}, {"tool": "ImageMagick, PIL (Python)", "description": "(CV Method) 1. Take the full-res image. 2. Save it as a JPEG at 90% quality. 3. `Load_Proxy = file_size_in_bytes / total_pixels`. A high ratio = high-frequency detail (complex). A low ratio = large simple surfaces (fluent)."}], "cfa_relevance": "This is a 'pure' measure of visual information density. 'High Load' (e.g., a patterned wallpaper) literally requires more data to store, just as it requires more cognitive effort to process. 'Low Load' (a plain painted wall) is 'fluent'."}\n{"key": "cnfa.fluency.processing_load_proxy", "category": "Goldilocks", "tier": "L4", "label": "Visual Information Density", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive Load", "below": "'Sterile', 'Boring', 'Under-stimulating'. A plain white box.", "optimal": "'Engaging', 'Rich', 'Full'. A well-curated library or a room with a complex-but-ordered wallpaper.", "above": "'Chaotic', 'Overwhelming', 'High-load'. A hoarder's room."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) JPEG file size / total_pixels. High ratio = high-frequency detail."}]}\n{"key": "cnfa.fluency.clutter_density_count", "category": "Goldilocks", "tier": "L4", "label": "Clutter Density (Objects)", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive Load", "below": "'Sterile', 'Impersonal', 'Unlived-in'. A new-build show-home.", "optimal": "'Lived-in', 'Personalized', 'Cozy'. 'Organized density' like a full bookshelf.", "above": "'Cluttered', 'Stressful', 'Overwhelming'. Reduces focus and induces anxiety."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) `count(all_detected_objects) / area(room_floor)`."}]}\n{"key": "cnfa.fluency.visual_entropy_spatial", "category": "Goldilocks", "tier": "L4", "label": "Spatial Disorder", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive Load", "below": "'Rigid', 'Static', 'Boring'. All objects are on a perfect grid.", "optimal": "'Natural', 'Organic', 'Balanced Asymmetry'. Objects are placed intentionally but not rigidly.", "above": "'Chaotic', 'Messy', 'Disorganized'. Objects are scattered randomly."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) Shannon entropy of the 2D spatial histogram of object centers."}]}\n{"key": "cnfa.fluency.hierarchy_depth", "category": "Goldilocks", "tier": "L4", "label": "**NEW: Visual Hierarchy Depth**", "status": "active", "type": "ordinal", "scale": {"dimension": "**Cognitive Load / Fluency**", "below": "**'Shallow' (1-2 levels).** An empty, sterile room. (e.g., 'Room' -> 'Floor'). Low information, 'boring'.", "optimal": "**'Deep' (3+ levels).** 'Organized Complexity'. (e.g., 'Room' -> 'Reading Nook' (Zone) -> 'Bookshelf' (Object) -> 'Books' (Sub-Objects)). 'Rich' but 'legible' and 'easy-to-process'.", "above": "**'Flat' (1 level).** A 'hoarder' room. All objects are 'competing' on the same level, with no clear 'groups-within-groups'. 'Chaotic', 'overwhelming'."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "**(VLM Query) `Q: \"Describe this scene as a nested list of zones and objects. How many levels deep is this list?\"` (CV Method) 1. Segment `zones` (VLM). 2. Find `major_objects` in zones (GCV). 3. Find `minor_objects` on major_objects (GCV). 4. `Depth = max_nesting_level`."}]}\n{"key": "cnfa.fluency.color_palette_entropy", "category": "Goldilocks", "tier": "L4", "label": "Color Palette Complexity", "status": "active", "type": "ordinal", "scale": {"dimension": "Aesthetics & Affect", "below": "'Monochromatic', 'Boring', 'Sterile'. A single, low-saturation color.", "optimal": "'Balanced', 'Coherent', 'Accented'. A clear 2-3 color palette with a 'pop' of an accent color.", "above": "'Vibrant', 'Chaotic', 'Over-stimulating', 'Childish'. Too many competing, high-saturation colors."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) Shannon entropy of the dominant color palette's pixel-fraction distribution."}]}\n{"key": "cnfa.fluency.pattern_rhythm_regularity", "category": "Goldilocks", "tier": "L4", "label": "Pattern Regularity", "status": "active", "type": "ordinal", "scale": {"dimension": "Perceptual Fluency", "below": "'Boring', 'Plain'. A completely flat, un-patterned surface.", "optimal": "'Fluent', 'Calming', 'Hypnotic'. 'Just right' rhythm, like wood slats, a brick wall, or a simple striped rug.", "above": "'Dizzying', 'Overwhelming'. 'Op-Art' or 'vibrating' high-frequency patterns."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) FFT on a 1D slice of a repeating pattern. The 'peak' sharpness of the frequency."}]}\n{"key": "cnfa.fluency.symmetry_score_horizontal", "category": "Goldilocks", "tier": "L4", "label": "Symmetry", "status": "active", "type": "ordinal", "scale": {"dimension": "Perceptual Fluency", "below": "'Chaotic', 'Unbalanced', 'Stressful'. (Note: Low-end is *full asymmetry*).", "optimal": "'Balanced Asymmetry'. The state of being 'visually balanced' without being perfectly symmetrical. This is the goal of most design.", "above": "'Static', 'Boring', 'Rigid', 'Formal'. (Note: High-end is *perfect symmetry*)."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) `SSIM(image, flipped(image))`. The Goldilocks zone is *not* 1.0, but likely in the 0.4-0.7 range."}]}\n{"key": "cnfa.cognitive.activity_zones_count", "category": "Goldilocks", "tier": "L4", "label": "Functional Density", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive Load", "below": "'Uni-functional', 'Boring'. A room with only one purpose (e.g., just a bed).", "optimal": "'Multi-functional', 'Legible', 'Rich'. A few (2-3) *clearly defined* zones, e.g., a 'reading nook' and a 'conversation pit'.", "above": "'Chaotic', 'Confusing', 'Ambiguous'. A studio apt. where 10 functions overlap with no clear boundaries."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(VLM Query - Count) `Q: \"How many distinct activity zones can you see? List them.\" -> count(list)`"}]}\n{"key": "cnfa.fluency.zoning_clarity", "category": "Goldilocks", "tier": "L4", "label": "**NEW: Functional Zone Clarity**", "status": "active", "type": "ordinal", "scale": {"dimension": "**Cognitive Load / Fluency**", "below": "'Ambiguous', 'Messy'. A single 'mono-zone' where functions (eating, sleeping, working) are all mixed up. (High `activity_zones_count` but low `zoning_clarity`).", "optimal": "'Legible', 'Flowing'. 'Soft' boundaries (e.g., a rug, a pendant light, a half-wall) clearly *define* each zone without rigidly *separating* them.", "above": "'Rigid', 'Cellular'. A room with high, hard, and inflexible barriers between every single function."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "**(VLM Query) `Q: \"How clearly defined are the different activity zones? (e.g., 'Very Clear', 'Somewhat Clear', 'Not Clear')\"` (CV Method) `avg(distance_between_zone_centers) / avg(zone_size)`."}]}\n{"key": "cnfa.haptic.texture_variation_index", "category": "Goldilocks", "tier": "L4", "label": "Haptic/Textural Richness", "status": "active", "type": "ordinal", "scale": {"dimension": "Haptics & Affect", "below": "'Sterile', 'Smooth', 'One-note'. All surfaces are drywall and glass.", "optimal": "'Rich', 'Sensory', 'Grounded'. A 'just right' mix of soft (wool), hard (wood), and smooth (glass).", "above": "'Messy', 'Noisy', 'Unfocused'. Every surface is a different, competing texture."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) `variance(all_surface_texture_vectors)`. (VLM) `Q: \"Count the number of different textures you can see.\""}]}\n{"key": "cnfa.biophilic.natural_material_ratio", "category": "Goldilocks", "tier": "L4", "label": "Biophilic Ratio", "status": "active", "type": "ordinal", "scale": {"dimension": "Biophilia & Affect", "below": "'Artificial', 'Sterile', 'Cold'. 0% natural materials. Plastic and laminate.", "optimal": "'Warm', 'Grounded', 'Calm'. A 'just right' balance of natural materials (wood, stone, plants) with neutral carrier\" surfaces (drywall).\"", "above": "'Overwhelming', 'Rustic', 'Dark'. 100% natural materials, like a raw, dark log cabin."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(VLM/SAM Method) `Ratio = area(Natural_Materials) / area(Total_Surfaces)`."}]}\n{"key": "cnfa.fractal_dimension", "category": "Goldilocks", "tier": "L4", "label": "Fractal Dimension (Complexity)", "status": "active", "type": "ordinal", "scale": {"dimension": "Biophilia & Fluency", "below": "'Boring', 'Euclidean', 'Man-made'. (D ‚âà 1.0). A blank wall.", "optimal": "'Restorative', 'Natural', 'Effortless'. The Goldilocks Zone\" (D ‚âà 1.3-1.7) that mimics natural forms (trees", "above": "'Noisy', 'Chaotic', 'Unnatural'. (D > 1.9). Visual white noise, a furry\" pattern.\""}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": " clouds).\""}]}\n{"key": "cnfa.light.brightness_variance", "category": "Goldilocks", "tier": "L4", "label": "Light Contrast (Chiaroscuro)", "status": "active", "type": "ordinal", "scale": {"dimension": "Light & Affect", "below": "'Flat', 'Boring', 'Shadowless', 'Office-like'. Low-variance, diffuse light.", "optimal": "'Layered', 'Soft', 'Cozy', 'Focal'. 'Just right' contrast with a clear main light, ambient fill, and soft penumbras.", "above": "'Harsh', 'Dramatic', 'Interrogation-like'. Extreme high-contrast with deep, black shadows."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) `standard_deviation(grayscale_pixel_values)`. The sweet spot is a *medium* std.dev."}]}\n{"key": "cnfa.fluency.edge_clarity_mean", "category": "Goldilocks", "tier": "L4", "label": "**NEW: Edge Clarity / Predictability**", "status": "active", "type": "ordinal", "scale": {"dimension": "**Perceptual Fluency**", "below": "'Ambiguous', 'Blurry', 'Low-contrast'. (e.g., a beige sofa against a beige wall). 'High-load', 'uneasy' feeling.", "optimal": "'Clear', 'Legible', 'Softly Defined'. Clear figure-ground separation with a mix of 'soft' (shadow) and 'hard' (material) edges. 'Easy-to-process'.", "above": "'Harsh', 'Graphic', 'Over-defined'. Every single edge is a hard, black line. Can feel 'artificial' or 'cartoonish'."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "**(CV Method) 1. Run Canny/Sobel edge detection. 2. Calculate the *average gradient (sharpness)* of all detected edge pixels.**"}]}\n{"key": "cnfa.spatial.enclosure_index", "category": "Goldilocks", "tier": "L4", "label": "Sense of Enclosure", "status": "active", "type": "ordinal", "scale": {"dimension": "Spatial & Affect", "below": "'Exposed', 'Public', 'Vast', 'Unsafe'. An open field.", "optimal": "'Cozy', 'Safe', 'Intimate', 'Refuge'. A 'nook', a 'booth', or a room with a 'just right' ceiling height.", "above": "'Claustrophobic', 'Cramped', 'Confined'. A tiny, windowless room."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(3D/RoomPlan Method) `Index = 1 - (area(openings) / area(total_wall_surface))`"}]}\n{"key": "cnfa.dynamic.revelation_rate", "category": "Goldilocks", "tier": "L4", "label": "Mystery / Revelation Rate", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive & Affect", "below": "'Boring', 'Legible', 'Obvious'. A long, straight corridor where everything is visible.", "optimal": "'Engaging', 'Curious', 'Enticing'. A 'just right' curve in a path that *promises* new information without being confusing.", "above": "'Chaotic', 'Confusing', 'Disorienting'. A maze where every turn is a surprise."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(3D/RoomPlan Method) `d(Visible_Area) / d(Time)` along a simulated path."}]}\n{"key": "cnfa.cognitive.landmark_salience", "category": "Goldilocks", "tier": "L4", "label": "Landmark Clarity", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive Load", "below": "'Confusing', 'Monotonous', 'Hard to Navigate'. No clear focal point.", "optimal": "'Legible', 'Anchored', 'Clear'. One or two unambiguous landmarks (e.g., a fireplace, a large window) that anchor the mental map.", "above": "'Chaotic', 'Distracting', 'Competing'. Too many 'focal points' (e.g., 5 bright red chairs)."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(CV Method) `max(salience_peak_value)` and `entropy(salience_map)`. Optimal = 1-2 high peaks (low entropy)."}]}\n{"key": "cnfa.fluency.anomaly_count", "category": "Goldilocks", "tier": "L4", "label": "Novelty / Anomaly Count", "status": "active", "type": "ordinal", "scale": {"dimension": "Cognitive & Affect", "below": "'Boring', 'Predictable', 'Schema-confirming'. A 100% generic hotel room.", "optimal": "'Interesting', 'Novel', 'Memorable'. A room that is 95% 'normal' but has *one* 'just right' anomaly or surprise (e.g., an unexpected color, a 'scale.gulliver_effect' object).", "above": "'Bizarre', 'Confusing', 'Schema-violating'. A room where nothing makes sense (e.g., 'schema.functional_mismatch')."}, "group": "goldilocks", "source": "Gemini features goldilocks_.xlsx", "methods": [{"tool": "CV/VLM", "description": "(VLM Query) `Q: \"Count the number of 'surprising' or 'unusual' objects or features in this room.\"`"}]}\n{"key": "materials.cues.brightness_mean", "category": "Materials", "tier": "L1", "read_only": true, "description": "Mean luminance of the scene (0-1), used as a cue for material and lighting inferences.", "method": "cv:gray_mean"}\n{"key": "materials.cues.texture_variance", "category": "Materials", "tier": "L1", "read_only": true, "description": "Global variance of grayscale intensity (roughness proxy).", "method": "cv:gray_var"}\n{"key": "materials.cues.saturation_mean", "category": "Materials", "tier": "L1", "read_only": true, "description": "Mean HSV saturation, supporting cues to painted vs raw materials.", "method": "cv:hsv_mean_s"}\n{"key": "materials.cues.value_mean", "category": "Materials", "tier": "L1", "read_only": true, "description": "Mean HSV value (brightness) for material cueing.", "method": "cv:hsv_mean_v"}\n{"key": "materials.cues.specularity_proxy", "category": "Materials", "tier": "L1", "read_only": true, "description": "Proportion of high-value, low-saturation pixels; coarse proxy for specular highlights.", "method": "cv:spec_mask_ratio"}\n{"key": "materials.substrate.stone_concrete", "category": "Materials", "tier": "L2", "read_only": true, "description": "Coverage fraction plausibly corresponding to stone or concrete surfaces.", "method": "cv:hsv_low_sat_mid_value"}\n{"key": "materials.substrate.plaster_gypsum", "category": "Materials", "tier": "L2", "read_only": true, "description": "Coverage fraction plausibly corresponding to plastered or painted gypsum surfaces.", "method": "cv:hsv_very_low_sat_high_value"}\n{"key": "materials.substrate.tile_ceramic", "category": "Materials", "tier": "L2", "read_only": true, "description": "Coverage fraction plausibly corresponding to ceramic or tile surfaces.", "method": "cv:hsv_bright_sat_plus_local_var"}\n{"key": "material.wood_coverage", "category": "Materials", "tier": "L1", "read_only": true, "description": "Estimated coverage fraction of wood-like pixels.", "method": "cv:hsv_brownish"}\n{"key": "material.metal_coverage", "category": "Materials", "tier": "L1", "read_only": true, "description": "Estimated coverage fraction of metal-like pixels.", "method": "cv:hsv_low_sat_high_v"}\n{"key": "material.glass_coverage", "category": "Materials", "tier": "L1", "read_only": true, "description": "Estimated coverage fraction of glass-like pixels.", "method": "cv:bright_low_var"}\n{"key": "arch.pattern.prospect_strong", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Strong sense of outward view and visual reach.", "method": "vlm:pattern_v1", "tags": ["Prospect"]}\n{"key": "arch.pattern.refuge_strong", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Strong sense of enclosure or protection.", "method": "vlm:pattern_v1", "tags": ["Refuge"]}\n{"key": "arch.pattern.refuge_nook", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Local 'nook' or alcove affording retreat.", "method": "vlm:pattern_v1", "tags": ["Refuge"]}\n{"key": "arch.pattern.axial_circulation_clear", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Clear axial circulation path.", "method": "vlm:pattern_v1", "tags": ["Circulation"]}\n{"key": "arch.pattern.circulation_maze_like", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Complex or maze-like circulation.", "method": "vlm:pattern_v1", "tags": ["Circulation"]}\n{"key": "arch.pattern.double_height_space", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Double-height or tall volume.", "method": "vlm:pattern_v1", "tags": ["Volume"]}\n{"key": "arch.pattern.corner_window", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Corner window or wrap-around glazing.", "method": "vlm:pattern_v1", "tags": ["Fenestration"]}\n{"key": "arch.pattern.perimeter_seating", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Built-in or arranged seating at room perimeter.", "method": "vlm:pattern_v1", "tags": ["Social"]}\n{"key": "arch.pattern.central_hearth", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Fireplace or hearth as social focal point.", "method": "vlm:pattern_v1", "tags": ["Social"]}\n{"key": "arch.pattern.gallery_edge", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Balcony or gallery overlooking lower space.", "method": "vlm:pattern_v1", "tags": ["Circulation"]}\n{"key": "arch.pattern.daylight_soft", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Soft, diffuse daylight with limited glare.", "method": "vlm:pattern_v1", "tags": ["Light"]}\n{"key": "arch.pattern.daylight_hard", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Strong, direct daylight with sharp shadows.", "method": "vlm:pattern_v1", "tags": ["Light"]}\n{"key": "arch.pattern.skylight_dominant", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Skylights as primary daylight source.", "method": "vlm:pattern_v1", "tags": ["Light"]}\n{"key": "arch.pattern.threshold_emphasized", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Emphasis on entry threshold.", "method": "vlm:pattern_v1", "tags": ["Threshold"]}\n{"key": "arch.pattern.colonnade", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Series of columns forming a colonnade.", "method": "vlm:pattern_v1", "tags": ["Structure"]}\n{"key": "arch.pattern.bay_window", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Projecting bay window or oriel.", "method": "vlm:pattern_v1", "tags": ["Fenestration"]}\n{"key": "arch.pattern.staircase_sculptural", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Staircase acting as sculptural element.", "method": "vlm:pattern_v1", "tags": ["Feature"]}\n{"key": "arch.pattern.long_view_corridor", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Long view along a corridor or axis.", "method": "vlm:pattern_v1", "tags": ["Prospect"]}\n{"key": "arch.pattern.loft_mezzanine", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Loft or mezzanine overlooking main space.", "method": "vlm:pattern_v1", "tags": ["Volume"]}\n{"key": "arch.pattern.window_seat_niche", "category": "ArchitecturalPattern", "tier": "L3", "read_only": true, "description": "Window seat or deep sill forming niche.", "method": "vlm:pattern_v1", "tags": ["Refuge"]}\n----- CONTENT END -----
----- FILE PATH: backend/science/features_registry.py
----- CONTENT START -----
"""
Canonical feature registry for Image Tagger.

This module loads the forward-looking CNfA feature/attribute list from
a JSONL file produced from David's v7 and Goldilocks spreadsheets.

It is intentionally file-backed (not DB-backed) for now, so that the
Feature Navigator GUI can browse the ontology without schema churn.
"""

from __future__ import annotations

from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional


FEATURES_PATH = Path(__file__).with_name("features_canonical.jsonl")


@dataclass
class FeatureDefinition:
    key: str
    category: str
    tier: str
    label: str
    status: str = "active"
    type: str = "continuous"  # binary | ordinal | categorical | continuous
    group: Optional[str] = None
    description: Optional[str] = None
    cfa_relevance: Optional[str] = None
    source: Optional[str] = None
    scale: Optional[Dict[str, Any]] = None
    methods: Optional[List[Dict[str, Any]]] = None

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "FeatureDefinition":
        return cls(
            key=data.get("key", ""),
            category=data.get("category", "unknown"),
            tier=data.get("tier", "L4"),
            label=data.get("label", data.get("key", "")),
            status=data.get("status", "active"),
            type=data.get("type", "continuous"),
            group=data.get("group"),
            description=data.get("description"),
            cfa_relevance=data.get("cfa_relevance"),
            source=data.get("source"),
            scale=data.get("scale"),
            methods=data.get("methods"),
        )


@lru_cache(maxsize=1)
def load_features() -> List[FeatureDefinition]:
    feats: List[FeatureDefinition] = []
    if not FEATURES_PATH.exists():
        return feats
    import json

    with FEATURES_PATH.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                continue
            feats.append(FeatureDefinition.from_dict(data))
    return feats


def list_features(
    tier: Optional[str] = None,
    category: Optional[str] = None,
    status: Optional[str] = None,
) -> List[FeatureDefinition]:
    feats = load_features()
    result: List[FeatureDefinition] = []
    for feat in feats:
        if tier and feat.tier != tier:
            continue
        if category and feat.category != category:
            continue
        if status and feat.status != status:
            continue
        result.append(feat)
    return result


def get_feature(key: str) -> Optional[FeatureDefinition]:
    for feat in load_features():
        if feat.key == key:
            return feat
    return None
----- CONTENT END -----
----- FILE PATH: backend/science/index_catalog.py
----- CONTENT START -----
"""Canonical index catalog for the v3 science pipeline."""

from __future__ import annotations
from typing import Dict, List, Literal, Optional, TypedDict


class BinInfo(TypedDict, total=False):
    field: str
    values: List[str]


class IndexInfo(TypedDict, total=False):
    label: str
    description: str
    type: Literal["float", "int", "str"]
    bins: Optional[BinInfo]
    tags: List[str]


INDEX_CATALOG: Dict[str, IndexInfo] = {
    "science.visual_richness": {
        "label": "Visual richness",
        "description": "Composite index combining color entropy, edge density, and texture variation.",
        "type": "float",
        "bins": {"field": "science.visual_richness_bin", "values": ["low", "mid", "high"]},
        "tags": ["composite", "candidate_bn_input"],
    },
    "science.organized_complexity": {
        "label": "Organized complexity",
        "description": "Composite index combining fractal dimension with organization ratio.",
        "type": "float",
        "bins": {"field": "science.organized_complexity_bin", "values": ["low", "mid", "high"]},
        "tags": ["composite", "candidate_bn_input"],
    },
}


def get_candidate_bn_keys() -> List[str]:
    return [k for k, info in INDEX_CATALOG.items() if "candidate_bn_input" in info.get("tags", [])]


def get_index_metadata() -> Dict[str, IndexInfo]:
    return INDEX_CATALOG
----- CONTENT END -----
----- FILE PATH: backend/science/perception.py
----- CONTENT START -----
"""
High-Level AI Perception Logic.
Handles VLM prompts and Semantic Analysis.
"""
import os
import base64
from io import BytesIO
from PIL import Image
from backend.science.core import AnalysisFrame

class PerceptionProcessor:
    
    def __init__(self):
        # Check for keys but don't crash if missing (allow partial functionality)
        self.has_openai = "OPENAI_API_KEY" in os.environ
        self.has_anthropic = "ANTHROPIC_API_KEY" in os.environ

    async def analyze_aesthetics(self, frame: AnalysisFrame):
        """
        Uses VLM to extract abstract qualities (Modernity, Coziness).
        """
        if not self.has_openai:
            # Fallback for dev/test without cost
            frame.add_attribute('style.modernity', 0.5, confidence=0.0)
            return

        # Convert to base64
        pil_img = Image.fromarray(frame.original_image)
        buff = BytesIO()
        pil_img.save(buff, format="JPEG")
        b64_img = base64.b64encode(buff.getvalue()).decode('utf-8')
        
        # Mock VLM Call - In production, insert actual OpenAI/Claude call here
        # We stub the network call to keep this file runnable immediately
        # result = await call_gpt4v(b64_img, "Rate modernity 0-1")
        
        # Simulated result
        frame.add_attribute('style.modernity', 0.85, confidence=0.9)
        frame.add_attribute('psych.coziness', 0.42, confidence=0.8)----- CONTENT END -----
----- FILE PATH: backend/science/pipeline.py
----- CONTENT START -----
"""
Science Pipeline Orchestrator v3.3 (Grand Jury Edition).
Integrates DepthAnalyzer and removed IsovistAnalyzer.
"""
import logging
from typing import Optional
import numpy as np
from sqlalchemy.orm import Session

from backend.database.core import SessionLocal

try:
    import cv2
except ImportError:
    cv2 = None

from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.core import AnalysisFrame

# v3.3 Modular Imports
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.math.symmetry import SymmetryAnalyzer
from backend.science.math.naturalness import NaturalnessAnalyzer
from backend.science.math.fluency import FluencyAnalyzer
from backend.science.spatial.depth import DepthAnalyzer # Replaces Isovist
from backend.science.context.cognitive import CognitiveStateAnalyzer
from backend.science.semantics.semantic_tags_vlm import SemanticTagAnalyzer

logger = logging.getLogger(__name__)

class SciencePipelineConfig:
    def __init__(self, enable_all: bool = True):
        self.enable_color = enable_all
        self.enable_complexity = enable_all
        self.enable_texture = enable_all
        self.enable_fractals = enable_all
        self.enable_spatial = enable_all
        # Expensive L2 analyzers are explicit opt-ins.
        self.enable_cognitive = False  # Cognitive VLM (Kaplan-style dimensions)
        self.enable_semantic = False   # Semantic VLM (style.*, room_function.*)

class SciencePipeline:
    def __init__(self, db: Optional[Session] = None, session: Optional[Session] = None, config: Optional[SciencePipelineConfig] = None):
        self.db = db or session or SessionLocal()
        self._owns_session = (db is None and session is None)
        self.config = config or SciencePipelineConfig()
        
        # Init Analyzers
        self.color = ColorAnalyzer()
        self.complexity = ComplexityAnalyzer()
        self.texture = TextureAnalyzer()
        self.fractals = FractalAnalyzer()
        self.symmetry = SymmetryAnalyzer()
        self.naturalness = NaturalnessAnalyzer()
        self.fluency = FluencyAnalyzer()
        self.spatial = DepthAnalyzer() # The new Spatial Engine
        self.cognitive = CognitiveStateAnalyzer()
        self.semantic = SemanticTagAnalyzer()

    def process_image(self, image_id: int) -> bool:
        image_record = self.db.query(Image).get(image_id)
        if not image_record:
            logger.warning(f"Image {image_id} not found.")
            return False

        # Load Image
        rgb = self._load_image(image_record)
        if rgb is None: return False

        # Init Frame
        frame = AnalysisFrame(image_id=image_id, original_image=rgb)

        try:
            # L0: Physics & Basic Stats
            if self.config.enable_color: self.color.analyze(frame)
            if self.config.enable_complexity: self.complexity.analyze(frame)
            if self.config.enable_texture: self.texture.analyze(frame)
            if self.config.enable_fractals: self.fractals.analyze(frame)
            if self.config.enable_spatial: 
                self.symmetry.analyze(frame)
                self.naturalness.analyze(frame)
                self.spatial.analyze(frame) # Runs Depth/Clutter

            # L1: Perceptual (Dependent on L0)
            if self.config.enable_spatial:
                self.fluency.analyze(frame)

            # L2: Cognitive (VLM)
            if self.config.enable_cognitive:
                self.cognitive.analyze(frame)

        except Exception:
            logger.exception(f"Analysis failed for image {image_id}")
            return False

        self._save_results(image_id, frame.attributes)
        return True

    def _load_image(self, image_record: Image) -> Optional[np.ndarray]:
        # Simple local loader
        import os
        path = f"data_store/{image_record.storage_path}"
        if not os.path.exists(path): return None
        bgr = cv2.imread(path)
        if bgr is None: return None
        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

    def _save_results(self, image_id: int, attributes: dict) -> None:
        for key, value in attributes.items():
            if value != value: value = 0.0 # NaN check
            val = Validation(
                image_id=image_id,
                attribute_key=key,
                value=float(value),
                source="science_pipeline_v3.3"
            )
            self.db.add(val)
        self.db.commit()----- CONTENT END -----
----- FILE PATH: backend/science/summary.py
----- CONTENT START -----
"""
High-level science composites for Image Tagger v3.3.

This module defines ScienceSummaryAnalyzer, which computes:

  * science.visual_richness        (0.0‚Äì1.0)
  * science.organized_complexity   (0.0‚Äì1.0)
  * science.visual_richness_bin    (0=low, 1=mid, 2=high)
  * science.organized_complexity_bin (0=low, 1=mid, 2=high)

It is designed to sit on top of the lower-level analyzers in
backend.science.math and backend.science.spatial.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Sequence, Dict, Any, Optional

import math

from backend.science.core import AnalysisFrame


COLOR_KEYS = []
COMPLEXITY_KEYS = []
TEXTURE_KEYS = ['texture.macro.homogeneity', 'texture.micro.contrast', 'texture.micro.homogeneity', 'texture.macro.contrast']
FRACTAL_KEYS = ['fractal.D']


def _safe_avg(frame: AnalysisFrame, keys: Sequence[str]) -> Optional[float]:
    values = [frame.attributes[k] for k in keys if k in frame.attributes]
    if not values:
        return None
    return float(sum(values) / len(values))


def _clamp01(x: float) -> float:
    if math.isnan(x):
        return 0.0
    if x < 0.0:
        return 0.0
    if x > 1.0:
        return 1.0
    return x


def _to_bin(x: float) -> int:
    if x < 0.33:
        return 0
    if x < 0.66:
        return 1
    return 2


@dataclass
class ScienceSummaryAnalyzer:
    """
    Lightweight composite index builder.

    The goal is not to be "the final word" on the science, but to
    provide stable, BN-friendly scalars and discrete bins.
    """

    def analyze(self, frame: AnalysisFrame) -> None:
        # 1. Visual richness: color + texture + a complexity touch.
        color   = _safe_avg(frame, COLOR_KEYS)
        texture = _safe_avg(frame, TEXTURE_KEYS)
        comp    = _safe_avg(frame, COMPLEXITY_KEYS)

        components = [v for v in (color, texture, comp) if v is not None]
        if components:
            raw_vr = sum(components) / len(components)
            vr = _clamp01(raw_vr)
            frame.add_attribute("science.visual_richness", vr, confidence=1.0)
            frame.add_attribute("science.visual_richness_bin", float(_to_bin(vr)), confidence=1.0)

        # 2. Organized complexity: complexity + fractals (if any).
        comp2 = _safe_avg(frame, COMPLEXITY_KEYS)
        frac  = _safe_avg(frame, FRACTAL_KEYS)

        components2 = [v for v in (comp2, frac) if v is not None]
        if components2:
            raw_oc = sum(components2) / len(components2)
            oc = _clamp01(raw_oc)
            frame.add_attribute("science.organized_complexity", oc, confidence=1.0)
            frame.add_attribute("science.organized_complexity_bin", float(_to_bin(oc)), confidence=1.0)
----- CONTENT END -----
----- FILE PATH: backend/science/vision.py
----- CONTENT START -----
"""
Low-Level Computer Vision Logic.
Consolidates Color, Texture, and Geometry extractors into a single optimized module.
"""
import numpy as np
import cv2
from backend.science.core import AnalysisFrame

class VisionProcessor:
    
    @staticmethod
    def extract_color_features(frame: AnalysisFrame):
        """Extracts Luminance, Temperature, and Saturation."""
        img = frame.original_image
        
        # 1. Mean Luminance
        mean_lum = np.mean(frame.gray_image) / 255.0
        frame.add_attribute('color.luminance', mean_lum)
        
        # 2. Saturation (HSV)
        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        sat = np.mean(hsv[:, :, 1]) / 255.0
        frame.add_attribute('color.saturation', sat)
        
        # 3. Warm/Cool Ratio
        # Hue: 0-60 (Warm), 90-150 (Cool) in OpenCV
        hue = hsv[:, :, 0]
        warm_pixels = np.sum((hue >= 0) & (hue <= 60))
        cool_pixels = np.sum((hue >= 90) & (hue <= 150))
        total = warm_pixels + cool_pixels + 1e-6
        frame.add_attribute('color.warmth', warm_pixels / total)

    @staticmethod
    def extract_geometry_features(frame: AnalysisFrame):
        """Extracts Complexity, Edges, and Symmetry."""
        gray = frame.gray_image
        h, w = gray.shape
        
        # 1. Edge Density (Complexity Proxy)
        edge_pixels = np.sum(frame.edges > 0)
        density = edge_pixels / (h * w)
        frame.add_attribute('complexity.edge_density', min(density * 5, 1.0)) # Scale 0-1
        
        # 2. Symmetry (Horizontal Flip correlation)
        left = gray[:, :w//2]
        right = gray[:, w//2:]
        # Handle odd widths
        min_w = min(left.shape[1], right.shape[1])
        
        # Flip right side
        right_flipped = np.fliplr(right[:, :min_w])
        left_trimmed = left[:, -min_w:]
        
        # Simple correlation
        corr = np.corrcoef(left_trimmed.flatten(), right_flipped.flatten())[0, 1]
        symmetry_score = max(0, (corr + 1) / 2) # Normalize -1..1 to 0..1
        frame.add_attribute('fluency.symmetry', symmetry_score)

    @staticmethod
    def run_all(frame: AnalysisFrame):
        VisionProcessor.extract_color_features(frame)
        VisionProcessor.extract_geometry_features(frame)----- CONTENT END -----
----- FILE PATH: backend/science/context/cognitive.py
----- CONTENT START -----

import logging
from typing import Dict, Any

import cv2
import numpy as np

from backend.science.core import AnalysisFrame
from backend.services.vlm import get_vlm_engine, StubEngine, get_cognitive_prompt, describe_vlm_configuration
from backend.services.costs import log_vlm_usage

logger = logging.getLogger(__name__)


class CognitiveStateAnalyzer:
    """Cognitive / affective analyzer driven by a Visual Language Model.

    The analyzer asks a VLM to rate the scene on:
    - Five core environmental-psychology dimensions (Kaplan & Kaplan + restoration)
    - Five affective / experiential dimensions (cozy, welcoming, tranquil, scary, jarring)
    """

    PROMPT = (
        "Analyze this architectural space as an environmental psychologist.\n\n"
        "Rate the following attributes from 0.0 (very low) to 1.0 (very high):\n\n"
        "1. coherence   ‚Äì How organized and structured is the scene?\n"
        "2. complexity  ‚Äì How much visual richness and variety is present?\n"
        "3. legibility  ‚Äì How easy would it be to navigate and understand this space?\n"
        "4. mystery     ‚Äì Does the environment promise more information if explored?\n"
        "5. restoration ‚Äì Potential for stress recovery / mental restoration.\n\n"
        "Now also rate the affective tone of the space on these dimensions (0.0‚Äì1.0):\n"
        "6. cozy        ‚Äì How cozy / snug / intimate does it feel?\n"
        "7. welcoming   ‚Äì How welcoming / socially inviting does it feel?\n"
        "8. tranquil    ‚Äì How calm / tranquil does it feel?\n"
        "9. scary       ‚Äì How scary / threatening does it feel?\n"
        "10. jarring    ‚Äì How visually or affectively jarring does it feel?\n\n"
        "Return ONLY valid JSON in the following form:\n"
        "{"
        "\"coherence\": float, "
        "\"complexity\": float, "
        "\"legibility\": float, "
        "\"mystery\": float, "
        "\"restoration\": float, "
        "\"cozy\": float, "
        "\"welcoming\": float, "
        "\"tranquil\": float, "
        "\"scary\": float, "
        "\"jarring\": float"
        "}\n"
    )

    def analyze(self, frame: AnalysisFrame) -> None:
        """Run VLM-based cognitive + affective analysis on the given frame.

        This is synchronous by design: the surrounding science pipeline is
        synchronous and this method may perform a blocking network call when
        a real VLM is configured, or return fast when using StubEngine.
        """
        try:
            if frame.original_image is None:
                logger.warning("CognitiveStateAnalyzer: frame has no original_image; skipping.")
                return

            # The pipeline already keeps images in RGB (np.ndarray).
            img = frame.original_image
            if not isinstance(img, np.ndarray):
                logger.warning("CognitiveStateAnalyzer: unsupported image type %r", type(img))
                return

            ok, buffer = cv2.imencode(".jpg", img)
            if not ok:
                logger.error("CognitiveStateAnalyzer: JPEG encoding failed; skipping.")
                return
            image_bytes = buffer.tobytes()

            engine = get_vlm_engine()
            prompt = get_cognitive_prompt(self.PROMPT)
            result: Dict[str, Any] = engine.analyze_image(image_bytes, prompt)

            # Stub / classroom path
            if isinstance(engine, StubEngine) or result.get("stub"):
                logger.info(
                    "CognitiveStateAnalyzer: running in STUB mode; "
                    "skipping cognitive/affect writes to avoid contaminating data."
                )
                # We deliberately do not write any cognitive.* or affect.* attributes here.
                # Downstream BN exports can treat the absence of these attributes as
                # "no VLM data available" instead of neutral 0.5 placeholders.
                return

            # Real data path: record cost for a single cognitive VLM call.
            try:
                cfg = describe_vlm_configuration()
                provider = cfg.get("provider", "auto")
                model_name = cfg.get("engine", type(engine).__name__)
                cost_per_1k = cfg.get("cost_per_1k_images_usd") or 0.0
                estimated_cost = float(cost_per_1k) / 1000.0 if cost_per_1k else 0.0
                log_vlm_usage(
                    provider=str(provider),
                    model_name=str(model_name),
                    cost_usd=estimated_cost,
                    meta={
                        "source": "science_pipeline_cognitive",
                        "image_id": getattr(frame, "image_id", None),
                        "cost_per_1k_images_usd": cost_per_1k,
                    },
                )
            except Exception:
                # Cost logging must never break the science pipeline.
                logger.debug("CognitiveStateAnalyzer: cost logging failed", exc_info=True)

            # Real data path
            def _clamp(x: float) -> float:
                try:
                    v = float(x)
                except Exception:
                    return 0.5
                if v < 0.0:
                    return 0.0
                if v > 1.0:
                    return 1.0
                return v

            # Core cognitive dimensions
            for key in ["coherence", "complexity", "legibility", "mystery", "restoration"]:
                if key in result:
                    frame.add_attribute(f"cognitive.{key}", _clamp(result[key]), confidence=0.9)

            # Affective tone dimensions
            for key in ["cozy", "welcoming", "tranquil", "scary", "jarring"]:
                if key in result:
                    frame.add_attribute(f"affect.{key}", _clamp(result[key]), confidence=0.9)

        except Exception as exc:
            logger.error("CognitiveStateAnalyzer failed: %s", exc)
            # We deliberately do not raise; the rest of the pipeline can still succeed.
            frame.add_attribute("cognitive.error", 1.0)----- CONTENT END -----
----- FILE PATH: backend/science/context/social.py
----- CONTENT START -----
from backend.science.core import AnalysisFrame

class SocialDispositionAnalyzer:
    """
    Generates VLM Prompts specifically for Architectural Psychology.
    """
    
    PROMPT_TEMPLATE = """
    Analyze this architectural interior as an environmental psychologist.
    Assess the following social affordances on a scale of 0.0 to 1.0:
    
    1. Sociopetal (Encourages interaction): {sociopetal_score}
    2. Sociofugal (Discourages interaction): {sociofugal_score}
    3. Privacy (Visual/Acoustic isolation): {privacy_score}
    4. Hierarchy (Clear distinction of status): {hierarchy_score}
    
    Output strictly in JSON format: { "sociopetal": 0.X, "sociofugal": 0.X, "privacy": 0.X, "hierarchy": 0.X }
    """

    @staticmethod
    async def analyze(frame: AnalysisFrame, perception_engine):
        """
        Uses the shared PerceptionProcessor (VLM) to run this specific study.
        """
        # In a real system, 'perception_engine' is the VLM client wrapper
        # result = await perception_engine.ask(frame.original_image, PROMPT_TEMPLATE)
        
        # MOCK RESULT (until VLM is live)
        frame.add_attribute("social.sociopetal", 0.75, confidence=0.8)
        frame.add_attribute("social.privacy", 0.30, confidence=0.8)----- CONTENT END -----
----- FILE PATH: backend/science/math/color.py
----- CONTENT START -----
"""
Perceptual Color Analysis Module.
Moves away from RGB statistics to CIELAB perceptual space for scientific validity.
"""
import numpy as np
from scipy.spatial import ConvexHull
from backend.science.core import AnalysisFrame

class ColorAnalyzer:
    """
    Extracts color metrics using the CIELAB colorspace, which aligns 
    with human visual perception (unlike RGB/HSV).
    """

    @staticmethod
    def analyze(frame: AnalysisFrame) -> None:
        # frame.lab_image is shape (H, W, 3) -> L, a, b
        lab = frame.lab_image
        l_channel = lab[:, :, 0] # Lightness (0-100)
        a_channel = lab[:, :, 1] # Green-Red
        b_channel = lab[:, :, 2] # Blue-Yellow

        # 1. Perceptual Lightness (Mean L*)
        # Scaled to 0-1 for database consistency
        mean_lightness = np.mean(l_channel) / 100.0
        frame.add_attribute("color.perceptual_lightness", mean_lightness)

        # 2. Color Volume (Richness)
        # Calculates the volume of the Convex Hull of the pixel distribution in ab space.
        # Higher volume = wider variety of distinct hues/saturations.
        try:
            # Downsample for performance (hull calculation is O(N log N))
            ab_pixels = lab[:, :, 1:].reshape(-1, 2)
            # Take a random sample of 1000 pixels to estimate volume
            if ab_pixels.shape[0] > 1000:
                indices = np.random.choice(ab_pixels.shape[0], 1000, replace=False)
                sample = ab_pixels[indices]
            else:
                sample = ab_pixels
            
            if len(sample) > 3:
                hull = ConvexHull(sample)
                # Normalize volume roughly (max theoretical area in ab plane is large)
                # A very colorful image might have vol ~3000-5000. We log-scale it.
                vol_score = np.log1p(hull.volume) / 10.0 
                frame.add_attribute("color.lab_volume", min(vol_score, 1.0))
            else:
                frame.add_attribute("color.lab_volume", 0.0)
        except Exception:
            frame.add_attribute("color.lab_volume", 0.0)

        # 3. Warm/Cool Balance (A-channel dominance)
        # Positive 'a' is Red/Magenta (Warm), Negative 'a' is Green (Cool)
        # Positive 'b' is Yellow (Warm), Negative 'b' is Blue (Cool)
        # We use a simple integration of the 'a' and 'b' channels.
        
        warm_mask = (a_channel > 0) | (b_channel > 0)
        warm_ratio = np.mean(warm_mask)
        frame.add_attribute("color.warmth_ratio", warm_ratio)

        # 4. Contrast (Lightness Standard Deviation)
        l_std = np.std(l_channel) / 50.0 # Normalize roughly
        frame.add_attribute("color.lightness_contrast", min(l_std, 1.0))----- CONTENT END -----
----- FILE PATH: backend/science/math/complexity.py
----- CONTENT START -----
import numpy as np
import cv2
from scipy.stats import entropy
from skimage.feature import graycomatrix
from backend.science.core import AnalysisFrame

class ComplexityAnalyzer:
    """
    Quantifies 'Visual Complexity' using both Information Theory (Entropy)
    and Structural Analysis (Spatial Entropy).
    """

    @staticmethod
    def calculate_shannon_entropy(image_gray: np.ndarray) -> float:
        """
        Global histogram entropy. Measures 'amount of information' but 
        ignores spatial arrangement (snow vs checkerboard).
        """
        hist = cv2.calcHist([image_gray], [0], None, [256], [0, 256])
        hist = hist.ravel() / hist.sum()
        return entropy(hist, base=2)

    @staticmethod
    def calculate_spatial_entropy(image_gray: np.ndarray) -> float:
        """
        Measures entropy of the GLCM. This captures 'spatial disorder'.
        A checkerboard has Low Spatial Entropy (high order).
        White noise has High Spatial Entropy (low order).
        """
        # Downscale for speed if needed, GLCM is expensive
        h, w = image_gray.shape
        if h > 512:
            scale = 512 / h
            small = cv2.resize(image_gray, (0,0), fx=scale, fy=scale)
        else:
            small = image_gray
            
        # Quantize to 32 levels to stabilize GLCM
        small_quant = (small // 8).astype(np.uint8)
        
        glcm = graycomatrix(small_quant, distances=[1], angles=[0, np.pi/4, np.pi/2], 
                            levels=32, symmetric=True, normed=True)
        
        # Compute entropy of the non-zero GLCM elements
        glcm_flat = glcm.flatten()
        glcm_flat = glcm_flat[glcm_flat > 0]
        spatial_ent = -np.sum(glcm_flat * np.log2(glcm_flat))
        
        # Normalize (Max entropy for 32x32 matrix is log2(32*32) = 10)
        return min(spatial_ent / 10.0, 1.0)

    @staticmethod
    def analyze(frame: AnalysisFrame):
        # 1. Global Entropy (Abundance of gray levels)
        ent = ComplexityAnalyzer.calculate_shannon_entropy(frame.gray_image)
        # Normalize roughly (8 bits = max 8)
        frame.add_attribute("complexity.shannon_entropy", min(ent / 8.0, 1.0))
        
        # 2. Spatial Entropy (Disorder of texture)
        spatial = ComplexityAnalyzer.calculate_spatial_entropy(frame.gray_image)
        frame.add_attribute("complexity.spatial_entropy", spatial)
        
        # 3. Edge Density (Clutter Proxy)
        # Simple ratio of Canny pixels to total area
        total_pixels = frame.edges.size
        edge_pixels = np.count_nonzero(frame.edges)
        frame.add_attribute("complexity.edge_density", edge_pixels / total_pixels)----- CONTENT END -----
----- FILE PATH: backend/science/math/fluency.py
----- CONTENT START -----
"""backend.science.math.fluency

Primary function:
  Compute a lightweight perceptual-fluency proxy.

Inputs:
  frame: AnalysisFrame with complexity + texture results already in metrics.

Outputs (stored into frame.metrics):
  fluency.score : float in [0,1], higher = easier to perceptually parse.

Notes:
  This combines low edge density, moderate entropy, and texture regularity.
  It is a heuristic starting point suitable for early experiments and teaching.
"""

from __future__ import annotations

import numpy as np


class FluencyAnalyzer:
    def analyze(self, frame) -> None:
        m = getattr(frame, "metrics", {}) or {}
        edge_density = float(m.get("complexity.edge_density", np.nan))
        entropy = float(m.get("complexity.shannon_entropy", np.nan))
        glcm_contrast = float(m.get("texture.glcm_contrast_mean", np.nan))

        # Normalize inputs to rough [0,1] scales with conservative bounds.
        # Missing values propagate to nan then to fallback neutral score.
        def clamp01(x):
            return float(np.clip(x, 0.0, 1.0))

        ed_n = clamp01(edge_density / 0.25) if np.isfinite(edge_density) else np.nan  # 0.25 is "busy"
        ent_n = clamp01(entropy / 8.0) if np.isfinite(entropy) else np.nan           # entropy ~0..8 for 256 bins
        con_n = clamp01(glcm_contrast / 10.0) if np.isfinite(glcm_contrast) else np.nan

        # Fluency rises when edges and contrast are low/moderate and entropy is not extreme.
        components = []
        if np.isfinite(ed_n):
            components.append(1.0 - ed_n)
        if np.isfinite(ent_n):
            components.append(1.0 - abs(ent_n - 0.5) * 2.0)  # best around mid-entropy
        if np.isfinite(con_n):
            components.append(1.0 - con_n)

        if not components:
            score = 0.5
        else:
            score = float(np.clip(np.mean(components), 0.0, 1.0))

        frame.metrics["fluency.score"] = score
----- CONTENT END -----
----- FILE PATH: backend/science/math/fractals.py
----- CONTENT START -----
import numpy as np
import cv2
from backend.science.core import AnalysisFrame

class FractalAnalyzer:
    """
    Implements Box Counting Method for Fractal Dimension (D).
    Standard architecture metric for 'visual richness'.
    """

    @staticmethod
    def analyze(frame: AnalysisFrame):
        # Use the pre-computed edges from the AnalysisFrame
        # This ensures we measure the D of the *structure*, not the noise
        d_score = FractalAnalyzer.box_counting(frame.edges)
        
        # Fractal D usually ranges 1.0 (Line) to 2.0 (Plane).
        # We normalize 1.0 -> 2.0 to 0.0 -> 1.0 for the DB
        norm_d = max(0.0, min((d_score - 1.0), 1.0))
        frame.add_attribute("fractal.D", norm_d)

    @staticmethod
    def box_counting(Z: np.ndarray) -> float:
        """
        Minkowski-Bouligand dimension.
        Z: Binary array (edges).
        """
        if np.sum(Z) == 0:
            return 0.0

        # Only check up to min dimension / 2
        p = min(Z.shape)
        n = int(np.floor(np.log(p)/np.log(2)))
        sizes = 2**np.arange(n, 1, -1)
        
        counts = []
        for size in sizes:
            # Fast box counting using add.reduceat
            count = FractalAnalyzer._fast_box_count(Z, size)
            counts.append(count)

        # Linear Regression on log-log scale
        # Fit: log(N) = D * log(1/s) + c
        if len(counts) < 2: 
            return 0.0
            
        coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)
        return -coeffs[0] # The slope is -D

    @staticmethod
    def _fast_box_count(Z, k):
        S = np.add.reduceat(
            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),
                               np.arange(0, Z.shape[1], k), axis=1)
        return len(np.where((S > 0) & (S < k*k))[0])----- CONTENT END -----
----- FILE PATH: backend/science/math/glcm.py
----- CONTENT START -----
import numpy as np
from skimage.feature import graycomatrix, graycoprops
from backend.science.core import AnalysisFrame

class TextureAnalyzer:
    """
    GLCM Texture Analysis.
    Updated to expose multi-scale texture detection (near vs far).
    """
    
    @staticmethod
    def analyze(frame: AnalysisFrame):
        gray = frame.gray_image
        # Downsample for performance
        h, w = gray.shape
        if h > 512:
            scale = 512 / h
            gray = cv2.resize(gray, (0,0), fx=scale, fy=scale)

        # Quantize to 64 levels
        gray = (gray // 4).astype(np.uint8)

        # Analyze at two distances: 1 (Micro-texture) and 5 (Macro-structure)
        distances = [1, 5]
        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]
        
        glcm = graycomatrix(gray, distances=distances, angles=angles, 
                            levels=64, symmetric=True, normed=True)
        
        # Extract features and average across angles
        # property array shape: (num_distances, num_angles)
        contrast = graycoprops(glcm, 'contrast')
        energy = graycoprops(glcm, 'energy')
        homogeneity = graycoprops(glcm, 'homogeneity')

        # Micro-Texture (Distance 1)
        frame.add_attribute("texture.micro.contrast", np.mean(contrast[0]))
        frame.add_attribute("texture.micro.homogeneity", np.mean(homogeneity[0]))

        # Macro-Texture (Distance 5)
        frame.add_attribute("texture.macro.contrast", np.mean(contrast[1]))
        frame.add_attribute("texture.macro.homogeneity", np.mean(homogeneity[1]))----- CONTENT END -----
----- FILE PATH: backend/science/math/naturalness.py
----- CONTENT START -----
"""backend.science.math.naturalness

Primary function:
  Compute heuristic 'naturalness' proxies from color statistics.

Inputs:
  frame: AnalysisFrame with rgb_image or lab_image.

Outputs (stored into frame.metrics):
  naturalness.green_ratio   : fraction of pixels in green-hue band.
  naturalness.blue_ratio    : fraction of pixels in blue-hue band.
  naturalness.earth_ratio   : fraction of pixels in low-sat warm band.
  naturalness.score         : float in [0,1] combining the above.

Notes:
  This is intentionally lightweight. It is *not* a semantic detector of nature,
  but a perceptual proxy capturing biophilic chromatic signatures.
"""

from __future__ import annotations

import numpy as np

try:
    import cv2  # type: ignore
except Exception:  # pragma: no cover
    cv2 = None  # type: ignore


class NaturalnessAnalyzer:
    def analyze(self, frame) -> None:
        rgb = getattr(frame, "rgb_image", None)
        if rgb is None:
            return

        img = np.asarray(rgb)
        if img.ndim != 3 or img.shape[2] != 3:
            return

        # Convert to HSV for hue bands if cv2 is available; else approximate.
        if cv2 is not None:
            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
            h = hsv[..., 0].astype(np.float32) * 2.0  # 0..360
            s = hsv[..., 1].astype(np.float32) / 255.0
            v = hsv[..., 2].astype(np.float32) / 255.0
        else:
            # crude fallback using normalized RGB
            r, g, b = img[..., 0].astype(np.float32), img[..., 1].astype(np.float32), img[..., 2].astype(np.float32)
            denom = (r + g + b + 1e-6)
            r_n, g_n, b_n = r / denom, g / denom, b / denom
            h = np.zeros_like(r_n)
            h[g_n > r_n] = 120.0
            h[b_n > g_n] = 240.0
            s = 1.0 - np.minimum.reduce([r_n, g_n, b_n])
            v = denom / denom.max()

        # Green band: 70‚Äì170 deg, adequate saturation/brightness
        green = (h >= 70) & (h <= 170) & (s >= 0.15) & (v >= 0.15)
        # Blue band: 190‚Äì260 deg
        blue = (h >= 190) & (h <= 260) & (s >= 0.12) & (v >= 0.12)
        # Earth / warm low-sat band: 15‚Äì60 deg with low saturation, medium brightness
        earth = (h >= 15) & (h <= 60) & (s <= 0.35) & (v >= 0.2)

        total = float(img.shape[0] * img.shape[1])
        green_ratio = float(green.sum() / total)
        blue_ratio = float(blue.sum() / total)
        earth_ratio = float(earth.sum() / total)

        # Simple convex combination: greens count most, then blues, then earth tones.
        score = float(np.clip(0.55 * green_ratio + 0.30 * blue_ratio + 0.15 * earth_ratio, 0.0, 1.0))

        frame.metrics["naturalness.green_ratio"] = green_ratio
        frame.metrics["naturalness.blue_ratio"] = blue_ratio
        frame.metrics["naturalness.earth_ratio"] = earth_ratio
        frame.metrics["naturalness.score"] = score
----- CONTENT END -----
----- FILE PATH: backend/science/math/regional_frequency.py
----- CONTENT START -----
"""
Regional (patchwise) spatial frequency analysis.

This module slices the grayscale image into patches and computes
band-limited power per patch, then summarizes their distribution. It is
designed as a forward-looking scaffold for more advanced multiscale,
oriented frequency analysis.
"""

import numpy as np
from backend.science.core import AnalysisFrame
from backend.science.contracts import fail


class RegionalSpatialFrequencyAnalyzer:
    name = "regional_spatial_frequency"
    tier = "L0"
    requires = ["gray_image"]
    provides = [
        "spatial_freq_reg.low_mean",
        "spatial_freq_reg.low_var",
        "spatial_freq_reg.mid_mean",
        "spatial_freq_reg.mid_var",
        "spatial_freq_reg.high_mean",
        "spatial_freq_reg.high_var",
    ]

    def __init__(self, patch: int = 64, stride: int | None = None) -> None:
        self.patch = patch
        self.stride = stride or patch

    def _band_powers_fft(self, patch: np.ndarray) -> tuple[float, float, float]:
        f = np.fft.fft2(patch.astype(np.float32))
        fshift = np.fft.fftshift(f)
        ps = np.abs(fshift) ** 2
        h, w = ps.shape
        cy, cx = h // 2, w // 2
        y, x = np.indices((h, w))
        r = np.sqrt((y - cy) ** 2 + (x - cx) ** 2)
        r = r / r.max()

        low_mask = r < 0.33
        mid_mask = (r >= 0.33) & (r < 0.66)
        high_mask = r >= 0.66

        low = float(ps[low_mask].mean()) if np.any(low_mask) else 0.0
        mid = float(ps[mid_mask].mean()) if np.any(mid_mask) else 0.0
        high = float(ps[high_mask].mean()) if np.any(high_mask) else 0.0
        return low, mid, high

    def analyze(self, frame: AnalysisFrame) -> None:
        gray = frame.gray_image
        if gray is None:
            fail(frame, self.name, "missing gray_image")
            return

        H, W = gray.shape
        p, s = self.patch, self.stride
        lows, mids, highs = [], [], []

        for y in range(0, max(H - p + 1, 1), s):
            if y + p > H:
                break
            for x in range(0, max(W - p + 1, 1), s):
                if x + p > W:
                    break
                patch = gray[y : y + p, x : x + p]
                low, mid, high = self._band_powers_fft(patch)
                lows.append(low)
                mids.append(mid)
                highs.append(high)

        if not lows:
            fail(frame, self.name, "image too small for regional analysis")
            return

        lows = np.array(lows, dtype=np.float32)
        mids = np.array(mids, dtype=np.float32)
        highs = np.array(highs, dtype=np.float32)

        frame.add_attribute("spatial_freq_reg.low_mean", float(lows.mean()))
        frame.add_attribute("spatial_freq_reg.low_var", float(lows.var()))
        frame.add_attribute("spatial_freq_reg.mid_mean", float(mids.mean()))
        frame.add_attribute("spatial_freq_reg.mid_var", float(mids.var()))
        frame.add_attribute("spatial_freq_reg.high_mean", float(highs.mean()))
        frame.add_attribute("spatial_freq_reg.high_var", float(highs.var()))
----- CONTENT END -----
----- FILE PATH: backend/science/math/spatial_frequency.py
----- CONTENT START -----
"""
Global spatial frequency analysis for architectural images.

This module computes band-limited power in the Fourier domain as a
baseline for more sophisticated (regional, oriented) measures.
"""

import numpy as np
from backend.science.core import AnalysisFrame


class SpatialFrequencyAnalyzer:
    """
    Computes low/mid/high band power and a simple low/high ratio from
    the grayscale image. This is deliberately conservative and fast.
    """

    name = "spatial_frequency"
    tier = "L0"
    requires = ["gray_image"]
    provides = [
        "spatial_freq.low_power",
        "spatial_freq.mid_power",
        "spatial_freq.high_power",
        "spatial_freq.low_high_ratio",
    ]

    @staticmethod
    def _radial_power_spectrum(gray: np.ndarray) -> np.ndarray:
        # Convert to float and compute FFT
        f = np.fft.fft2(gray.astype(np.float32))
        fshift = np.fft.fftshift(f)
        ps = np.abs(fshift) ** 2

        h, w = ps.shape
        cy, cx = h // 2, w // 2
        y, x = np.indices((h, w))
        r = np.sqrt((y - cy) ** 2 + (x - cx) ** 2)
        r = r / r.max()
        return ps, r

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        gray = frame.gray_image
        if gray is None:
            from backend.science.contracts import fail
            fail(frame, cls.name, "missing gray_image")
            return

        ps, r = cls._radial_power_spectrum(gray)

        # Define crude bands: [0, 0.33), [0.33, 0.66), [0.66, 1.0]
        low_mask = r < 0.33
        mid_mask = (r >= 0.33) & (r < 0.66)
        high_mask = r >= 0.66

        low_power = float(ps[low_mask].mean()) if np.any(low_mask) else 0.0
        mid_power = float(ps[mid_mask].mean()) if np.any(mid_mask) else 0.0
        high_power = float(ps[high_mask].mean()) if np.any(high_mask) else 0.0

        denom = high_power if high_power > 1e-6 else 1e-6
        low_high_ratio = float(low_power / denom)

        frame.add_attribute("spatial_freq.low_power", low_power)
        frame.add_attribute("spatial_freq.mid_power", mid_power)
        frame.add_attribute("spatial_freq.high_power", high_power)
        frame.add_attribute("spatial_freq.low_high_ratio", low_high_ratio)
----- CONTENT END -----
----- FILE PATH: backend/science/math/symmetry.py
----- CONTENT START -----
"""backend.science.math.symmetry

Primary function:
  Compute simple bilateral symmetry proxies for an AnalysisFrame.

Inputs:
  frame: AnalysisFrame with grayscale_image (H,W) float/uint8.

Outputs (stored into frame.metrics):
  symmetry.vertical_score  : float in [0,1], higher = more left-right symmetry.
  symmetry.horizontal_score: float in [0,1], higher = more top-bottom symmetry.
  symmetry.mean_score      : float in [0,1].

Notes:
  This is a deterministic, low-cost proxy intended for teaching and first-pass science.
  It measures pixelwise correlation between an image half and its mirror.
"""

from __future__ import annotations

import numpy as np

try:
    import cv2  # type: ignore
except Exception:  # pragma: no cover
    cv2 = None  # type: ignore

class SymmetryAnalyzer:
    """Bilateral symmetry proxies."""

    def analyze(self, frame) -> None:
        gray = getattr(frame, "grayscale_image", None)
        if gray is None:
            return
        img = np.asarray(gray).astype(np.float32)
        if img.ndim != 2:
            img = img.mean(axis=-1)

        h, w = img.shape
        if h < 4 or w < 4:
            return

        # Normalize to zero-mean, unit-variance for stability.
        mu = img.mean()
        sigma = img.std() + 1e-6
        z = (img - mu) / sigma

        # Vertical symmetry (left-right)
        left = z[:, : w // 2]
        right = z[:, w - left.shape[1] :][:, ::-1]
        v_score = self._corr(left, right)

        # Horizontal symmetry (top-bottom)
        top = z[: h // 2, :]
        bottom = z[h - top.shape[0] :, :][::-1, :]
        h_score = self._corr(top, bottom)

        mean_score = float(np.nanmean([v_score, h_score]))

        frame.metrics["symmetry.vertical_score"] = float(v_score)
        frame.metrics["symmetry.horizontal_score"] = float(h_score)
        frame.metrics["symmetry.mean_score"] = mean_score

    @staticmethod
    def _corr(a: np.ndarray, b: np.ndarray) -> float:
        a_f = a.reshape(-1)
        b_f = b.reshape(-1)
        if a_f.size == 0 or b_f.size == 0:
            return 0.0
        denom = (np.linalg.norm(a_f) * np.linalg.norm(b_f)) + 1e-6
        if denom == 0:
            return 0.0
        c = float(np.dot(a_f, b_f) / denom)
        # map from [-1,1] to [0,1]
        return max(0.0, min(1.0, (c + 1.0) / 2.0))
----- CONTENT END -----
----- FILE PATH: backend/science/semantics/arch_parts_vlm.py
----- CONTENT START -----
"""
VLM-assisted architectural parts detection.

This analyzer is intentionally conservative: it provides *candidates*
for architectural parts with confidences and evidence strings, to be
confirmed or corrected by human annotators in the Workbench.
"""

from __future__ import annotations

from typing import Any, Dict, List

from backend.science.core import AnalysisFrame
from backend.science.contracts import fail
from backend.science.semantics.ontology import ARCH_PARTS

# NOTE: We keep this as a stubbed integration point because the VLM
# service wiring can vary by deployment.
# STUB: integrate with backend.services.vlm.get_vlm_engine when ready.


class ArchPartsVLMAnalyzer:
    name = "arch_parts_vlm"
    tier = "L3"
    requires = ["image_url"]
    provides = ["arch.parts.candidates"]

    def __init__(self, prompt_version: str = "arch_parts_v1") -> None:
        self.prompt_version = prompt_version

    def build_prompt(self) -> str:
        categories = ", ".join(sorted(ARCH_PARTS.keys()))
        return (
            "Identify architectural elements in this interior image. "
            "Use the following high-level categories: "
            f"{categories}. "
            "For each detected element, return JSON with fields "
            "{'part': <string>, 'category': <string>, 'confidence': <0-1>, 'evidence': <short text>}."
        )

    def analyze(self, frame: AnalysisFrame) -> None:
        url = frame.metadata.get("image_url") or frame.metadata.get("image.uri")
        if not url:
            fail(frame, self.name, "no image_url in frame.metadata")
            return

        prompt = self.build_prompt()

        # STUB: call into the configured VLM engine with (url, prompt)
        # For now we just record the prompt as a placeholder and avoid emitting
        # any numeric priors that could contaminate science exports.
        candidates: List[Dict[str, Any]] = []

        frame.metadata["arch.parts.candidates"] = {
            "prompt": prompt,
            "prompt_version": self.prompt_version,
            "candidates": candidates,
            "status": "stub_not_implemented",
            "note": "VLM integration not yet wired; this is a schema stub.",
        }
----- CONTENT END -----
----- FILE PATH: backend/science/semantics/arch_patterns_vlm.py
----- CONTENT START -----
"""
VLM-assisted architectural pattern detection (stubbed).

This module defines the prompt and schema for architectural patterns
(e.g., prospect/refuge, circulation), but deliberately avoids making
strong numeric commitments when no real VLM is configured.
"""

from __future__ import annotations

from typing import Any, Dict, List

from backend.science.core import AnalysisFrame
from backend.science.contracts import fail
from backend.science.semantics.ontology import ARCH_PARTS  # noqa: F401  # placeholder for future cross-linking
from backend.services.vlm import get_vlm_engine, StubEngine, describe_vlm_configuration
from backend.services.costs import log_vlm_usage
import cv2
import numpy as np


ARCH_PATTERNS: Dict[str, str] = {
    "arch.pattern.prospect_strong": "Strong outward views and long sightlines (Prospect).",
    "arch.pattern.refuge_strong": "Strong sense of enclosure/refuge via partial enclosure.",
    "arch.pattern.refuge_nook": "Local nook/alcove suitable for retreat.",
    "arch.pattern.axial_circulation_clear": "Clear axial circulation path.",
    "arch.pattern.circulation_maze_like": "Maze-like, complex circulation.",
    "arch.pattern.double_height_space": "Double-height or tall volume.",
    "arch.pattern.corner_window": "Corner window or wrapping glazing.",
    "arch.pattern.perimeter_seating": "Seating arranged along the room perimeter.",
    "arch.pattern.central_hearth": "Fireplace or central hearth as focal point.",
    "arch.pattern.gallery_edge": "Balcony or gallery overlooking lower space.",
    "arch.pattern.daylight_soft": "Soft diffuse daylight with low glare.",
    "arch.pattern.daylight_hard": "Strong direct daylight with sharp shadows.",
    "arch.pattern.skylight_dominant": "Skylights as primary daylight source.",
    "arch.pattern.threshold_emphasized": "Emphasized entry threshold or portal.",
    "arch.pattern.colonnade": "Series of columns forming a colonnade.",
    "arch.pattern.bay_window": "Projecting bay window.",
    "arch.pattern.staircase_sculptural": "Staircase acting as sculptural feature.",
    "arch.pattern.long_view_corridor": "Long view along a corridor.",
    "arch.pattern.loft_mezzanine": "Loft or mezzanine overlooking space.",
    "arch.pattern.window_seat_niche": "Window seat or deep sill niche.",
}
# Subset of patterns for which we are willing to emit numeric attributes
# in this initial VLM wiring sprint. All others remain metadata-only.
ACTIVE_PATTERN_KEYS = {
    "arch.pattern.prospect_strong",
    "arch.pattern.refuge_strong",
    "arch.pattern.daylight_soft",
    "arch.pattern.daylight_hard",
    "arch.pattern.double_height_space",
}



class ArchPatternsVLMAnalyzer:
    name = "arch_patterns_vlm"
    tier = "L4"
    requires = ["image_url"]
    provides = ["arch.patterns.candidates"]

    def __init__(self, prompt_version: str = "arch_patterns_v1") -> None:
        self.prompt_version = prompt_version

    def build_prompt(self) -> str:
        items = []
        for key, desc in sorted(ARCH_PATTERNS.items()):
            items.append(f"- {key}: {desc}")
        patterns_block = "\n".join(items)
        return (
            "You are an architectural cognition assistant. "
            "Given this interior image, estimate the presence of the following architectural patterns.\n"
            f"{patterns_block}\n"
            "Return STRICT JSON as a list of objects with fields "
            "{'key': <pattern_key>, 'present': <0-1>, 'confidence': <0-1>, 'evidence': <short text>}."
        )

    def analyze(self, frame: AnalysisFrame) -> None:
        """Run VLM-based pattern analysis on the given frame.

        This mirrors the CognitiveStateAnalyzer pattern:
        - If running with StubEngine (or stub result), we only record metadata.
        - If running with a real VLM, we emit numeric attributes for a small,
          carefully chosen subset of patterns (ACTIVE_PATTERN_KEYS) and
          keep the full candidate list in metadata.
        """
        # We prefer to work from the in-memory RGB image, not the URL.
        img = frame.original_image
        if img is None:
            fail(frame, self.name, "no original_image available for VLM encoding")
            return

        if not isinstance(img, np.ndarray):
            fail(frame, self.name, f"unsupported image type for VLM: {type(img)}")
            return

        ok, buffer = cv2.imencode(".jpg", img)
        if not ok:
            fail(frame, self.name, "JPEG encoding failed for VLM.")
            return
        image_bytes = buffer.tobytes()

        prompt = self.build_prompt()
        engine = get_vlm_engine()
        try:
            result: Any = engine.analyze_image(image_bytes, prompt)
        except Exception as exc:  # pragma: no cover - network
            fail(frame, self.name, f"VLM error: {exc}")
            return

        # Stub / classroom path: record metadata only, no numeric priors.
        if isinstance(engine, StubEngine) or (isinstance(result, dict) and result.get("stub")):
            frame.metadata["arch.patterns.candidates"] = {
                "prompt": prompt,
                "prompt_version": self.prompt_version,
                "candidates": [],
                "engine": type(engine).__name__,
                "note": "VLM running in stub mode; no numeric arch.pattern.* attributes emitted.",
            }
            return
        # Real-data path: record a single cost entry for this VLM call.
        try:
            cfg = describe_vlm_configuration()
            provider = cfg.get("provider", "auto")
            model_name = cfg.get("engine", type(engine).__name__)
            cost_per_1k = cfg.get("cost_per_1k_images_usd") or 0.0
            estimated_cost = float(cost_per_1k) / 1000.0 if cost_per_1k else 0.0
            log_vlm_usage(
                provider=str(provider),
                model_name=str(model_name),
                cost_usd=estimated_cost,
                meta={
                    "source": "science_pipeline_arch_patterns",
                    "image_id": getattr(frame, "image_id", None),
                    "cost_per_1k_images_usd": cost_per_1k,
                },
            )
        except Exception:
            # Cost logging must never break the analysis.
            pass


        # Normalize result into a list of candidate dicts.
        candidates: List[Dict[str, Any]]
        if isinstance(result, list):
            candidates = result
        elif isinstance(result, dict) and "patterns" in result:
            patterns_val = result.get("patterns")
            if isinstance(patterns_val, list):
                candidates = patterns_val
            else:
                candidates = []
        else:
            # Unexpected shape; record and bail without emitting numerics.
            frame.metadata["arch.patterns.candidates"] = {
                "prompt": prompt,
                "prompt_version": self.prompt_version,
                "raw_result": result,
                "engine": type(engine).__name__,
                "note": "VLM returned unexpected JSON shape; no numeric arch.pattern.* attributes emitted.",
            }
            return

        def _clamp(x: Any) -> float:
            try:
                v = float(x)
            except Exception:
                return 0.0
            if v < 0.0:
                return 0.0
            if v > 1.0:
                return 1.0
            return v

        # Emit numeric attributes only for a small, high-value subset of patterns.
        for cand in candidates:
            if not isinstance(cand, dict):
                continue
            key = cand.get("key")
            if not isinstance(key, str):
                continue
            if key not in ACTIVE_PATTERN_KEYS:
                continue

            present = _clamp(cand.get("present", cand.get("value", 0.0)))
            confidence = _clamp(cand.get("confidence", 0.7))
            evidence = cand.get("evidence") or cand.get("reason") or ""

            # Use explicit string keys so our governance tests can see coverage.
            if key == "arch.pattern.prospect_strong":
                frame.add_attribute("arch.pattern.prospect_strong", present, confidence=confidence)
                attr_key = "arch.pattern.prospect_strong"
            elif key == "arch.pattern.refuge_strong":
                frame.add_attribute("arch.pattern.refuge_strong", present, confidence=confidence)
                attr_key = "arch.pattern.refuge_strong"
            elif key == "arch.pattern.daylight_soft":
                frame.add_attribute("arch.pattern.daylight_soft", present, confidence=confidence)
                attr_key = "arch.pattern.daylight_soft"
            elif key == "arch.pattern.daylight_hard":
                frame.add_attribute("arch.pattern.daylight_hard", present, confidence=confidence)
                attr_key = "arch.pattern.daylight_hard"
            elif key == "arch.pattern.double_height_space":
                frame.add_attribute("arch.pattern.double_height_space", present, confidence=confidence)
                attr_key = "arch.pattern.double_height_space"
            else:
                # Any other keys (even if in ACTIVE_PATTERN_KEYS) are ignored for now.
                continue

            # Attach evidence to metadata for inspection in the Feature Navigator / Admin tools.
            meta = frame.metadata.get(attr_key, {})
            meta["evidence"] = str(evidence)
            meta["source"] = "vlm"
            meta["engine"] = type(engine).__name__
            frame.metadata[attr_key] = meta

        # Always keep the full candidate list for later inspection.
        frame.metadata["arch.patterns.candidates"] = {
            "prompt": prompt,
            "prompt_version": self.prompt_version,
            "candidates": candidates,
            "engine": type(engine).__name__,
        }----- CONTENT END -----
----- FILE PATH: backend/science/semantics/ontology.py
----- CONTENT START -----
"""
Architectural parts and semantics ontology.

This module provides a forward-looking taxonomy for architectural
elements (Tier L3). Detection and labeling may use VLMs or learned
detectors, but the ontology itself should remain stable over time.
"""

from __future__ import annotations

from typing import Dict, List


ARCH_PARTS: Dict[str, List[str]] = {
    "structural": [
        "column",
        "beam",
        "arch",
        "vault",
        "truss",
        "buttress",
    ],
    "enclosure": [
        "wall",
        "floor",
        "ceiling",
        "parapet",
        "balustrade",
    ],
    "openings": [
        "door",
        "window",
        "skylight",
        "clerestory",
        "oculus",
    ],
    "circulation": [
        "stair",
        "ramp",
        "corridor",
        "landing",
        "bridge",
    ],
    "assemblies": [
        "arcade",
        "colonnade",
        "atrium",
        "courtyard",
        "nave",
        "apse",
    ],
    "furnishing": [
        "bench",
        "altar",
        "desk",
        "chair",
        "table",
        "partition",
    ],
}

SYNONYMS: Dict[str, List[str]] = {
    "clerestory": ["high window band", "upper window strip"],
    "oculus": ["round skylight", "circular opening"],
}
----- CONTENT END -----
----- FILE PATH: backend/science/semantics/semantic_tags_vlm.py
----- CONTENT START -----
"""
VLM-based semantic tagging for interior style and room function.

This analyzer turns a single Visual Language Model (VLM) call into a small
set of semantic attributes:

- style.* (modern, traditional, minimalist, scandinavian, industrial, rustic,
  bohemian, farmhouse, japandi)
- spatial.room_function.* (living_room, kitchen, bedroom, home_office, bathroom)

Design goals:
- Mirror the CognitiveStateAnalyzer pattern (JPEG encoding, stub vs. real path).
- Log cost for each real VLM call via backend.services.costs.log_vlm_usage.
- Keep the science pipeline resilient: failures record metadata but do not
  raise exceptions.
"""

from __future__ import annotations

import logging
from typing import Any, Dict

import cv2
import numpy as np

from backend.science.core import AnalysisFrame
from backend.services.vlm import get_vlm_engine, StubEngine, describe_vlm_configuration
from backend.services.costs import log_vlm_usage

logger = logging.getLogger(__name__)


class SemanticTagAnalyzer:
    """Semantic VLM analyzer for style.* and spatial.room_function.*."""

    name = "semantic_tags_vlm"

    # Single, explicit prompt that asks for normalized scores in [0, 1].
    PROMPT = (
        "You are an environmental psychology and interior-architecture expert.\n"
        "Given a single photograph of an architectural or interior space, estimate:\n"
        "1) The strength of several interior design styles (0.0‚Äì1.0 each).\n"
        "2) The plausibility that the scene serves specific room functions (0.0‚Äì1.0 each).\n\n"
        "Return ONLY strict JSON with these keys (all floats between 0.0 and 1.0):\n"
        "{\n"
        '  "style_modern": float,\n'
        '  "style_traditional": float,\n'
        '  "style_minimalist": float,\n'
        '  "style_scandinavian": float,\n'
        '  "style_industrial": float,\n'
        '  "style_rustic": float,\n'
        '  "style_bohemian": float,\n'
        '  "style_farmhouse": float,\n'
        '  "style_japandi": float,\n'
        '  "room_function_living_room": float,\n'
        '  "room_function_kitchen": float,\n'
        '  "room_function_bedroom": float,\n'
        '  "room_function_home_office": float,\n'
        '  "room_function_bathroom": float\n'
        "}\n"
    )

    def __init__(self) -> None:
        # Reserved for future versioning if we evolve the prompt.
        self.prompt_version = "1.0"

    def analyze(self, frame: AnalysisFrame) -> None:
        """Run VLM-based semantic analysis on the given frame.

        This intentionally mirrors CognitiveStateAnalyzer:
        - Uses the in-memory RGB image (np.ndarray) from the pipeline.
        - Short-circuits safely when no VLM is configured (StubEngine).
        - Logs cost once per successful real VLM call.
        """
        try:
            img = frame.original_image
            if img is None:
                logger.warning("SemanticTagAnalyzer: frame has no original_image; skipping.")
                return

            if not isinstance(img, np.ndarray):
                logger.warning(
                    "SemanticTagAnalyzer: unsupported image type %r; skipping.", type(img)
                )
                return

            ok, buffer = cv2.imencode(".jpg", img)
            if not ok:
                logger.error("SemanticTagAnalyzer: JPEG encoding failed; skipping.")
                return
            image_bytes = buffer.tobytes()

            engine = get_vlm_engine()
            prompt = self.PROMPT
            result: Dict[str, Any] = engine.analyze_image(image_bytes, prompt)

            # Stub / classroom path: record metadata only, no numeric attributes.
            if isinstance(engine, StubEngine) or result.get("stub"):
                frame.metadata["semantics.vlm"] = {
                    "status": "stub",
                    "prompt_version": self.prompt_version,
                    "engine": type(engine).__name__,
                }
                logger.info(
                    "SemanticTagAnalyzer: running in STUB mode; "
                    "no style.* or spatial.room_function.* attributes will be written."
                )
                return

            # Real-data path: record a single cost entry for this VLM call.
            try:
                cfg = describe_vlm_configuration()
                provider = cfg.get("provider", "auto")
                model_name = cfg.get("engine", type(engine).__name__)
                cost_per_1k = cfg.get("cost_per_1k_images_usd") or 0.0
                estimated_cost = float(cost_per_1k) / 1000.0 if cost_per_1k else 0.0
                log_vlm_usage(
                    provider=str(provider),
                    model_name=str(model_name),
                    cost_usd=estimated_cost,
                    meta={
                        "source": "science_pipeline_semantic_tags",
                        "image_id": getattr(frame, "image_id", None),
                        "cost_per_1k_images_usd": cost_per_1k,
                    },
                )
            except Exception:
                # Cost logging must never break the analysis.
                logger.debug("SemanticTagAnalyzer: cost logging failed", exc_info=True)

            # Map JSON keys to canonical feature keys.
            style_map = {
                "style_modern": "style.modern",
                "style_traditional": "style.traditional",
                "style_minimalist": "style.minimalist",
                "style_scandinavian": "style.scandinavian",
                "style_industrial": "style.industrial",
                "style_rustic": "style.rustic",
                "style_bohemian": "style.bohemian",
                "style_farmhouse": "style.farmhouse",
                "style_japandi": "style.japandi",
            }
            room_map = {
                "room_function_living_room": "spatial.room_function.living_room",
                "room_function_kitchen": "spatial.room_function.kitchen",
                "room_function_bedroom": "spatial.room_function.bedroom",
                "room_function_home_office": "spatial.room_function.home_office",
                "room_function_bathroom": "spatial.room_function.bathroom",
            }

            def _clamp(x: float) -> float:
                try:
                    v = float(x)
                except Exception:
                    return 0.0
                if v < 0.0:
                    return 0.0
                if v > 1.0:
                    return 1.0
                return v

            style_scores: Dict[str, float] = {}
            room_scores: Dict[str, float] = {}

            # Emit style.* attributes.
            for json_key, feature_key in style_map.items():
                if json_key in result:
                    value = _clamp(result[json_key])
                    style_scores[feature_key] = value
                    frame.add_attribute(feature_key, value, confidence=0.85)

            # Emit spatial.room_function.* attributes.
            for json_key, feature_key in room_map.items():
                if json_key in result:
                    value = _clamp(result[json_key])
                    room_scores[feature_key] = value
                    frame.add_attribute(feature_key, value, confidence=0.9)

            # Attach primary guesses and raw payload for later inspection.
            def _argmax(mapping: Dict[str, float]) -> Dict[str, Any]:
                if not mapping:
                    return {}
                best_key = max(mapping, key=lambda k: mapping[k])
                return {"key": best_key, "score": mapping[best_key]}

            frame.metadata.setdefault("semantics", {})
            frame.metadata["semantics"]["primary_style"] = _argmax(style_scores)
            frame.metadata["semantics"]["primary_room_function"] = _argmax(room_scores)
            frame.metadata["semantics"]["raw_vlm_result"] = result
            frame.metadata["semantics"]["prompt_version"] = self.prompt_version

        except Exception as exc:
            # Fail soft: record an error flag and keep the rest of the pipeline alive.
            logger.error("SemanticTagAnalyzer failed: %s", exc)
            frame.metadata.setdefault("semantics", {})
            frame.metadata["semantics"]["error"] = str(exc)
----- CONTENT END -----
----- FILE PATH: backend/science/spatial/depth.py
----- CONTENT START -----
"""Spatial structure, depth maps, and clutter proxies for depth-like properties.

This module implements 2D edge/clutter heuristics as proxies for
depth-related concepts (openness, refuge, isovist area), and optionally
integrates a monocular depth estimation model via ONNX Runtime when
configured.

The goal is to make the depth layer *scientifically honest*:
- If a depth model is available, we compute a normalized depth map and
  expose simple summary statistics as BN variables.
- If not, we fall back to the existing edge-based heuristics so the
  rest of the pipeline continues to function.
"""

from __future__ import annotations

import logging
import os
from typing import Optional

import numpy as np

from backend.science.core import AnalysisFrame

logger = logging.getLogger(__name__)

try:  # Optional dependency; we degrade gracefully if missing.
    import onnxruntime as ort  # type: ignore
except Exception:  # pragma: no cover - onnxruntime not installed
    ort = None  # type: ignore


class DepthAnalyzer:
    """Analyze spatial structure using edges and optional monocular depth.

    This class maintains backward-compatible 2D edge/clutter proxies while
    optionally computing a true depth map when an ONNX model is available.

    Configuration
    -------------
    If you wish to enable monocular depth:

    1. Install onnxruntime in the backend environment.
    2. Download a suitable depth model (e.g. DepthAnything / MiDaS) in ONNX
       format.
    3. Set the environment variable ``DEPTH_ANYTHING_ONNX_PATH`` to the
       model path inside the container.

    When misconfigured (missing library or model), the analyzer logs a
    warning and silently falls back to the legacy heuristics.
    """

    _onnx_session: Optional["ort.InferenceSession"] = None

    @classmethod
    def _get_onnx_session(cls) -> Optional["ort.InferenceSession"]:
        """Lazily initialise the ONNX Runtime session if possible."""
        if ort is None:
            return None
        if cls._onnx_session is not None:
            return cls._onnx_session

        model_path = os.getenv("DEPTH_ANYTHING_ONNX_PATH")
        if not model_path or not os.path.exists(model_path):
            return None

        try:
            cls._onnx_session = ort.InferenceSession(  # type: ignore[attr-defined]
                model_path,
                providers=["CPUExecutionProvider"],
            )
            logger.info("DepthAnalyzer: loaded depth ONNX model from %s", model_path)
        except Exception:  # pragma: no cover - runtime environment dependent
            logger.warning(
                "DepthAnalyzer: failed to load depth ONNX model from %s; "
                "falling back to edge-based heuristics.",
                model_path,
                exc_info=True,
            )
            cls._onnx_session = None

        return cls._onnx_session

    @classmethod
    def _compute_depth_map(cls, frame: AnalysisFrame) -> Optional[np.ndarray]:
        """Compute a normalized depth map for the frame, if configured.

        Returns a H√óW float32 array in [0, 1] when successful, otherwise
        ``None``. Any errors cause a clean fallback to the legacy
        edge-based heuristics.
        """
        session = cls._get_onnx_session()
        if session is None:
            return None

        if frame.original_image is None:
            return None

        try:
            import cv2  # type: ignore
        except Exception:  # pragma: no cover - cv2 not available
            logger.warning(
                "DepthAnalyzer: OpenCV (cv2) not available; "
                "cannot run ONNX depth model. Falling back to heuristics."
            )
            return None

        img = frame.original_image
        if img.ndim == 2:  # grayscale ‚Üí fake 3-channel
            img = np.stack([img, img, img], axis=-1)

        # Many monocular models expect a square input; 384√ó384 is common.
        target_size = (384, 384)
        resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)
        inp = resized.astype(np.float32) / 255.0
        inp = np.transpose(inp, (2, 0, 1))[None, ...]  # NCHW

        try:
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: inp})
        except Exception:  # pragma: no cover - runtime specific
            logger.warning(
                "DepthAnalyzer: ONNX inference failed; "
                "falling back to edge-based heuristics.",
                exc_info=True,
            )
            return None

        depth = np.squeeze(outputs[0])
        if depth.ndim == 3:
            depth = depth[0]

        h, w = frame.original_image.shape[:2]
        depth_resized = cv2.resize(depth, (w, h), interpolation=cv2.INTER_LINEAR)

        d_min = float(depth_resized.min())
        d_max = float(depth_resized.max())
        if d_max > d_min:
            depth_norm = (depth_resized - d_min) / (d_max - d_min)
        else:
            depth_norm = np.zeros_like(depth_resized, dtype=np.float32)

        return depth_norm.astype(np.float32)

    @staticmethod
    def _summarise_depth(depth: np.ndarray) -> tuple[float, float]:
        """Return (mean_depth, depth_contrast) in [0, 1]."""
        if depth.size == 0:
            return 0.0, 0.0

        mean_depth = float(depth.mean())
        std_depth = float(depth.std())

        # Clamp to [0, 1] for BN friendliness
        norm_mean = float(np.clip(mean_depth, 0.0, 1.0))
        # Contrast emphasises mid-range variance but avoids extreme outliers
        contrast = float(np.clip(std_depth * 0.5, 0.0, 1.0))
        return norm_mean, contrast

    def analyze(self, frame: AnalysisFrame) -> None:
        """Compute depth-related attributes for a single frame.

        - Always produces the legacy edge-based clutter/openness/refuge
          proxies used by existing BN scripts.
        - Optionally computes a depth map and summary statistics when a
          monocular depth model is configured.
        """
        # Optional: true depth map via ONNX
        depth_map = self._compute_depth_map(frame)
        if depth_map is not None:
            frame.depth_map = depth_map
            mean_depth, depth_contrast = self._summarise_depth(depth_map)
            frame.add_attribute("spatial.depth_mean", mean_depth)
            frame.add_attribute("spatial.depth_contrast", depth_contrast)

        # Always compute legacy 2D edge proxies so existing BN exports
        # remain stable, even when no depth model is available.
        if frame.edges is None:
            # AnalysisFrame.precompute_basics() should normally ensure edges,
            # but if it was skipped we avoid crashing the pipeline.
            logger.warning(
                "DepthAnalyzer: frame.edges is None; skipping clutter/openness "
                "computation for this frame."
            )
            return

        # 1. Visual Clutter (Edge Variance)
        clutter_score = DepthAnalyzer.calculate_clutter_proxy(frame.edges)
        frame.add_attribute("spatial.visual_clutter", clutter_score)

        # 2. Central Openness (Prospect proxy)
        openness = DepthAnalyzer.calculate_central_openness(frame.edges)
        frame.add_attribute("spatial.central_openness", openness)

        # 3. Refuge Quality (Bayesian Node)
        refuge = DepthAnalyzer.calculate_refuge_quality(frame)
        frame.add_attribute("spatial.refuge_quality", refuge)

        # 4. Isovist Area (Affordance)
        frame.add_attribute("affordance.isovist_area", openness * 0.8)

    @staticmethod
    def calculate_clutter_proxy(edges: np.ndarray) -> float:
        """Heuristic clutter score based on edge density variance.

        High variance in edge density across a grid implies disordered
        clutter; we normalise the result to [0, 1].
        """
        h, w = edges.shape
        grid_h, grid_w = max(h // 8, 1), max(w // 8, 1)
        variances = []
        for y in range(0, h, grid_h):
            for x in range(0, w, grid_w):
                cell = edges[y : y + grid_h, x : x + grid_w]
                if cell.size > 0:
                    density = np.count_nonzero(cell) / float(cell.size)
                    variances.append(density)
        if not variances:
            return 0.0
        return float(min(np.std(variances) * 5.0, 1.0))

    @staticmethod
    def calculate_central_openness(edges: np.ndarray) -> float:
        """Prospect proxy based on edge density in the central window."""
        h, w = edges.shape
        cy, cx = h // 2, w // 2
        dy, dx = max(h // 6, 1), max(w // 6, 1)
        center_crop = edges[cy - dy : cy + dy, cx - dx : cx + dx]
        if center_crop.size == 0:
            return 0.0
        edge_density = np.count_nonzero(center_crop) / float(center_crop.size)
        return float(1.0 - min(edge_density * 5.0, 1.0))

    @staticmethod
    def calculate_refuge_quality(frame: AnalysisFrame) -> float:
        """Estimate 'Refuge' potential from near-floor occlusion / shelter.

        This metric now prefers depth-map-aware reasoning when a monocular
        depth model is configured, and falls back to the legacy edge-based
        heuristic otherwise.

        Intuition
        ---------
        - In a depth map, *near* pixels (small depth) near the bottom of the
          frame are interpreted as potential occluding / sheltering elements
          (sofas, counters, railings, low walls).
        - We summarise this using the average *nearness* (1 - normalised depth)
          over the bottom band of the frame.
        - If no depth map is available, we retain the previous Canny-based
          proxy so the rest of the pipeline continues to function.
        """
        # Preferred path: use the depth map if available.
        depth = getattr(frame, "depth_map", None)
        if depth is not None:
            arr = np.asarray(depth, dtype="float32")
            if arr.ndim == 3:
                # Handle HxWx1 or HxWxC by collapsing channels.
                arr = arr[..., 0]
            if arr.ndim != 2 or arr.size == 0:
                return 0.0

            h, _w = arr.shape
            if h < 4:
                return 0.0

            # Bottom 30% of the frame as a near-floor / near-viewing band.
            start_row = int(h * 0.7)
            band = arr[start_row:, :]
            if band.size == 0:
                return 0.0

            # Robust per-band normalisation to [0, 1]
            band_min = float(np.nanmin(band))
            band_max = float(np.nanmax(band))
            if not np.isfinite(band_min) or not np.isfinite(band_max):
                return 0.0
            if band_max > band_min:
                norm = (band - band_min) / (band_max - band_min)
            else:
                norm = np.zeros_like(band, dtype="float32")

            # Higher nearness = stronger sense of nearby refuge.
            nearness = 1.0 - norm
            score = float(np.clip(np.nanmean(nearness), 0.0, 1.0))
            return score

        # Fallback: legacy edge-density heuristic.
        edges = frame.edges
        if edges is None:
            return 0.0

        h, _w = edges.shape
        if h < 4:
            return 0.0

        bottom_crop = edges[int(h * 0.8) :, :]
        if bottom_crop.size == 0:
            return 0.0

        occ_density = np.count_nonzero(bottom_crop) / float(bottom_crop.size)
        return float(np.clip(occ_density * 2.0, 0.0, 1.0))
----- CONTENT END -----
----- FILE PATH: backend/science/spatial/depth_plugin.py
----- CONTENT START -----
"""
Depth provider plugin interface.

This module defines a minimal protocol for plugging in a monocular depth
estimator without tying the science pipeline to a particular model.
"""

from __future__ import annotations

from typing import Protocol

import numpy as np
from backend.science.core import AnalysisFrame
from backend.science.contracts import fail


class DepthProvider(Protocol):
    name: str

    def infer_depth(self, image_rgb: np.ndarray) -> np.ndarray:
        raise NotImplementedError("DepthProvider is a protocol; implement infer_depth in a plugin.")


class DepthPluginAnalyzer:
    """
    Optional analyzer that calls a configured DepthProvider and stores
    the result in the AnalysisFrame. By default this is a no-op stub
    until a provider is configured.
    """

    name = "depth_plugin"
    tier = "L2"
    requires = ["original_image"]
    provides = ["depth_map"]

    # TODO: wire in a real provider via configuration.
    provider: DepthProvider | None = None

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        if cls.provider is None:
            fail(frame, cls.name, "no depth provider configured")
            return

        depth = cls.provider.infer_depth(frame.original_image)
        frame.depth_map = depth
----- CONTENT END -----
----- FILE PATH: backend/science/spatial/isovist.py
----- CONTENT START -----
import numpy as np
import cv2
from backend.science.core import AnalysisFrame

class IsovistAnalyzer:
    """
    Implements 2D Isovist analysis on the floor plan (or segmentation mask).
    Calculates 'Visual Area' from a vantage point.
    """

    @staticmethod
    def compute_2d_isovist(frame: AnalysisFrame, floor_mask: np.ndarray = None):
        """
        Simulates a 360-degree raycast from the image center (or floor center).
        If no floor_mask provided, uses basic edge avoidance.
        """
        h, w, _ = frame.original_image.shape
        center = (w // 2, h // 2)
        
        # Create an obstacle map from edges (simplistic proxy for walls)
        obstacles = frame.edges > 0
        
        # Raycasting (Simplified 36-ray sweep)
        # In production, use a proper visibility polygon algorithm
        visible_distance_sum = 0
        rays = 36
        max_dist = np.sqrt(h**2 + w**2) / 2
        
        for i in range(rays):
            angle = (i * 360 / rays) * (np.pi / 180)
            dx = np.cos(angle)
            dy = np.sin(angle)
            
            dist = 0
            for r in range(1, int(max_dist)):
                cx, cy = int(center[0] + dx * r), int(center[1] + dy * r)
                
                # Bounds check
                if cx < 0 or cx >= w or cy < 0 or cy >= h:
                    break
                    
                # Obstacle check (Hit a wall/edge)
                if obstacles[cy, cx]:
                    break
                
                dist = r
            
            visible_distance_sum += dist
            
        # Normalize "Openness" based on average ray length vs image size
        avg_dist = visible_distance_sum / rays
        openness_score = min(avg_dist / (min(h,w)/2), 1.0)
        
        frame.add_attribute("spatial.isovist_openness", openness_score)----- CONTENT END -----
----- FILE PATH: backend/science/spatial/isovist_25d.py
----- CONTENT START -----
"""
Approximate 2.5D isovist analysis based on a depth map.

This is an intentionally cautious implementation: until a high-quality
depth model is wired in, we treat the outputs as low-confidence proxies.
"""

from __future__ import annotations

import numpy as np
from backend.science.core import AnalysisFrame
from backend.science.contracts import fail


class Isovist25DAnalyzer:
    name = "isovist_25d"
    tier = "L2"
    requires = ["depth_map"]
    provides = [
        "isovist.area_25d",
        "isovist.compactness_25d",
        "isovist.confidence",
    ]

    @staticmethod
    def _perimeter(mask: np.ndarray) -> float:
        from scipy.ndimage import binary_erosion

        eroded = binary_erosion(mask)
        border = mask ^ eroded
        return float(border.sum())

    @classmethod
    def analyze(cls, frame: AnalysisFrame) -> None:
        depth = frame.depth_map
        if depth is None:
            fail(frame, cls.name, "no depth_map available")
            return

        # Simple near-space threshold as a proxy for "occupied" region.
        thresh = np.percentile(depth, 60.0)
        free = depth < thresh
        area = free.mean()
        perim = cls._perimeter(free)
        compactness = (4.0 * np.pi * area) / (perim ** 2 + 1e-9)

        # Low confidence until depth model is validated.
        confidence = 0.4

        frame.add_attribute("isovist.area_25d", float(area), confidence=confidence)
        frame.add_attribute("isovist.compactness_25d", float(compactness), confidence=confidence)
        frame.metadata["isovist_25d"] = {
            "threshold_percentile": 60.0,
            "confidence": confidence,
        }
        frame.add_attribute("isovist.confidence", confidence)
----- CONTENT END -----
----- FILE PATH: backend/science/vision/materials.py
----- CONTENT START -----
"""
Material analysis module for v3 Science Pipeline.

Ported heuristics from v2.6.3 MaterialClassifier into the v3
AnalysisFrame pattern. This module is deliberately lightweight and
does not depend on the v2 AttributeExtractor base class.
"""

from __future__ import annotations

from typing import Optional

import numpy as np
import cv2

from backend.science.core import AnalysisFrame
from backend.services.vlm import get_vlm_engine, StubEngine


class MaterialAnalyzer:
    """
    Heuristic-based material classification.

    Uses simple HSV and luminance/texture rules to estimate coverage
    of wood, metal, and glass in the scene. Values are normalized
    coverage ratios in [0, 1].
    """

    @staticmethod
    def analyze(frame: AnalysisFrame) -> None:
        img = frame.original_image
        if img is None:
            return

        # Convert to uint8 RGB for OpenCV operations
        if img.dtype == np.float32 or img.dtype == np.float64:
            image_uint8 = (img * 255).astype(np.uint8)
        else:
            image_uint8 = img

        # Convert to HSV for material heuristics
        hsv = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2HSV)

        # --- Wood heuristic (ported from v2 logic) ---
        # Brown-ish hues (approx 5‚Äì30 in OpenCV HSV),
        # with reasonable saturation and value.
        wood_mask = (
            (hsv[:, :, 0] >= 5)
            & (hsv[:, :, 0] <= 30)
            & (hsv[:, :, 1] > 30)
            & (hsv[:, :, 2] > 50)
        )
        wood_coverage = float(np.sum(wood_mask) / wood_mask.size)
        frame.add_attribute("material.wood_coverage", wood_coverage)

        # --- Metal heuristic (simplified from v2) ---
        # Low saturation, mid-to-high value ‚Üí shiny / metallic regions.
        metal_mask = (hsv[:, :, 1] < 30) & (hsv[:, :, 2] > 150)
        metal_coverage = float(np.sum(metal_mask) / metal_mask.size)
        frame.add_attribute("material.metal_coverage", metal_coverage)

        # --- Glass heuristic (ported from v2) ---
        # High luminance + low local variance ‚Üí smooth bright panes.
        gray = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2GRAY)
        bright_mask = gray > 200

        # Local variance proxy using a smoothing kernel
        kernel = np.ones((5, 5), np.float32) / 25.0
        local_mean = cv2.filter2D(gray.astype(float), -1, kernel)
        local_var = (gray.astype(float) - local_mean) ** 2
        smooth_mask = local_var < 100.0

        glass_mask = bright_mask & smooth_mask
        glass_coverage = float(np.sum(glass_mask) / glass_mask.size)
        frame.add_attribute("material.glass_coverage", glass_coverage)
        # --- L1 material cues (read-only; support higher tiers) ---
        # Normalized brightness (0-1) from grayscale.
        gray_float = gray.astype(float) / 255.0
        brightness_mean = float(gray_float.mean())
        frame.add_attribute("materials.cues.brightness_mean", brightness_mean, confidence=0.7)

        # Global texture variance (roughness proxy).
        texture_variance = float(gray_float.var())
        frame.add_attribute("materials.cues.texture_variance", min(texture_variance * 10.0, 1.0), confidence=0.6)

        # Mean saturation and value in HSV.
        sat_mean = float(hsv[:, :, 1].mean() / 255.0)
        val_mean = float(hsv[:, :, 2].mean() / 255.0)
        frame.add_attribute("materials.cues.saturation_mean", sat_mean, confidence=0.7)
        frame.add_attribute("materials.cues.value_mean", val_mean, confidence=0.7)

        # Specularity proxy: proportion of high-value, low-saturation pixels.
        spec_mask = (hsv[:, :, 1] < 40) & (hsv[:, :, 2] > 200)
        specularity_proxy = float(spec_mask.sum() / spec_mask.size)
        frame.add_attribute("materials.cues.specularity_proxy", specularity_proxy, confidence=0.6)

        # --- Substrate heuristics beyond wood/metal/glass ---
        # Stone/Concrete: low saturation, mid value, higher roughness.
        stone_mask = (
            (hsv[:, :, 1] < 60) &
            (hsv[:, :, 2] > 60) &
            (hsv[:, :, 2] < 200)
        )
        stone_coverage = float(stone_mask.sum() / stone_mask.size)
        frame.add_attribute("materials.substrate.stone_concrete", stone_coverage, confidence=0.5)

        # Plaster/Gypsum: very low saturation, high value, low variance.
        plaster_mask = (
            (hsv[:, :, 1] < 30) &
            (hsv[:, :, 2] > 180)
        )
        plaster_coverage = float(plaster_mask.sum() / plaster_mask.size)
        frame.add_attribute("materials.substrate.plaster_gypsum", plaster_coverage, confidence=0.5)

        # Tile/Ceramic: bright and moderately saturated with elevated local variance.
        # We reuse local_var from the glass heuristic as a crude texture cue.
        tile_mask = (
            (hsv[:, :, 2] > 150) &
            (hsv[:, :, 1] > 40) &
            (local_var > 50.0)
        )
        tile_coverage = float(tile_mask.sum() / tile_mask.size)
        frame.add_attribute("materials.substrate.tile_ceramic", tile_coverage, confidence=0.4)


def _maybe_run_materials_vlm(frame: AnalysisFrame, image_uint8: np.ndarray) -> None:
    """Optional VLM pass for materials.

    This function is intentionally conservative: it never writes numeric
    materials.* attributes. Instead, it records a structured candidate list
    under frame.metadata["materials.vlm_candidates"] when a real VLM is configured.

    When running under StubEngine, we record only a note so downstream science
    knows that no VLM data were present.
    """
    try:
        ok, buffer = cv2.imencode(".jpg", image_uint8)
        if not ok:
            return
        image_bytes = buffer.tobytes()
        engine = get_vlm_engine()
        substrates = [
            "materials.substrate.stone_concrete",
            "materials.substrate.plaster_gypsum",
            "materials.substrate.tile_ceramic",
        ]
        prompt = (
            "You are an architectural materials analyst. "
            "Given this interior image, estimate the presence of the following materials substrates, "
            "each from 0.0 to 1.0, and provide a brief evidence string.\n"
            "Substrates:\n- " + "\n- ".join(substrates) + "\n"
            "Return STRICT JSON as a list of objects with fields "
            "{'key': <substrate_key>, 'present': <0-1>, 'confidence': <0-1>, 'evidence': <short text>}."
        )
        result = engine.analyze_image(image_bytes, prompt)
    except Exception:
        # We silently skip VLM errors for materials; core CV cues remain available.
        return

    # Stub / classroom path.
    if isinstance(engine, StubEngine) or (isinstance(result, dict) and result.get("stub")):
        frame.metadata["materials.vlm_candidates"] = {
            "note": "VLM in stub mode; no materials.* VLM candidates.",
            "engine": type(engine).__name__,
        }
        return

    candidates = []
    if isinstance(result, list):
        candidates = result
    elif isinstance(result, dict) and "materials" in result and isinstance(result["materials"], list):
        candidates = result["materials"]

    frame.metadata["materials.vlm_candidates"] = {
        "engine": type(engine).__name__,
        "candidates": candidates,
    }
----- CONTENT END -----
----- FILE PATH: backend/science/vision/objects.py
----- CONTENT START -----
import numpy as np
from backend.science.core import AnalysisFrame
import logging

# Lazy load ultralytics to keep startup fast if not used
YOLO_MODEL = None

logger = logging.getLogger("v3.science.objects")

class ObjectAnalyzer:
    """
    Wraps YOLOv8 for counting architectural elements (chairs, people, plants).
    Essential for 'Social Affordance' metrics.
    """
    
    @staticmethod
    def load_model():
        global YOLO_MODEL
        if YOLO_MODEL is None:
            from ultralytics import YOLO
            # Downloads 'yolov8n.pt' (nano) automatically on first run (~6MB)
            # Use 'yolov8x.pt' for production accuracy
            logger.info("Loading YOLOv8 model")
            YOLO_MODEL = YOLO("yolov8n.pt")
            
    @staticmethod
    def analyze(frame: AnalysisFrame):
        ObjectAnalyzer.load_model()
        
        # Run Inference
        results = YOLO_MODEL(frame.original_image, verbose=False)
        
        # Count classes
        counts = {}
        for result in results:
            for box in result.boxes:
                cls_id = int(box.cls[0])
                label = YOLO_MODEL.names[cls_id]
                counts[label] = counts.get(label, 0) + 1
                
        # Mapping to CNfA attributes
        # 1. Seating (Social)
        seating_count = counts.get('chair', 0) + counts.get('couch', 0) + counts.get('bench', 0)
        frame.add_attribute("affordance.seating_count", seating_count)
        
        # 2. Biophilia (Plants)
        plant_count = counts.get('potted plant', 0)
        frame.add_attribute("biophilia.plant_count", plant_count)
        
        # 3. Occupancy
        person_count = counts.get('person', 0)
        frame.add_attribute("social.occupancy", person_count)----- CONTENT END -----
----- FILE PATH: backend/scripts/audit_vlm_variance.py
----- CONTENT START -----
"""Audit script for VLM-derived attribute variance.

This script scans Validation records for cognitive.* and affect.*
attributes produced by the science pipeline / VLM, and flags cases
where the output distribution is suspiciously collapsed (e.g. the
same value for >90% of images).

Usage (inside the Docker `api` container)
-----------------------------------------

    python -m backend.scripts.audit_vlm_variance

The output is a JSON blob with a summary for each attribute key as
well as a list of suspicious keys that may indicate a failed prompt
or misconfigured VLM model.
"""
from __future__ import annotations

import json
import math
from collections import Counter
from typing import Dict, List, Tuple

from sqlalchemy.orm import Session

from backend.database.core import SessionLocal
from backend.models.annotation import Validation


TARGET_PREFIXES: Tuple[str, ...] = ("cognitive.", "affect.")
MIN_COUNT_DEFAULT: int = 50
MODE_COLLAPSE_THRESHOLD_DEFAULT: float = 0.9


def _load_values_for_key(session: Session, key: str) -> List[float]:
    """Load numeric values for a given attribute key.

    We restrict to science_pipeline sources, which is where the VLM-backed
    cognitive / affective attributes land in the current architecture.
    """
    rows = (
        session.query(Validation.value)
        .filter(Validation.attribute_key == key)
        .filter(Validation.value.is_not(None))
        .filter(Validation.source.like("science_pipeline%"))
    )
    values: List[float] = []
    for (val,) in rows:
        try:
            values.append(float(val))
        except (TypeError, ValueError):
            continue
    return values


def audit_vlm_variance(
    session: Session,
    min_count: int = MIN_COUNT_DEFAULT,
    mode_collapse_threshold: float = MODE_COLLAPSE_THRESHOLD_DEFAULT,
) -> Dict[str, Dict[str, float]]:
    """Compute variance diagnostics for VLM-backed attributes.

    Returns a mapping from attribute_key to a small summary dict containing:
    - count
    - mean
    - std
    - mode_value
    - mode_ratio
    - is_suspicious
    """
    report: Dict[str, Dict[str, float]] = {}

    # Discover candidate keys
    keys: List[str] = []
    for prefix in TARGET_PREFIXES:
        q = (
            session.query(Validation.attribute_key)
            .filter(Validation.attribute_key.like(f"{prefix}%"))
            .distinct()
        )
        keys.extend(k for (k,) in q)

    for key in sorted(set(keys)):
        values = _load_values_for_key(session, key)
        n = len(values)
        if n < min_count:
            continue

        rounded = [round(v, 2) for v in values]
        counts = Counter(rounded)
        if not counts:
            continue
        mode_value, mode_freq = counts.most_common(1)[0]
        mode_ratio = mode_freq / float(n)

        mean = sum(rounded) / float(n)
        var = sum((x - mean) ** 2 for x in rounded) / float(n)
        std = math.sqrt(var)

        is_suspicious = mode_ratio >= mode_collapse_threshold or std < 0.02

        report[key] = {
            "count": n,
            "mean": mean,
            "std": std,
            "mode_value": mode_value,
            "mode_ratio": mode_ratio,
            "is_suspicious": is_suspicious,
        }

    return report


def main() -> None:
    session = SessionLocal()
    try:
        report = audit_vlm_variance(session)
    finally:
        session.close()

    suspicious = {k: v for k, v in report.items() if v.get("is_suspicious")}
    payload = {
        "ok": not bool(suspicious),
        "suspicious_keys": suspicious,
        "all_keys": report,
    }
    print(json.dumps(payload, indent=2, sort_keys=True))


if __name__ == "__main__":  # pragma: no cover - CLI helper
    main()
----- CONTENT END -----
----- FILE PATH: backend/scripts/bn_db_health.py
----- CONTENT START -----
"""BN / DB health checker.

This module inspects the live database to ensure that:

- All Validation.attribute_key values are present in the canonical
  Attribute registry (attributes.key).
- All BN candidate keys exposed by the science index catalog are
  also present (and typically active) in the Attribute registry.

It is intended to be run either:

  - as a standalone CLI:

        python -m backend.scripts.bn_db_health

    (or equivalently: ``python backend/scripts/bn_db_health.py``)

  - or via the governance guardian when
    ``constraints.check_bn_db_health`` is enabled in v3_governance.yml.
"""
from __future__ import annotations

from typing import Any, Dict, Set

from sqlalchemy.exc import SQLAlchemyError

from backend.database.core import SessionLocal
from backend.models.attribute import Attribute
from backend.models.annotation import Validation
from backend.science.index_catalog import get_candidate_bn_keys


def _fetch_attribute_keys() -> Set[str]:
    db = SessionLocal()
    try:
        rows = db.query(Attribute.key).filter(Attribute.is_active.is_(True)).all()
        return {row[0] for row in rows if row[0] is not None}
    finally:
        db.close()


def _fetch_validation_keys() -> Set[str]:
    db = SessionLocal()
    try:
        rows = db.query(Validation.attribute_key).distinct().all()
        return {row[0] for row in rows if row[0] is not None}
    finally:
        db.close()


def run_health_check(exit_on_failure: bool = True) -> Dict[str, Any]:
    """Run BN / DB health checks.

    Parameters
    ----------
    exit_on_failure:
        When True (CLI usage), the process will exit with status 1 if any
        violations are detected. When False (guardian usage), a summary
        dictionary is returned and no ``sys.exit`` is called.

    Returns
    -------
    summary:
        Dict containing keys:

        - ok: bool
        - orphan_validations: int
        - missing_candidates: int
        - orphan_validation_keys: optional list[str] (truncated)
        - missing_candidate_keys: optional list[str] (truncated)
        - error: optional str if the check could not be completed
    """
    import sys

    summary: Dict[str, Any] = {
        "ok": False,
        "orphan_validations": 0,
        "missing_candidates": 0,
    }

    try:
        attr_keys = _fetch_attribute_keys()
        validation_keys = _fetch_validation_keys()
    except SQLAlchemyError as exc:  # pragma: no cover - depends on DB wiring
        msg = f"[bn_db_health] Database error while fetching keys: {exc}"
        print(msg)
        summary["error"] = msg
        if exit_on_failure:
            sys.exit(1)
        return summary

    # BN candidates are the keys we expect to show up in BN exports.
    candidate_keys = set(get_candidate_bn_keys())

    orphan_validation_keys = sorted(validation_keys - attr_keys)
    missing_candidate_keys = sorted(candidate_keys - attr_keys)

    summary["orphan_validations"] = len(orphan_validation_keys)
    summary["missing_candidates"] = len(missing_candidate_keys)
    summary["ok"] = not orphan_validation_keys and not missing_candidate_keys

    print("[bn_db_health] Attribute keys in registry:", len(attr_keys))
    print("[bn_db_health] Distinct Validation.attribute_key values:", len(validation_keys))
    print("[bn_db_health] BN candidate keys:", len(candidate_keys))

    if orphan_validation_keys:
        preview = ", ".join(orphan_validation_keys[:10])
        print(
            f"[bn_db_health] Orphan Validation.attribute_key values "
            f"(not in attributes.key): {len(orphan_validation_keys)} "
            f"(e.g., {preview})"
        )
        summary["orphan_validation_keys"] = orphan_validation_keys[:50]

    if missing_candidate_keys:
        preview = ", ".join(missing_candidate_keys[:10])
        print(
            f"[bn_db_health] BN candidate keys missing from attributes.key: "
            f"{len(missing_candidate_keys)} (e.g., {preview})"
        )
        summary["missing_candidate_keys"] = missing_candidate_keys[:50]

    if summary["ok"]:
        print("[bn_db_health] OK: No orphan validations and all BN candidate keys present.")
    else:
        print("[bn_db_health] FAIL: See details above.")

    if exit_on_failure and not summary["ok"]:
        sys.exit(1)

    return summary


def main() -> None:  # pragma: no cover - thin CLI wrapper
    run_health_check(exit_on_failure=True)


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: backend/scripts/migrate_3_4_63_add_validation_fk.py
----- CONTENT START -----
"""Migration helper: add Validation.attribute_key ‚Üí attributes.key foreign key.

This script is intended for PostgreSQL databases that were created
*before* v3.4.63, when the Validation.attribute_key column was not
yet backed by a real FOREIGN KEY in the live schema.

For databases initialised via v3.4.63+ with Base.metadata.create_all,
the constraint should already exist and this script will be a no-op.

Usage (inside the Docker `api` container)
----------------------------------------

    python -m backend.scripts.migrate_3_4_63_add_validation_fk

The script is idempotent and safe to run multiple times.
"""
from __future__ import annotations

from typing import Any, Dict, List

from sqlalchemy import inspect, text
from sqlalchemy.exc import SQLAlchemyError

from backend.database.core import engine
from backend.models.annotation import Validation


CONSTRAINT_NAME = "fk_validations_attribute_key_attributes_key"


def _has_fk() -> bool:
    """Return True if the Validation.attribute_key FK already exists.

    This uses SQLAlchemy's inspection API so it does not depend on
    any specific constraint name; it will accept either the canonical
    name defined here or an automatically-generated name, as long as
    the constrained and referred columns match.
    """
    inspector = inspect(engine)
    table_name = Validation.__table__.name  # type: ignore[attr-defined]
    fks: List[Dict[str, Any]] = inspector.get_foreign_keys(table_name)

    for fk in fks:
        constrained = fk.get("constrained_columns") or []
        referred_table = fk.get("referred_table")
        referred_cols = fk.get("referred_columns") or []
        name = fk.get("name") or ""

        if constrained == ["attribute_key"] and referred_table == "attributes" and referred_cols == ["key"]:
            return True
        if name == CONSTRAINT_NAME:
            return True

    return False


def _add_fk() -> None:
    """Issue the ALTER TABLE statement to add the FK.

    This assumes PostgreSQL semantics and will run inside a short-lived
    transaction. If the constraint already exists, PostgreSQL will
    raise an error and the caller will handle it.
    """
    table_name = Validation.__table__.name  # type: ignore[attr-defined]
    ddl = text(
        f"ALTER TABLE {table_name} "
        f"ADD CONSTRAINT {CONSTRAINT_NAME} "
        "FOREIGN KEY (attribute_key) REFERENCES attributes(key);"
    )

    with engine.begin() as conn:
        conn.execute(ddl)


def main(exit_on_failure: bool = True) -> int:
    """Entry point for the migration.

    Parameters
    ----------
    exit_on_failure:
        When True (default), a non-zero exit code will trigger
        ``sys.exit(code)`` in the CLI wrapper. When False, the caller
        (e.g. a higher-level script) can inspect the returned code.
    """
    try:
        if _has_fk():
            print(
                "[migrate_3_4_63_add_validation_fk] "
                "Foreign key already present; no migration needed."
            )
            return 0

        print(
            "[migrate_3_4_63_add_validation_fk] "
            "Adding foreign key on Validation.attribute_key ‚Üí attributes.key..."
        )
        _add_fk()
        print("[migrate_3_4_63_add_validation_fk] Migration complete.")
        return 0
    except SQLAlchemyError as exc:
        msg = (
            "[migrate_3_4_63_add_validation_fk] Database error while applying migration: "
            f"{exc}"
        )
        print(msg)
        return 1
    except Exception as exc:  # pragma: no cover - unexpected runtime errors
        msg = (
            "[migrate_3_4_63_add_validation_fk] Unexpected error: "
            f"{exc}"
        )
        print(msg)
        return 1


if __name__ == "__main__":  # pragma: no cover - thin CLI wrapper
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description=(
            "Add the missing Validation.attribute_key ‚Üí attributes.key foreign key "
            "for databases created before v3.4.63."
        )
    )
    parser.add_argument(
        "--no-exit-on-failure",
        action="store_true",
        help=(
            "Return a non-zero status code instead of calling sys.exit "
            "when an error occurs."
        ),
    )
    args = parser.parse_args()
    code = main(exit_on_failure=not args.no_exit_on_failure)
    if code != 0:
        sys.exit(code)
----- CONTENT END -----
----- FILE PATH: backend/scripts/seed_attributes.py
----- CONTENT START -----
"""
Seed script for attribute taxonomy.

Reads contracts/attributes.yml (v2.6.3-compatible format) and inserts
Attribute rows into the v3 database if they are missing.

Intended usage (from repo root):

    python -m backend.scripts.seed_attributes

In Docker (install.sh):

    docker-compose exec -T api python backend/scripts/seed_attributes.py
"""
from __future__ import annotations

import sys
from pathlib import Path
from typing import List, Dict, Any

import yaml
from sqlalchemy.orm import Session

from backend.database.core import SessionLocal, engine
from backend.models import Base, Attribute


REPO_ROOT = Path(__file__).resolve().parents[2]
ATTRIBUTES_YML = REPO_ROOT / "contracts" / "attributes.yml"


def parse_attributes(lines: List[str]) -> List[Dict[str, Any]]:
    """
    Parse the slightly non-standard attributes.yml shipped in v2.6.3.

    The file has a header:

        schema:
          - id: string
        # (additional schema fields omitted)
        attributes:
          - id: geometry.curvilinearity
            name: Curvilinearity

    followed by a long list of '- id:' blocks. We do a lightweight
    streaming parse that:

    - ignores the 'schema' section
    - starts collecting once we see 'attributes:'
    - treats any line starting with '- id:' as a new record
    - collects subsequent 'key: value' lines into the same record
    """
    entries: List[Dict[str, Any]] = []
    in_attrs = False
    current: Dict[str, Any] | None = None

    for raw in lines:
        line = raw.rstrip("\n")
        stripped = line.strip()

        if not in_attrs:
            if stripped.startswith("attributes:"):
                in_attrs = True
            continue

        if not stripped or stripped.startswith("#"):
            continue

        if stripped.startswith("- id:"):
            if current:
                entries.append(current)
            current = {}
            _, _, val = stripped.partition(":")
            current["id"] = val.strip()
            continue

        if current is not None and ":" in stripped:
            key, _, val = stripped.partition(":")
            key = key.strip()
            val = val.strip()
            current[key] = val

    if current:
        entries.append(current)

    return entries


def seed() -> None:
    if not ATTRIBUTES_YML.exists():
        print(f"[seed_attributes] attributes.yml not found at {ATTRIBUTES_YML}")
        return

    lines = ATTRIBUTES_YML.read_text(encoding="utf-8").splitlines()
    raw_entries = parse_attributes(lines)
    print(f"[seed_attributes] Parsed {len(raw_entries)} attribute entries from YAML.")

    # Ensure tables exist
    Base.metadata.create_all(bind=engine)

    db: Session = SessionLocal()
    created = 0
    try:
        for raw in raw_entries:
            key = raw.get("id")
            if not key:
                continue
            existing = db.query(Attribute).filter_by(key=key).first()
            if existing:
                continue

            attr = Attribute(
                key=key,
                name=raw.get("name", key),
                category=None,  # Could be inferred later from canonical tree
                level=raw.get("level"),
                range=raw.get("range"),
                sources=raw.get("sources"),
                notes=raw.get("notes"),
                is_active=True,
                source_version="v2.6.3",
            )
            db.add(attr)
            created += 1

        db.commit()
    finally:
        db.close()

    print(f"[seed_attributes] Created {created} new Attribute rows.")


if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: backend/scripts/seed_tool_configs.py
----- CONTENT START -----
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy.orm import Session
from backend.database.core import SessionLocal, engine
from backend.models.config import ToolConfig, Base

def seed():
    # Ensure tables exist
    Base.metadata.create_all(bind=engine)
    db = SessionLocal()
    
    print("üå± Seeding Tool Configurations")
    
    tools = [
        # Segmentation Models
        {"name": "sam_vit_b", "type": "segmentation", "provider": "local", "cost_per_image": 0.0001, "settings": {"speed": "fast"}},
        {"name": "sam_vit_l", "type": "segmentation", "provider": "local", "cost_per_image": 0.0003, "settings": {"speed": "medium"}},
        {"name": "sam_vit_h", "type": "segmentation", "provider": "local", "cost_per_image": 0.0008, "settings": {"speed": "slow"}},
        
        # VLM Labeling Models
        {"name": "gemini-1.5-flash", "type": "labeling", "provider": "google", "cost_per_1k_tokens": 0.0001, "settings": {"context": "1m"}},
        {"name": "gpt-4-vision", "type": "labeling", "provider": "openai", "cost_per_1k_tokens": 0.01, "settings": {"context": "128k"}},
        {"name": "claude-3-sonnet", "type": "labeling", "provider": "anthropic", "cost_per_1k_tokens": 0.003, "settings": {"context": "200k"}},
        {"name": "deepseek-vl", "type": "labeling", "provider": "local", "cost_per_1k_tokens": 0.0, "settings": {"quantization": "4bit"}},
    ]

    for tool in tools:
        exists = db.query(ToolConfig).filter_by(name=tool["name"]).first()
        if not exists:
            t = ToolConfig(
                name=tool["name"],
                tool_type=tool["type"],
                provider=tool["provider"],
                cost_per_image=tool.get("cost_per_image", 0.0),
                cost_per_1k_tokens=tool.get("cost_per_1k_tokens", 0.0),
                settings=tool["settings"],
                is_enabled=True
            )
            db.add(t)
            print(f"  + Added {tool['name']}")
        else:
            print(f"  . Skipped {tool['name']} (Exists)")
            
    db.commit()
    db.close()
    print("‚úÖ Seeding Complete.")

if __name__ == "__main__":
    seed()----- CONTENT END -----
----- FILE PATH: backend/services/__init__.py
----- CONTENT START -----
# backend/services package----- CONTENT END -----
----- FILE PATH: backend/services/annotation.py
----- CONTENT START -----
from sqlalchemy.orm import Session
from sqlalchemy import select, func
from backend.database.core import Base
from backend.models.assets import Image, Region
from backend.models.annotation import Validation
from backend.schemas.annotation import ValidationRequest, RegionCreateRequest
import logging

logger = logging.getLogger("v3.services.annotation")

class AnnotationService:
    """
    Business Logic for the Tagger Workbench.
    Handles the 'Flow' state (getting the next image) and 'Persistence' (saving tags).
    """

    def __init__(self, db: Session):
        self.db = db

    def get_next_image_for_user(self, user_id: int) -> Image | None:
        """
        PRIORITY QUEUE LOGIC:
        1. Find images assigned to the user's current batch (if any).
        2. Fallback: Find images with FEWEST validations (to ensure coverage).
        3. Filter out images this user has already validated.
        """
        # Subquery: Images ID validated by THIS user
        validated_ids = select(Validation.image_id).where(Validation.user_id == user_id)

        # Main Query: Images NOT in subquery, ordered by validation count (asc)
        stmt = (
            select(Image)
            .outerjoin(Validation, Image.id == Validation.image_id)
            .where(Image.id.not_in(validated_ids))
            .group_by(Image.id)
            .order_by(func.count(Validation.id).asc())
            .limit(1)
        )
        
        result = self.db.execute(stmt).scalar_one_or_none()
        return result

    def create_validation(self, user_id: int, data: ValidationRequest) -> Validation:
        """
        Records a human judgment.
        """
        # TODO: Add check for existing validation to prevent duplicates?
        # For high-speed tagging, we might accept duplicates and filter later, 
        # or upsert. Here we append.
        
        new_val = Validation(
            user_id=user_id,
            image_id=data.image_id,
            attribute_key=data.attribute_key,
            value=data.value,
            duration_ms=data.duration_ms
        )
        
        self.db.add(new_val)
        self.db.commit()
        self.db.refresh(new_val)
        
        return new_val

    def create_region(self, user_id: int, data: RegionCreateRequest) -> Region:
        """
        Records a manual segmentation (bounding box/polygon).
        """
        new_region = Region(
            image_id=data.image_id,
            geometry=data.geometry,
            manual_label=data.manual_label,
            # We verify that the image exists via FK constraint in DB
        )
        
        self.db.add(new_region)
        self.db.commit()
        self.db.refresh(new_region)
        
        return new_region----- CONTENT END -----
----- FILE PATH: backend/services/auth.py
----- CONTENT START -----
import os
from typing import Literal, Optional
from fastapi import Depends, Header, HTTPException, status
from pydantic import BaseModel

Role = Literal["tagger", "scientist", "supervisor", "admin"]

# Security: Load Secret from ENV or default to a known dev key
# In production, this MUST be set via docker-compose
API_SECRET = os.getenv("API_SECRET", "dev_secret_key_change_me")

class CurrentUser(BaseModel):
    id: str
    role: Role

async def get_current_user(
    x_user_id: Optional[str] = Header(default=None, alias="X-User-Id"),
    x_user_role: Optional[str] = Header(default=None, alias="X-User-Role"),
    x_auth_token: Optional[str] = Header(default=None, alias="X-Auth-Token")
) -> CurrentUser:
    """
    Authenticated User Factory.
    
    Security Upgrade (v3.3):
    - Low privilege roles (tagger) can pass with just ID headers (internal trust).
    - High privilege roles (admin, supervisor) MUST provide the X-Auth-Token.
    """
    user_id = x_user_id or "1"
    role = (x_user_role or "tagger").lower()

    # Normalize role
    if role not in {"tagger", "scientist", "supervisor", "admin"}:
        role = "tagger"

    # RBAC Enforcement
    is_privileged = role in {"admin", "supervisor"}
    
    if is_privileged:
        if x_auth_token != API_SECRET:
            # Log this attempt in production
            print(f"SECURITY ALERT: Failed admin login attempt for user {user_id}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid Authentication Token for Privileged Role"
            )

    return CurrentUser(id=user_id, role=role)

def require_tagger(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    return user

def require_supervisor(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    if user.role not in {"supervisor", "admin"}:
        raise HTTPException(status_code=403, detail="Supervisor role required")
    return user

def require_admin(user: CurrentUser = Depends(get_current_user)) -> CurrentUser:
    if user.role != "admin":
        raise HTTPException(status_code=403, detail="Admin role required")
    return user
----- CONTENT END -----
----- FILE PATH: backend/services/costs.py
----- CONTENT START -----
"""Cost and usage accounting helpers.

Sprint 1 goal:
- Provide a truthful, queryable ledger of VLM usage so the Admin
  budget dashboard no longer relies on hard-coded placeholder numbers.
- Keep the API surface small and dependency-free so it is easy to
  extend to other tools later.
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from sqlalchemy import select, func

from backend.database.core import SessionLocal
from backend.models.usage import ToolUsage

logger = logging.getLogger(__name__)


def log_vlm_usage(
    provider: str,
    model_name: str,
    cost_usd: float,
    meta: Optional[Dict[str, Any]] = None,
) -> None:
    """Persist a single VLM usage record.

    This is intentionally fire-and-forget: any exception is logged and
    swallowed so that failures in the ledger never break the main
    request/response path.
    """
    db = SessionLocal()
    try:
        entry = ToolUsage(
            tool_name="vlm_analyze_image",
            provider=provider or "unknown",
            model_name=model_name or "unknown",
            cost_usd=float(cost_usd) if cost_usd is not None else 0.0,
            meta=meta or {},
        )
        db.add(entry)
        db.commit()
    except Exception as exc:  # pragma: no cover - defensive
        logger.error("Failed to log VLM usage: %s", exc)
        db.rollback()
    finally:
        db.close()


def get_total_spent() -> float:
    """Return the total recorded spend in USD.

    If anything goes wrong (e.g. table missing on a fresh DB),
    we log and return 0.0 rather than breaking the Admin UI.
    """
    db = SessionLocal()
    try:
        stmt = select(func.coalesce(func.sum(ToolUsage.cost_usd), 0.0))
        result = db.execute(stmt).scalar_one()
        return float(result or 0.0)
    except Exception as exc:  # pragma: no cover - defensive
        logger.error("Failed to compute total spend: %s", exc)
        return 0.0
    finally:
        db.close()
----- CONTENT END -----
----- FILE PATH: backend/services/query_builder.py
----- CONTENT START -----
from typing import Any, Dict, List, Optional, Union
from sqlalchemy.orm import Session
from sqlalchemy import and_

from backend.models.image import Image
from backend.models.attribute import Attribute
from backend.models.annotation import Validation


class QueryBuilder:
    """
    Query builder for Explorer search.

    Supported filters (keys in filters dict):
      - "text": str (simple substring match on Image.filename/storage_path/meta_data json)
      - "attribute_ids": List[int] (images validated positive for any of these attributes)
      - "min_score": float (minimum validation score threshold)
      - "has_meta": bool (images with non-null meta_data)
      - "created_after": datetime/date ISO string

    The primary public method is execute(filters, page, page_size).
    search_images(query) remains as a backward-compatible adapter.
    """

    def __init__(self, db: Session, user_id: Optional[int] = None):
        self.db = db
        self.user_id = user_id

    def execute(
        self,
        filters: Dict[str, Any],
        page: int = 1,
        page_size: int = 24,
    ) -> Dict[str, Any]:
        q = self.db.query(Image)

        clauses = []

        text = filters.get("text")
        if text:
            like = f"%{text}%"
            clauses.append(
                or_(
                    Image.filename.ilike(like),
                    Image.storage_path.ilike(like),
                    Image.meta_data.cast(String).ilike(like),
                )
            )

        attribute_ids = filters.get("attribute_ids") or []
        min_score = filters.get("min_score", 0.5)
        if attribute_ids:
            q = q.join(Validation).filter(
                Validation.attribute_id.in_(attribute_ids),
                Validation.value >= min_score,
            )

        if filters.get("has_meta") is True:
            clauses.append(Image.meta_data.isnot(None))

        if clauses:
            q = q.filter(and_(*clauses))

        total = q.count()
        items = (
            q.order_by(Image.id.desc())
            .offset((page - 1) * page_size)
            .limit(page_size)
            .all()
        )

        return {"items": items, "total": total}

    # ------------------------------------------------------------------
    # Backward-compatibility adapter for older router/frontend calls.
    # ------------------------------------------------------------------
    def search_images(self, query: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """
        Accepts either:
          - dict with keys {filters, page, page_size}
          - SearchQuery pydantic object with those fields.
        Delegates to execute().
        """
        if isinstance(query, dict):
            filters = query.get("filters", {}) or {}
            page = query.get("page", 1) or 1
            page_size = query.get("page_size", 24) or 24
        else:
            filters = getattr(query, "filters", {}) or {}
            page = getattr(query, "page", 1) or 1
            page_size = getattr(query, "page_size", 24) or 24
        return self.execute(filters=filters, page=page, page_size=page_size)----- CONTENT END -----
----- FILE PATH: backend/services/storage.py
----- CONTENT START -----
"""backend.services.storage

Image storage root discovery and path normalization.

This module centralizes the IMAGE_STORAGE_ROOT behavior so that:
  * Admin uploads, debug endpoints, and API URL builders agree on where images live.
  * The /static mount can safely expose only the image store.

Storage paths in the DB may be:
  1) absolute paths (legacy tests, manual seeding),
  2) relative paths already under the store (e.g., 'data_store/<uuid>.jpg'),
  3) bare filenames (preferred new behavior).

Public helpers:
  - get_image_storage_root(): Path to store (mkdir'ed).
  - resolve_image_path(storage_path): best-effort filesystem resolution.
  - to_static_path(storage_path): relative path used for /static/<...> URLs.
"""

from __future__ import annotations

import os
from pathlib import Path

DEFAULT_IMAGE_STORAGE_ROOT = "data_store"

def get_image_storage_root() -> Path:
    """Return the image storage root, ensuring it exists."""
    root = os.getenv("IMAGE_STORAGE_ROOT", DEFAULT_IMAGE_STORAGE_ROOT)
    p = Path(root)
    try:
        p.mkdir(parents=True, exist_ok=True)
    except Exception:
        # If mkdir fails (e.g., read-only FS), still return the path.
        pass
    return p


def resolve_image_path(storage_path: str) -> Path:
    """Resolve a DB storage_path to a real file on disk if possible."""
    raw = Path(storage_path)
    if raw.is_file():
        return raw

    root = get_image_storage_root()
    # If storage_path already includes the root prefix (relative)
    try:
        sp = str(raw).lstrip("/")
        root_str = str(root).strip("/")
        if sp.startswith(root_str + "/"):
            candidate = Path(sp)
            if candidate.is_file():
                return candidate
    except Exception:
        pass

    # Try root / raw.name (covers 'data_store/<name>' and absolute paths)
    candidate = root / raw.name
    if candidate.is_file():
        return candidate

    # Try root / raw (covers bare relative paths)
    candidate = root / raw
    if candidate.is_file():
        return candidate

    return raw  # best-effort


def to_static_path(storage_path: str) -> str:
    """Convert DB storage_path to a safe relative path for /static URLs."""
    root = get_image_storage_root()
    root_str = str(root).strip("/")
    sp = storage_path.lstrip("/")

    # If already rooted-relative like 'data_store/<uuid>.jpg', strip prefix.
    if root_str and sp.startswith(root_str + "/"):
        sp = sp[len(root_str) + 1 :]

    # If absolute, attempt to relativize to root; otherwise fall back to name.
    try:
        p = Path(storage_path)
        if p.is_absolute():
            try:
                sp = str(p.relative_to(root))
            except Exception:
                sp = p.name
    except Exception:
        pass

    return sp
----- CONTENT END -----
----- FILE PATH: backend/services/training_export.py
----- CONTENT START -----
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import select

from backend.models.assets import Image
from backend.models.annotation import Validation


class TrainingExporter:
    """Export human validations into a training dataset.

    This is the v3.2 counterpart to the v2.6.x TrainingDataExporter.
    It emits a list of JSON-serializable dictionaries that can be written
    as JSON or JSONL for fine-tuning / active learning loops.
    """
    def __init__(self, db: Session):
        self.db = db

    def export_for_images(self, image_ids: List[int]) -> List[Dict[str, Any]]:
        if not image_ids:
            return []

        stmt = (
            select(Validation, Image)
            .join(Image, Image.id == Validation.image_id)
            .where(Validation.image_id.in_(image_ids))
        )
        rows = self.db.execute(stmt).all()
        examples: List[Dict[str, Any]] = []

        for validation, image in rows:
            examples.append(
                {
                    "image_id": validation.image_id,
                    "image_filename": image.filename,
                    "attribute_key": validation.attribute_key,
                    "value": float(validation.value),
                    "user_id": validation.user_id,
                    "region_id": validation.region_id,
                    "duration_ms": validation.duration_ms,
                    "created_at": validation.created_at,
                    "source": "human_validation",
                }
            )

        return examples----- CONTENT END -----
----- FILE PATH: backend/services/upload_jobs.py
----- CONTENT START -----
"""Upload job orchestration helpers.

This module provides a minimal job abstraction on top of the existing
SciencePipeline so that large batches of images can be processed
asynchronously without holding an HTTP request open.

It intentionally avoids any task-queue dependency (Celery, RQ, etc.)
so that the same code can later be wired into a real worker
implementation without changing the public API.
"""

from __future__ import annotations

import logging
from typing import Iterable, List, Optional, Sequence, Tuple

from sqlalchemy.orm import Session

from backend.database.core import SessionLocal
from backend.models.jobs import UploadJob, UploadJobItem
from backend.models.assets import Image
from backend.science.pipeline import SciencePipeline, SciencePipelineConfig

logger = logging.getLogger(__name__)


UploadedRecord = Tuple[int, str, str]  # (image_id, storage_path, original_filename)


def create_upload_job_for_images(
    db: Session,
    user_id: Optional[int],
    records: Sequence[UploadedRecord],
) -> UploadJob:
    """Persist an UploadJob + UploadJobItems for the given images.

    Parameters
    ----------
    db:
        Open SQLAlchemy session.
    user_id:
        Optional admin user id who initiated the upload.
    records:
        Sequence of (image_id, storage_path, original_filename).
    """
    job = UploadJob(
        created_by_id=user_id,
        status="PENDING",
        total_items=len(records),
        completed_items=0,
        failed_items=0,
    )
    db.add(job)
    db.flush()

    for image_id, storage_path, original_name in records:
        item = UploadJobItem(
            job_id=job.id,
            image_id=image_id,
            filename=original_name,
            storage_path=storage_path,
            status="PENDING",
        )
        db.add(item)

    db.commit()
    db.refresh(job)
    logger.info("Created UploadJob %s with %d items", job.id, len(records))
    return job


def _run_upload_job_inner(session: Session, job_id: int) -> None:
    job = session.query(UploadJob).get(job_id)
    if not job:
        logger.warning("UploadJob %s not found; nothing to run.", job_id)
        return

    if not job.items:
        job.status = "COMPLETED"
        session.commit()
        logger.info("UploadJob %s has no items; marking as COMPLETED.", job_id)
        return

    job.status = "RUNNING"
    session.commit()

    # Science pipeline is instantiated once per job to reuse analyzers.
    config = SciencePipelineConfig(enable_all=True)
    pipeline = SciencePipeline(config=config, db=session)

    completed = 0
    failed = 0

    for item in list(job.items):
        if item.status not in ("PENDING", "RUNNING"):
            # Skip items that were already processed.
            continue

        item.status = "RUNNING"
        session.commit()

        try:
            ok = False
            if item.image_id is not None:
                ok = pipeline.process_image(item.image_id)
            else:
                logger.warning(
                    "UploadJobItem %s has no image_id; marking as FAILED.", item.id
                )

            if ok:
                item.status = "COMPLETED"
                completed += 1
            else:
                item.status = "FAILED"
                failed += 1

        except Exception as exc:  # pragma: no cover - defensive
            logger.exception(
                "Science pipeline failed for UploadJobItem %s: %s", item.id, exc
            )
            item.status = "FAILED"
            item.error_message = str(exc)
            failed += 1

        job.completed_items = completed
        job.failed_items = failed
        session.commit()

    if failed and not completed:
        job.status = "FAILED"
    elif failed:
        job.status = "COMPLETED_WITH_ERRORS"
    else:
        job.status = "COMPLETED"

    session.commit()
    logger.info(
        "UploadJob %s finished. completed=%d failed=%d",
        job.id,
        completed,
        failed,
    )


def run_upload_job(job_id: int) -> None:
    """Entry point for background workers.

    This function owns its own SessionLocal and is suitable for use with
    FastAPI's BackgroundTasks or an external worker process.
    """
    session = SessionLocal()
    try:
        _run_upload_job_inner(session, job_id)
    finally:
        session.close()
----- CONTENT END -----
----- FILE PATH: backend/services/vlm.py
----- CONTENT START -----

"""Unified Visual Language Model (VLM) service.

This module abstracts different VLM providers (OpenAI, Anthropic, Gemini)
behind a single interface so the science pipeline can call a single
`get_vlm_engine()` entry point.

Design goals:
- Keep the pipeline code simple and synchronous.
- Prefer environment variables for API keys.
- Allow a lightweight runtime preference via a small config file.
- Fall back to a Stub engine that returns neutral placeholders when
  no keys are configured (useful for classrooms and tests).
"""

from __future__ import annotations

import base64
import json
import logging
import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)

# Optional SDK imports. These may not be installed in all environments;
# the Docker backend image installs them explicitly.
try:  # OpenAI >= 1.x
    import openai  # type: ignore
except Exception:  # pragma: no cover - optional
    openai = None  # type: ignore

try:  # Anthropic
    import anthropic  # type: ignore
except Exception:  # pragma: no cover - optional
    anthropic = None  # type: ignore

try:  # Gemini / Google Generative AI
    import google.generativeai as genai  # type: ignore
except Exception:  # pragma: no cover - optional
    genai = None  # type: ignore

# Where we store a tiny bit of runtime configuration.
_CONFIG_PATH = Path(__file__).resolve().parents[1] / "data" / "vlm_config.json"


def _safe_json_loads(raw: str) -> Dict[str, Any]:
    """Parse JSON from a VLM response with light, conservative repair.

    This helper is more forgiving than a single `json.loads` call, which can
    fail when a model wraps JSON in extra commentary or Markdown fences.

    Strategy:
    - Strip surrounding whitespace.
    - If code fences are present, extract the fenced block.
    - Try `json.loads` on the cleaned string.
    - On failure, look for the first '{' and last '}' and try that span.
    - If parsing still fails, re-raise the JSONDecodeError so callers see
      a clear failure rather than a silent mis-parse.

    This is intentionally conservative and does *not* attempt arbitrary
    "JSON repair"; it only handles the most common wrapping patterns.
    """
    cleaned = raw.strip()
    # Handle ```json or generic ``` fences.
    if "```json" in cleaned:
        try:
            cleaned = cleaned.split("```json", 1)[1].split("```", 1)[0]
        except Exception:
            cleaned = cleaned.replace("```json", "").replace("```", "")
    elif "```" in cleaned:
        try:
            cleaned = cleaned.split("```", 1)[1].split("```", 1)[0]
        except Exception:
            cleaned = cleaned.replace("```", "")

    try:
        return json.loads(cleaned)
    except json.JSONDecodeError:
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            return json.loads(snippet)
        # Re-raise original error if we cannot find a plausible JSON snippet.
        raise


class VLMEngine(ABC):
    """Abstract base class for a Visual Language Model provider."""

    @abstractmethod
    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        """Return a JSON-serializable dict for the given image + prompt."""
        raise NotImplementedError

    @staticmethod
    def _encode_image(image_bytes: bytes) -> str:
        return base64.b64encode(image_bytes).decode("utf-8")


class StubEngine(VLMEngine):
    """Fallback engine when no API keys are configured.

    This is deliberately boring: it returns a small JSON stub so downstream
    code has the right shape but no one mistakes it for real analysis.
    """

    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        logger.warning(
            "VLM StubEngine invoked (no real VLM provider configured). "
            "Returning placeholder values."
        )
        return {
            "stub": True,
            "note": (
                "This is dummy data from StubEngine. "
                "Configure GEMINI_API_KEY, OPENAI_API_KEY, or ANTHROPIC_API_KEY "
                "to enable real cognitive analysis."
            ),
        }


class OpenAIEngine(VLMEngine):
    """Adapter for OpenAI GPT-4o / GPT-4o-mini vision."""

    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        if openai is None:
            raise RuntimeError("openai package is not installed in this environment.")
        # OpenAI 1.x client
        self._client = openai.OpenAI(api_key=api_key)  # type: ignore[attr-defined]
        self._model = model

    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        b64_image = self._encode_image(image_bytes)
        try:
            response = self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt + " Return ONLY valid JSON.",
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{b64_image}"
                                },
                            },
                        ],
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=500,
            )
            raw_content = response.choices[0].message.content  # type: ignore[index]
            return _safe_json_loads(raw_content)
        except Exception as exc:  # pragma: no cover - network
            logger.error("OpenAI VLM error: %s", exc)
            raise


class AnthropicEngine(VLMEngine):
    """Adapter for Anthropic Claude 3.5 Sonnet."""

    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20240620"):
        if anthropic is None:
            raise RuntimeError(
                "anthropic package is not installed in this environment."
            )
        self._client = anthropic.Anthropic(api_key=api_key)  # type: ignore[attr-defined]
        self._model = model

    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        b64_image = self._encode_image(image_bytes)
        try:
            response = self._client.messages.create(
                model=self._model,
                max_tokens=1024,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": b64_image,
                                },
                            },
                            {
                                "type": "text",
                                "text": prompt
                                + " Return strictly JSON (machine readable).",
                            },
                        ],
                    }
                ],
            )
            # Claude does not yet have strict JSON mode; parse text content.
            raw = response.content[0].text  # type: ignore[index]
            if "```json" in raw:
                raw = raw.split("```json", 1)[1].split("```", 1)[0]
            return _safe_json_loads(raw)
        except Exception as exc:  # pragma: no cover - network
            logger.error("Anthropic VLM error: %s", exc)
            raise


class GeminiEngine(VLMEngine):
    """Adapter for Google Gemini (e.g. 1.5 Flash / Pro) vision models."""

    def __init__(self, api_key: str, model: str = "gemini-1.5-flash"):
        if genai is None:
            raise RuntimeError(
                "google.generativeai package is not installed in this environment."
            )
        # Configure client; library will often also look at GEMINI_API_KEY env,
        # but we pass explicitly for clarity.
        try:
            # Newer SDK style
            from google import genai as google_genai  # type: ignore

            self._client = google_genai.Client(api_key=api_key)
            self._model_name = model
            self._mode = "client"
        except Exception:  # pragma: no cover - fallback to older style
            genai.configure(api_key=api_key)  # type: ignore[call-arg]
            self._model = genai.GenerativeModel(model)  # type: ignore[attr-defined]
            self._mode = "legacy"

    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        try:
            if getattr(self, "_mode", "legacy") == "client":
                # Newer client style
                from google.genai.types import Part  # type: ignore

                image_part = Part.from_bytes(
                    data=image_bytes, mime_type="image/jpeg"
                )
                response = self._client.models.generate_content(
                    model=self._model_name,
                    contents=[prompt, image_part],
                )
                raw = response.text  # type: ignore[attr-defined]
            else:
                # Older google.generativeai style
                response = self._model.generate_content(  # type: ignore[attr-defined]
                    [
                        {"mime_type": "image/jpeg", "data": image_bytes},
                        prompt,
                    ]
                )
                raw = response.text  # type: ignore[attr-defined]

            if "```json" in raw:
                raw = raw.split("```json", 1)[1].split("```", 1)[0]
            return _safe_json_loads(raw)
        except Exception as exc:  # pragma: no cover - network
            logger.error("Gemini VLM error: %s", exc)
            raise


def _detect_available_backends() -> Dict[str, bool]:
    """Return which providers appear to be available based on API keys."""
    # Gemini can use GEMINI_API_KEY or GOOGLE_API_KEY depending on setup.
    gemini_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    return {
        "gemini": bool(gemini_key),
        "openai": bool(os.getenv("OPENAI_API_KEY")),
        "anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
    }


def _load_config() -> Dict[str, Any]:
    if _CONFIG_PATH.exists():
        try:
            return json.loads(_CONFIG_PATH.read_text())
        except Exception as exc:  # pragma: no cover - defensive
            logger.warning("Failed to read VLM config file: %s", exc)
    return {}


def _save_config(cfg: Dict[str, Any]) -> None:
    _CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)
    _CONFIG_PATH.write_text(json.dumps(cfg, indent=2))


def _resolve_provider(config_provider: Optional[str] = None,
                      override: Optional[str] = None) -> str:
    """Decide which provider string to use.

    Preference order:
    1. Explicit override argument (e.g. from /vlm/test payload).
    2. Config file provider (set via Admin UI).
    3. VLM_PROVIDER environment variable.
    4. Auto-detect based on available keys, preferring Gemini > OpenAI > Anthropic.
    5. Fallback to "stub".
    """
    # Normalise inputs
    if override:
        override = override.lower()
    if config_provider:
        config_provider = config_provider.lower()

    if override and override != "auto":
        return override

    if config_provider and config_provider != "auto":
        return config_provider

    env_provider = os.getenv("VLM_PROVIDER")
    if env_provider and env_provider.lower() != "auto":
        return env_provider.lower()

    # Auto-detect
    avail = _detect_available_backends()
    if avail.get("gemini"):
        return "gemini"
    if avail.get("openai"):
        return "openai"
    if avail.get("anthropic"):
        return "anthropic"
    return "stub"


def get_vlm_engine(provider_override: Optional[str] = None) -> VLMEngine:
    """Factory for a VLMEngine instance.

    provider_override can be None (use config/env), "auto", or one of:
    "gemini", "openai", "anthropic", "stub".
    """
    cfg = _load_config()
    config_provider = cfg.get("provider")
    provider = _resolve_provider(config_provider, provider_override)

    avail = _detect_available_backends()

    if provider == "openai" and avail.get("openai"):
        key = os.getenv("OPENAI_API_KEY")
        return OpenAIEngine(key)  # type: ignore[arg-type]
    if provider == "anthropic" and avail.get("anthropic"):
        key = os.getenv("ANTHROPIC_API_KEY")
        return AnthropicEngine(key)  # type: ignore[arg-type]
    if provider == "gemini" and avail.get("gemini"):
        key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        return GeminiEngine(key)  # type: ignore[arg-type]

    # If we reach here, fall back to stub.
    logger.info(
        "VLM falling back to StubEngine (provider=%s, avail=%s)",
        provider,
        avail,
    )
    return StubEngine()


def describe_vlm_configuration() -> Dict[str, Any]:
    """Return a small JSON-ready summary for the Admin cockpit.

    In addition to the active provider and detected backends, this also exposes
    optional ergonomics settings used by the scientist-facing tools:
    - cognitive_prompt_override: optional override for the cognitive/affective prompt
    - max_batch_size: soft limit for recommended images per batch
    - cost_per_1k_images_usd: rough cost estimate per 1000 images
    """
    cfg = _load_config()
    provider = cfg.get("provider", "auto")
    avail = _detect_available_backends()
    engine = get_vlm_engine()
    return {
        "provider": provider,
        "engine": type(engine).__name__,
        "available_backends": avail,
        "cognitive_prompt_override": cfg.get("cognitive_prompt_override") or "",
        "max_batch_size": cfg.get("max_batch_size"),
        "cost_per_1k_images_usd": cfg.get("cost_per_1k_images_usd"),
    }


def update_vlm_config(
    provider: Optional[str] = None,
    cognitive_prompt_override: Optional[str] = None,
    max_batch_size: Optional[int] = None,
    cost_per_1k_images_usd: Optional[float] = None,
) -> Dict[str, Any]:
    """Update the VLM configuration and return the updated description.

    This is intentionally forgiving:
    - empty / whitespace-only prompt clears any existing override
    - non-positive batch size or cost values clear those fields
    """
    cfg = _load_config()

    if provider is not None:
        cfg["provider"] = (provider or "auto").lower()

    if cognitive_prompt_override is not None:
        text = cognitive_prompt_override.strip()
        if text:
            cfg["cognitive_prompt_override"] = text
        else:
            cfg.pop("cognitive_prompt_override", None)

    if max_batch_size is not None:
        try:
            value = int(max_batch_size)
        except (TypeError, ValueError):
            value = None
        if value and value > 0:
            cfg["max_batch_size"] = value
        else:
            cfg.pop("max_batch_size", None)

    if cost_per_1k_images_usd is not None:
        try:
            value = float(cost_per_1k_images_usd)
        except (TypeError, ValueError):
            value = None
        if value and value > 0:
            cfg["cost_per_1k_images_usd"] = value
        else:
            cfg.pop("cost_per_1k_images_usd", None)

    _save_config(cfg)
    return describe_vlm_configuration()




def get_cognitive_prompt(base_prompt: str) -> str:
    """Return the effective cognitive/affective prompt.

    If the admin has configured a cognitive_prompt_override, that text is used;
    otherwise the provided base_prompt is returned unchanged.
    """
    cfg = _load_config()
    override = cfg.get("cognitive_prompt_override")
    if isinstance(override, str) and override.strip():
        return override
    return base_prompt


def set_configured_provider(provider: str) -> Dict[str, Any]:
    """Persist only the chosen provider and return the updated description.

    This is kept for backwards compatibility; the Admin cockpit now prefers
    update_vlm_config so it can also configure prompt overrides and batch size.
    """
    return update_vlm_config(provider=provider or "auto")

----- CONTENT END -----
----- FILE PATH: backend/tests/test_rbac.py
----- CONTENT START -----
"""
RBAC enforcement tests for Admin endpoints.
These tests avoid external DB dependencies by overriding get_db to in-memory SQLite.
"""

from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from backend.main import app
from backend.database.core import Base, get_db


# In-memory SQLite for tests
engine = create_engine("sqlite:///:memory:", connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def override_get_db():
    Base.metadata.create_all(bind=engine)
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.close()


app.dependency_overrides[get_db] = override_get_db
client = TestClient(app)


def test_admin_endpoint_forbidden_without_admin_header():
    # No headers ‚Üí require_admin should reject
    r = client.get("/v1/admin/models")
    assert r.status_code in (401, 403)


def test_admin_endpoint_allows_admin_header():
    r = client.get("/v1/admin/models", headers={"X-User-Role": "admin"})
    assert r.status_code == 200----- CONTENT END -----
----- FILE PATH: contracts/attributes.yml
----- CONTENT START -----
schema:
  - id: string
  - name: string
  - level: string
  - range: string
  - sources: {cv: bool, vlm: bool}
  - notes: string
attributes:
  - id: geometry.curvilinearity
    name: Curvilinearity
    level: visible
    range: continuous
    sources: {cv: true, vlm: false}
    notes: Ratio of curved to straight contours
  - id: clutter.density
    name: Visual clutter density
    level: visible
    range: continuous
    sources: {cv: true, vlm: false}
    notes: Entropy/keypoint density normalized by area
  - id: lighting.daylight_evidence
    name: Daylight evidence
    level: inferred
    range: prob
    sources: {cv: true, vlm: true}
    notes: Windows/sky cues and VLM judgement
  - id: privacy.enclosure
    name: Enclosure (refuge)
    level: inferred
    range: prob
    sources: {cv: true, vlm: true}
    notes: Occluders, alcoves, high-back seating
  - id: privacy.prospect
    name: Prospect (openness)
    level: inferred
    range: prob
    sources: {cv: true, vlm: true}
    notes: Long sightlines, low occlusion
  - id: greenery.plants_presence
    name: Plants presence
    level: visible
    range: prob
    sources: {cv: true, vlm: true}
    notes: Green hues with leaf-like textures or VLM confirmation

# --- Extended Attribute Set (added 2025-11-10T21:50:15.811362Z) ---
# XIII. Materiality and Authenticity Indicators
- id: materials.construction_legibility
  name: Construction legibility
  level: visible
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Visibility of how elements are made/assembled; junction readability, exposed joinery

- id: materials.finish_honesty
  name: Finish honesty
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Alignment between apparent material expression and substrate (avoid faux finishes)

- id: materials.craft_evidence
  name: Craft evidence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Signs of handwork vs industrial production (tool marks, irregularities)

- id: materials.repair_visibility
  name: Repair visibility
  level: visible
  range: prob
  sources: {cv: true, vlm: true}
  notes: Evidence of maintenance and care over time (patches, inlays)

# Patina and Temporal Depth
- id: aging.desirable_patina
  name: Desirable patina
  level: visible
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Aesthetically valued wear vs degradation

- id: temporal.layer_visibility
  name: Layer visibility
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Discernible historical strata or modifications

# XIV. Embodied Energy and Ecological Cues
- id: sustainability.recycled_material_evidence
  name: Recycled material evidence
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Visible reuse/upcycling markers

- id: sustainability.low_impact_indicators
  name: Low-impact indicators
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Minimal processing cues; low embodied energy signatures

- id: sustainability.local_material_signatures
  name: Local material signatures
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Vernacular/locally sourced materials

- id: ecology.carbon_sequestration_visible
  name: Visible carbon sequestration
  level: visible
  range: prob
  sources: {cv: true, vlm: true}
  notes: Presence of living systems/biomass with sequestration potential

# XV. Phenomenological Qualities
- id: atmosphere.intimacy_scale
  name: Intimacy scale
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Psychological closeness vs monumentality

- id: atmosphere.gravity_vs_levity
  name: Gravity vs levity
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Heavy/grounded vs light/floating character

- id: atmosphere.opacity_gradient
  name: Opacity gradient
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Visual penetration depth, layering, translucency gradations

# Sensory Synesthesia Potential
- id: synesthesia.temperature_color_correspondence
  name: Temperature‚Äìcolor correspondence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Visual warmth matching thermal expectation

- id: synesthesia.acoustic_visual_match
  name: Acoustic‚Äìvisual match
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Expected sound quality inferred from visible materials/forms

- id: synesthesia.haptic_visual_correspondence
  name: Haptic‚Äìvisual correspondence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Touch expectation from appearance (roughness/softness cues)

# XVI. Control and Agency Indicators
- id: control.adjustable_element_density
  name: Adjustable element density
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Operable windows, shades, dimmers, vents per area

- id: control.modification_affordances
  name: Modification affordances
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Moveable furniture, personalizable surfaces, modular components

- id: control.microclimate_zones
  name: Microclimate zones
  level: inferred
  range: count
  sources: {cv: true, vlm: true}
  notes: Distinct controllable thermal/light regions

- id: agency.responsive_feedback
  name: Responsive feedback
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Visible environmental response to user actions (lights dimming, louvers moving)

# XVII. Topological and Network Properties
- id: topology.integration_value
  name: Topological integration value
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Accessibility from other spaces (space syntax/visibility graph proxies)

- id: topology.choice_coefficient
  name: Topological choice coefficient
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Likelihood of passing through (betweenness centrality proxies)

- id: network.visual_graph_connectivity
  name: Visual graph connectivity
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Inter-visibility network density from visibility graph analysis

# XVIII. Scale and Proportion Relations
- id: scale.body_part_resonance
  name: Body-part resonance
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Elements matching hand/arm/body dimensions

- id: proportion.golden_ratio_proximity
  name: Golden-ratio proximity
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Deviation from œÜ relationships in salient rectangles/triangles

- id: proportion.harmonic_intervals
  name: Harmonic intervals (proportion)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Prevalence of musical ratio correspondences (3:2, 4:3, etc.)

- id: scale.magnitude_range
  name: Magnitude range
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Ratio of largest to smallest perceptible elements

- id: scale.transition_smoothness
  name: Transition smoothness (scale)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Gradual vs abrupt size changes across hierarchies

# XIX. Information Gradient and Surprise
- id: information.conditional_entropy
  name: Conditional entropy (view sequence)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Uncertainty reduction as views unfold (information-theoretic measure)

- id: surprise.expectation_violation
  name: Expectation violation
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Schema-inconsistent elements quantified vs local priors

- id: mystery.progressive_disclosure
  name: Progressive disclosure (mystery)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Promise of further information (partial occlusion, curving paths)

# XX. Historical and Cultural Legibility
- id: narrative.chronological_layering
  name: Chronological layering (temporal narrative)
  level: inferred
  range: ordinal
  sources: {cv: true, vlm: true}
  notes: Sequential historical evidence readable in the environment

- id: cultural.vernacular_vocabulary_strength
  name: Vernacular vocabulary strength
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Degree of local architectural language adherence

# --- Extended Attribute Set (added 2025-11-10T21:50:15.811958Z) ---
# XIII. Materiality and Authenticity Indicators
- id: materials.construction_legibility
  name: Construction legibility
  level: visible
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Visibility of how elements are made/assembled; junction readability, exposed joinery

- id: materials.finish_honesty
  name: Finish honesty
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Alignment between apparent material expression and substrate (avoid faux finishes)

- id: materials.craft_evidence
  name: Craft evidence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Signs of handwork vs industrial production (tool marks, irregularities)

- id: materials.repair_visibility
  name: Repair visibility
  level: visible
  range: prob
  sources: {cv: true, vlm: true}
  notes: Evidence of maintenance and care over time (patches, inlays)

# Patina and Temporal Depth
- id: aging.desirable_patina
  name: Desirable patina
  level: visible
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Aesthetically valued wear vs degradation

- id: temporal.layer_visibility
  name: Layer visibility
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Discernible historical strata or modifications

# XIV. Embodied Energy and Ecological Cues
- id: sustainability.recycled_material_evidence
  name: Recycled material evidence
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Visible reuse/upcycling markers

- id: sustainability.low_impact_indicators
  name: Low-impact indicators
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Minimal processing cues; low embodied energy signatures

- id: sustainability.local_material_signatures
  name: Local material signatures
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Vernacular/locally sourced materials

- id: ecology.carbon_sequestration_visible
  name: Visible carbon sequestration
  level: visible
  range: prob
  sources: {cv: true, vlm: true}
  notes: Presence of living systems/biomass with sequestration potential

# XV. Phenomenological Qualities
- id: atmosphere.intimacy_scale
  name: Intimacy scale
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Psychological closeness vs monumentality

- id: atmosphere.gravity_vs_levity
  name: Gravity vs levity
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Heavy/grounded vs light/floating character

- id: atmosphere.opacity_gradient
  name: Opacity gradient
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Visual penetration depth, layering, translucency gradations

# Sensory Synesthesia Potential
- id: synesthesia.temperature_color_correspondence
  name: Temperature‚Äìcolor correspondence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Visual warmth matching thermal expectation

- id: synesthesia.acoustic_visual_match
  name: Acoustic‚Äìvisual match
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Expected sound quality inferred from visible materials/forms

- id: synesthesia.haptic_visual_correspondence
  name: Haptic‚Äìvisual correspondence
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Touch expectation from appearance (roughness/softness cues)

# XVI. Control and Agency Indicators
- id: control.adjustable_element_density
  name: Adjustable element density
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Operable windows, shades, dimmers, vents per area

- id: control.modification_affordances
  name: Modification affordances
  level: visible
  range: count
  sources: {cv: true, vlm: true}
  notes: Moveable furniture, personalizable surfaces, modular components

- id: control.microclimate_zones
  name: Microclimate zones
  level: inferred
  range: count
  sources: {cv: true, vlm: true}
  notes: Distinct controllable thermal/light regions

- id: agency.responsive_feedback
  name: Responsive feedback
  level: inferred
  range: prob
  sources: {cv: true, vlm: true}
  notes: Visible environmental response to user actions (lights dimming, louvers moving)

# XVII. Topological and Network Properties
- id: topology.integration_value
  name: Topological integration value
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Accessibility from other spaces (space syntax/visibility graph proxies)

- id: topology.choice_coefficient
  name: Topological choice coefficient
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Likelihood of passing through (betweenness centrality proxies)

- id: network.visual_graph_connectivity
  name: Visual graph connectivity
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Inter-visibility network density from visibility graph analysis

# XVIII. Scale and Proportion Relations
- id: scale.body_part_resonance
  name: Body-part resonance
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Elements matching hand/arm/body dimensions

- id: proportion.golden_ratio_proximity
  name: Golden-ratio proximity
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Deviation from œÜ relationships in salient rectangles/triangles

- id: proportion.harmonic_intervals
  name: Harmonic intervals (proportion)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Prevalence of musical ratio correspondences (3:2, 4:3, etc.)

- id: scale.magnitude_range
  name: Magnitude range
  level: visible
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Ratio of largest to smallest perceptible elements

- id: scale.transition_smoothness
  name: Transition smoothness (scale)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: false}
  notes: Gradual vs abrupt size changes across hierarchies

# XIX. Information Gradient and Surprise
- id: information.conditional_entropy
  name: Conditional entropy (view sequence)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Uncertainty reduction as views unfold (information-theoretic measure)

- id: surprise.expectation_violation
  name: Expectation violation
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Schema-inconsistent elements quantified vs local priors

- id: mystery.progressive_disclosure
  name: Progressive disclosure (mystery)
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Promise of further information (partial occlusion, curving paths)

# XX. Historical and Cultural Legibility
- id: narrative.chronological_layering
  name: Chronological layering (temporal narrative)
  level: inferred
  range: ordinal
  sources: {cv: true, vlm: true}
  notes: Sequential historical evidence readable in the environment

- id: cultural.vernacular_vocabulary_strength
  name: Vernacular vocabulary strength
  level: inferred
  range: continuous
  sources: {cv: true, vlm: true}
  notes: Degree of local architectural language adherence----- CONTENT END -----
----- FILE PATH: deploy/Dockerfile.backend
----- CONTENT START -----
# Use official Python runtime
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (Required for OpenCV and Postgres)
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libpq-dev \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
# We install directly to keep the container self-contained and explicit.
RUN pip install --no-cache-dir \
    fastapi==0.109.0 \
    uvicorn==0.27.0 \
    sqlalchemy==2.0.25 \
    psycopg2-binary==2.9.9 \
    pydantic==2.6.1 \
    pydantic-settings==2.1.0 \
    python-jose[cryptography]==3.3.0 \
    passlib[bcrypt]==1.7.4 \
    python-multipart==0.0.7 \
    pillow==10.2.0 \
    numpy==1.26.3 \
    opencv-python-headless==4.9.0.80 \
    scipy==1.11.4 \
    scikit-image==0.22.0 \
    requests==2.31.0 \
    PyYAML==6.0.1 \
    pytest==7.4.4 \
    httpx==0.26.0 \
    openai==1.12.0 \
    anthropic==0.33.0 \
    google-generativeai==0.8.3 \
    google-genai==0.1.0

# Copy the backend code and tests
COPY backend /app/backend
COPY scripts /app/scripts
COPY tests /app/tests

# Expose port (internal only, Nginx routes to this)
EXPOSE 8000

# Startup Command
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]----- CONTENT END -----
----- FILE PATH: deploy/Dockerfile.frontend
----- CONTENT START -----
# Stage 1: Build the React Apps
FROM node:18-alpine as builder

WORKDIR /app

# Copy package config
COPY frontend/package.json frontend/package-lock.json* ./
# Copy workspace configs
COPY frontend/apps ./frontend/apps
COPY frontend/shared ./frontend/shared
COPY frontend/vite.config.base.js ./
COPY frontend/tailwind.config.js ./
COPY frontend/postcss.config.js ./

# Install dependencies for the entire monorepo
WORKDIR /app/frontend
RUN npm install

# Build all 4 apps
# This runs the 'build' script in package.json which builds each workspace
RUN npm run build

# Stage 2: Serve with Nginx
FROM nginx:alpine

# Remove default nginx static assets
RUN rm -rf /usr/share/nginx/html/*

# Copy built assets from builder stage to Nginx
# We map them to subdirectories for routing
COPY --from=builder /app/dist/workbench /usr/share/nginx/html/workbench
COPY --from=builder /app/dist/monitor /usr/share/nginx/html/monitor
COPY --from=builder /app/dist/admin /usr/share/nginx/html/admin
COPY --from=builder /app/dist/explorer /usr/share/nginx/html/explorer

# Copy Nginx Configuration
COPY deploy/nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]----- CONTENT END -----
----- FILE PATH: deploy/docker-compose.yml
----- CONTENT START -----
version: '3.8'

services:
  # 1. The Source of Truth (Database)
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      POSTGRES_USER: tagger
      POSTGRES_PASSWORD: tagger_pass
      POSTGRES_DB: image_tagger_v3
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - tagger-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tagger"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. The Business Logic (Backend API)
  api:
    build:
      context: ../
      dockerfile: deploy/Dockerfile.backend
    restart: always
    environment:
      DATABASE_URL: postgresql://tagger:tagger_pass@db:5432/image_tagger_v3
      SECRET_KEY: "production_secret_key_change_me_in_prod"
      # Wait for DB to be healthy before starting
    depends_on:
      db:
        condition: service_healthy
    networks:
      - tagger-net
    volumes:
      # Mount the old v2 logic folder so we don't have to copy it into the docker image manually
      - ../backend/image_tagger:/app/backend/image_tagger

  # 3. The User Interface (Frontend Gateway)
  frontend:
    build:
      context: ../
      dockerfile: deploy/Dockerfile.frontend
    restart: always
    ports:
      - "8080:80" # The Single Entry Point for all users
    depends_on:
      - api
    networks:
      - tagger-net

volumes:
  postgres_data:

networks:
  tagger-net:
    driver: bridge----- CONTENT END -----
----- FILE PATH: deploy/nginx.conf
----- CONTENT START -----
server {
    listen 80;
    server_name localhost;

    # --- API Proxy ---
    # Routes all /api traffic to the Python backend
    location /api/ {
        proxy_pass http://api:8000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # --- Static Files (Images) ---
    # Proxies image requests to the backend's static mount
    location /static/ {
        proxy_pass http://api:8000/static/;
    }

    # --- GUI Routes ---
    # Routes to the specific React build for each role

    # 1. Tagger Workbench
    location /workbench/ {
        alias /usr/share/nginx/html/workbench/;
        try_files $uri $uri/ /index.html;
    }

    # 2. Supervisor Monitor
    location /monitor/ {
        alias /usr/share/nginx/html/monitor/;
        try_files $uri $uri/ /index.html;
    }

    # 3. Admin Cockpit
    location /admin/ {
        alias /usr/share/nginx/html/admin/;
        try_files $uri $uri/ /index.html;
    }

    # 4. Research Explorer
    location /explorer/ {
        alias /usr/share/nginx/html/explorer/;
        try_files $uri $uri/ /index.html;
    }

    # Root Redirect (Default to Explorer)
    location / {
        return 301 /explorer/;
    }
}----- CONTENT END -----
----- FILE PATH: docs/ACTIVE_LEARNING_PIPELINE.md
----- CONTENT START -----
# Active Learning Pipeline v3.0

This document describes the workflow for refining AI models using human validation data.

## 1. The Loop
1.  **Inference**: System predicts labels (e.g., "Modern: 0.9") using `gemini-1.5-flash`.
2.  **Correction**: Human tagger in Workbench changes value to "Modern: 0.4".
3.  **Capture**: System flags this Region/Image as `is_training_candidate=True`.
4.  **Export**: Admin exports candidates via `POST /api/training/export`.
5.  **Fine-Tuning**: Data is sent to OpenAI/Google fine-tuning endpoint.
6.  **Deployment**: New model ID is updated in `ToolConfig` via the Admin Panel.

## 2. Export Format (JSONL)
The system exports data in standard JSONL format for fine-tuning:
`{"messages": [{"role": "user", "content": [IMAGE]}, {"role": "assistant", "content": "Label"}]}`

## 3. Triggering a Training Run
Use the Admin API or CLI:
`curl -X POST http://localhost:8000/api/v1/admin/training/export -d '{"min_quality": "high"}'`----- CONTENT END -----
----- FILE PATH: docs/AI_COLLAB_WORKFLOW.md
----- CONTENT START -----
# AI Collaboration Workflow (Image Tagger v3)

This repository is often developed in collaboration with large language models
(LLMs) such as ChatGPT, Claude, and Gemini. To keep the history sane and the
system reproducible, we follow these conventions:

## 1. Deliverables from AI

When asking an AI to modify this repo, do not request patches or partial diffs.
Instead, always request:

1. A full ZIP of the repository at the new version.
2. A single concatenated TXT that:
   - Contains all repo files (excluding `__pycache__`),
   - Uses clear file markers (e.g. `----- FILE PATH: ...`),
   - Includes a small `deconcat.py` helper at the top that can reconstruct the
     directory tree from the TXT alone.

These artifacts should be versioned, e.g.:

- `Image_Tagger_v3.2.xx_*.zip`
- `Image_Tagger_v3.2.xx_*.txt`

## 2. Non-destructive updates

AI tools must not delete files. If something needs to be replaced:

- Archive the previous version under `archive/<version_phase>/...`
- Write the new version in-place.

## 3. Guardian + Governance

Any AI-driven code change should respect:

- `v3_governance.yml` for:
  - `protected_scopes`
  - `critical_files`
  - `constraints`
- `scripts/guardian.py` for:
  - `freeze` ‚Üí updating `governance.lock`
  - `verify` ‚Üí blocking installs when invariants are broken

## 4. "Vibe Coding" preferences (David)

The primary human collaborator prefers:

- Single-shot phases: each phase should bundle all code edits + packaging into
  one run (no long back-and-forth of tiny patches).
- Clear version bumps (v3.2.15 ‚Üí v3.2.16, etc.).
- Explicit stats with every artifact:
  - Total file count,
  - Number of `# STUB:` markers,
  - ZIP + TXT sizes and SHA256 hashes.

When prompting an AI, you can paraphrase this section as:

> "Please apply all requested changes in a single run, then give me a full ZIP
> and a concatenated TXT with a deconcat script, plus file counts and SHA256
> hashes. Do not delete any files; archive old versions under `archive/`."----- CONTENT END -----
----- FILE PATH: docs/BN_EXPORT_EXAMPLE.md
----- CONTENT START -----
# BN Export Example

This note shows how to call the BN export endpoint from Python and turn
the result into a pandas DataFrame for further modeling.

```python
import requests
import pandas as pd

API_URL = "http://localhost:8000/api/v1/export/bn-snapshot"

# If your API enforces roles via headers, include them as needed.
headers = {
    "X-User-Role": "admin",
}

resp = requests.get(API_URL, headers=headers)
resp.raise_for_status()

rows = resp.json()
df = pd.DataFrame(rows)

print(df.head())

# From here you can feed `df` into your Bayesian network tooling of choice,
# or export it to CSV:
df.to_csv("bn_snapshot.csv", index=False)
```


## Including VLM cognitive and affective attributes

If your pipeline has the VLM analyzer enabled, the BN snapshot rows will
include continuous attributes such as:

- `cognitive.coherence`, `cognitive.complexity`, `cognitive.legibility`,
  `cognitive.mystery`, `cognitive.restoration`
- `affect.cozy`, `affect.welcoming`, `affect.tranquil`,
  `affect.scary`, `affect.jarring`

You can discretize these into 3-state BN nodes as follows:

```python
def bin_3(x: float) -> str:
    if x < 0.33:
        return "LOW"
    if x < 0.66:
        return "MID"
    return "HIGH"

mapping = {
    "cognitive.coherence": "COHERENCE",
    "cognitive.complexity": "COMPLEXITY",
    "cognitive.legibility": "LEGIBILITY",
    "cognitive.mystery": "MYSTERY",
    "cognitive.restoration": "RESTORATION",
    "affect.cozy": "AFFECT_COSY",
    "affect.welcoming": "AFFECT_WELCOMING",
    "affect.tranquil": "AFFECT_TRANQUIL",
    "affect.scary": "AFFECT_SCARY",
    "affect.jarring": "AFFECT_JARRING",
}

for col, node in mapping.items():
    if col in df.columns:
        df[node] = df[col].apply(bin_3)

df.to_csv("bn_snapshot_with_bins.csv", index=False)
```

See `docs/BN_MAPPING_COG_AFFECT.md` and
`docs/BN_PRIORS_COG_AFFECT_EXAMPLE.csv` for more details.
----- CONTENT END -----
----- FILE PATH: docs/BN_MAPPING_COG_AFFECT.md
----- CONTENT START -----
# Mapping of VLM-derived cognitive & affective attributes to BN nodes

Image Tagger v3.4.x writes VLM-derived attributes into the Validation table
with keys of the form:

- `cognitive.coherence`
- `cognitive.complexity`
- `cognitive.legibility`
- `cognitive.mystery`
- `cognitive.restoration`

and affective judgments:

- `affect.cozy`
- `affect.welcoming`
- `affect.tranquil`
- `affect.scary`
- `affect.jarring`

For Bayesian network modeling we recommend introducing discrete BN nodes with
3-state bins (LOW / MID / HIGH) derived from the continuous scores in `[0, 1]`.

## Suggested BN node names

Cognitive:

- `COHERENCE`
- `COMPLEXITY`
- `LEGIBILITY`
- `MYSTERY`
- `RESTORATION`

Affective:

- `AFFECT_COSY`
- `AFFECT_WELCOMING`
- `AFFECT_TRANQUIL`
- `AFFECT_SCARY`
- `AFFECT_JARRING`

## Suggested binning scheme

For each scalar attribute `x ‚àà [0, 1]` (e.g. `cognitive.coherence`):

- `LOW`  if `0.0 ‚â§ x < 0.33`
- `MID`  if `0.33 ‚â§ x < 0.66`
- `HIGH` if `0.66 ‚â§ x ‚â§ 1.0`

You can materialize these as new columns on the BN snapshot DataFrame, for
example:

```python
def bin_3(x: float) -> str:
    if x < 0.33:
        return "LOW"
    if x < 0.66:
        return "MID"
    return "HIGH"

for key, node in [
    ("cognitive.coherence", "COHERENCE"),
    ("cognitive.complexity", "COMPLEXITY"),
    ("cognitive.legibility", "LEGIBILITY"),
    ("cognitive.mystery", "MYSTERY"),
    ("cognitive.restoration", "RESTORATION"),
    ("affect.cozy", "AFFECT_COSY"),
    ("affect.welcoming", "AFFECT_WELCOMING"),
    ("affect.tranquil", "AFFECT_TRANQUIL"),
    ("affect.scary", "AFFECT_SCARY"),
    ("affect.jarring", "AFFECT_JARRING"),
]:
    if key in df.columns:
        df[node] = df[key].apply(bin_3)
```

This keeps the BN-facing variables compact while preserving the ordinal
structure of the original VLM scores.
----- CONTENT END -----
----- FILE PATH: docs/BN_NAMING_GUIDE.md
----- CONTENT START -----
# BN Naming Guide (v3.4.36)

This guide explains the naming conventions for Bayesian Network nodes
and variables used in the Image Tagger science stack.

## Principles

- **One construct ‚Üí one canonical name.**
  Use a single, stable identifier for each psychological or physical
  construct (e.g., `RESTORATIVENESS_H1`, `VISUAL_COMPLEXITY_L1`).
- **No whitespace.**
  Use `UPPER_SNAKE_CASE` with underscores instead of spaces.
- **Explicit modality / level tags.**
  When relevant, include a short suffix indicating modality or level,
  e.g. `STRESS_SUBJECTIVE`, `STRESS_PHYSIO`, `RESTORATIVENESS_H1`.
- **Match BN names to docs.**
  Each node name should be defined in the BN documentation or glossary
  with a short description, measurement notes, and key references.

## How to use this in practice

- When adding a new node to a BN config, choose an identifier that:
  - is concise but descriptive,
  - has no spaces or punctuation other than `_`,
  - matches the construct name used in papers and docs.
- Run `python -m backend.science.bn_naming_guard` locally to inspect
  node names and spot obvious problems.
- When in doubt, prefer *clarity* over brevity: a slightly longer,
  unambiguous name is better than a short, cryptic one.

The goal is to keep the BN layer readable and scientifically auditable
for students, TAs, and external collaborators.
----- CONTENT END -----
----- FILE PATH: docs/BN_PRIORS_COG_AFFECT_EXAMPLE.csv
----- CONTENT START -----
node,parents,state,p
COHERENCE,,LOW,0.25
COHERENCE,,MID,0.50
COHERENCE,,HIGH,0.25
COMPLEXITY,,LOW,0.20
COMPLEXITY,,MID,0.50
COMPLEXITY,,HIGH,0.30
LEGIBILITY,,LOW,0.25
LEGIBILITY,,MID,0.50
LEGIBILITY,,HIGH,0.25
MYSTERY,,LOW,0.30
MYSTERY,,MID,0.50
MYSTERY,,HIGH,0.20
RESTORATION,,LOW,0.25
RESTORATION,,MID,0.45
RESTORATION,,HIGH,0.30
AFFECT_COSY,,LOW,0.20
AFFECT_COSY,,MID,0.45
AFFECT_COSY,,HIGH,0.35
AFFECT_WELCOMING,,LOW,0.20
AFFECT_WELCOMING,,MID,0.40
AFFECT_WELCOMING,,HIGH,0.40
AFFECT_TRANQUIL,,LOW,0.25
AFFECT_TRANQUIL,,MID,0.45
AFFECT_TRANQUIL,,HIGH,0.30
AFFECT_SCARY,,LOW,0.60
AFFECT_SCARY,,MID,0.30
AFFECT_SCARY,,HIGH,0.10
AFFECT_JARRING,,LOW,0.55
AFFECT_JARRING,,MID,0.30
AFFECT_JARRING,,HIGH,0.15----- CONTENT END -----
----- FILE PATH: docs/FIRST_DASHBOARD_QUICKSTART.md
----- CONTENT START -----
# First Dashboard Quickstart

This guide shows one minimal path from a fresh install to a useful
dashboard view in the Supervisor Monitor and Research Explorer.

1. **Bring up the stack**

   Run:

   ```bash
   ./install.sh
   ```

   This starts the database, backend API, and frontend portal. If the
   science smoketest reports that there are no images yet, that is expected
   on a fresh database.

2. **Upload a small batch of images as Admin**

   - Open the frontend portal in your browser.
   - Go to the **Admin Cockpit**.
   - Use the **Bulk Upload** panel to select a small folder of images.
   - Click **Upload**. The status line will report how many images were created.

3. **Run the science pipeline**

   Once images exist in the database, run:

   ```bash
   python3 scripts/import_harness.py
   python3 scripts/smoke_science.py
   ```

   The import harness will exercise the science pipeline on a sample of
   images; the smoketest will confirm that key indices are being written
   into `Validation` rows.

4. **Inspect team metrics in the Monitor**

   - Assign yourself or a student as a **tagger** (set `X-User-Role: tagger`
     in your API client or browser plugin).
   - Use the **Tagger Workbench** to create some validations for the
     uploaded images.
   - Open the **Supervisor Monitor** and confirm that team statistics and
     IRR metrics begin to appear. If the dashboard reports that no team
     statistics are available yet, continue tagging.

5. **Explore validated images**

   - Open the **Research Explorer**.
   - Use filters to narrow down by attributes or tool configuration.
   - Export the current selection for downstream analysis (e.g., BN or
     statistical modeling).

This sequence ‚Äì upload images, run science, tag, then monitor and explore ‚Äì
is the canonical ‚Äúfirst dashboard‚Äù path for Image Tagger.
----- CONTENT END -----
----- FILE PATH: docs/INTERPRETING_RUTHLESS_REPORTS.md
----- CONTENT START -----
# How to Read Ruthless Audit Reports

The Ruthless audit prompts produce multi-panel reports (e.g., systems
architect, governance officer, chief scientist). This file explains how
to use those reports when working with the Image Tagger repository.

## Typical Sections

- **Executive Verdict (GO / NO-GO / CONDITIONAL GO)**  
  High-level readiness judgement for teaching, research, or deployment.
- **Kill List / Blockers**  
  Items that must be fixed before the system is considered runnable.
- **High-Priority Warnings**  
  Issues that do not break the build but affect scientific validity,
  governance, or student experience.
- **Role-Specific Notes**  
  Recommendations from different perspectives (DevOps, UI/HITL, etc.).

## How to act on a report

1. Start with the **Kill List** and confirm that all blockers are fixed.
2. Treat high-priority warnings as the next sprint planning input.
3. Capture concrete actions in CHANGELOG and `reports/` for traceability.
4. When a new version is prepared, re-run the Ruthless prompts and store
   the new report alongside the old one to show progress over time.

For v3.4.36, the main themes are:
- Hardening the Admin upload pipeline.
- Improving scientific naming hygiene for BN and restorativeness models.
- Keeping GO/NO-GO checks transparent and reproducible for students.
----- CONTENT END -----
----- FILE PATH: docs/PRODUCTION_DEPLOYMENT.md
----- CONTENT START -----
# Production Deployment Guide (v3.4.x)

This document describes how to deploy Image Tagger beyond a single laptop
development environment.

For typical classroom use, running `install.sh` locally with Docker is enough.
Use this guide only if you intend to:

- expose the system on a shared department server,
- run it for multiple users over the network,
- or keep a long-running instance for a research group.

The goals are:

- keep data and credentials safe,
- keep VLM / API keys out of the repository,
- make upgrades predictable.

---

## 1. Core components

Image Tagger v3.x consists of:

- **Backend API** (FastAPI + Postgres)
- **Frontend** (React app, typically served by nginx)
- **Science pipeline** (Python / OpenCV / NumPy)
- **Optional VLMs** (OpenAI / Anthropic / others via `backend/services/vlm.py`)

In the default Docker setup:

- `deploy/Dockerfile.backend` builds the backend image.
- `deploy/Dockerfile.frontend` (or equivalent) builds the frontend.
- `deploy/docker-compose.yml` defines services:
  - `backend`
  - `frontend`
  - `db` (Postgres)
  - optional reverse proxy (nginx) in front.

---

## 2. Environment variables

These are the most important environment variables for a production-like setup.

### 2.1 Security / auth

- `API_SECRET`  
  - Used to sign JWT tokens.
  - **Production rule:** never use the default development key.
  - Generate a long random string, e.g.:

    ```bash
    python - << 'EOF'
    import secrets
    print(secrets.token_urlsafe(64))
    EOF
    ```

  - Set it in your environment (or `.env`) before running Docker:

    ```bash
    export API_SECRET="your-long-random-secret"
    ```

- `ADMIN_EMAIL`, `ADMIN_PASSWORD` (if supported in seeding / setup scripts)  
  - Use non-trivial values; do not reuse personal passwords.

### 2.2 Database

- `POSTGRES_DB`
- `POSTGRES_USER`
- `POSTGRES_PASSWORD`

These are defined in `docker-compose.yml`. For production-like use:

- change the defaults,
- ensure the DB is not exposed to the public internet (bind to localhost or an internal network),
- back up the database volume regularly.

### 2.3 VLM keys (optional)

- `OPENAI_API_KEY`
- `ANTHROPIC_API_KEY`

Rules:

- Never commit these keys to the repository.
- Prefer `.env` files or deployment-specific secret stores (e.g. Docker secrets,
  Kubernetes secrets, or a password manager).
- Remember that VLM calls cost money. Use batch size and sampling strategies
  appropriate for your budget.

---

## 3. Volumes and persistence

In `deploy/docker-compose.yml` you will see volumes for:

- Postgres data (e.g. `pgdata:/var/lib/postgresql/data`)
- Image storage (e.g. `data_store:/app/data_store`)

For a production-like deployment:

1. Ensure these volumes are backed up.
2. Prefer **named volumes** or explicit host paths, not anonymous volumes, so
   that data survives container recreation.

Example snippet:

```yaml
services:
  db:
    image: postgres:16
    environment:
      POSTGRES_DB: itagger
      POSTGRES_USER: itagger
      POSTGRES_PASSWORD: change_me
    volumes:
      - pgdata:/var/lib/postgresql/data

  backend:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.backend
    environment:
      API_SECRET: ${API_SECRET}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
    volumes:
      - data_store:/app/data_store

volumes:
  pgdata:
  data_store:
```

---

## 4. HTTPS and reverse proxy

For any deployment exposed beyond localhost, you should terminate HTTPS in a
reverse proxy such as nginx or Caddy.

Typical pattern:

- nginx listens on ports 80/443.
- nginx proxies:
  - `/api/` ‚Üí backend container
  - `/` ‚Üí frontend container (static assets)

High-level nginx sketch:

```nginx
server {
    listen 80;
    server_name your.domain.edu;

    # Redirect HTTP ‚Üí HTTPS
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name your.domain.edu;

    ssl_certificate     /etc/letsencrypt/live/your.domain.edu/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/your.domain.edu/privkey.pem;

    location /api/ {
        proxy_pass         http://backend:8000/api/;
        proxy_set_header   Host $host;
        proxy_set_header   X-Real-IP $remote_addr;
    }

    location / {
        root   /usr/share/nginx/html;
        index  index.html;
        try_files $uri /index.html;
    }
}
```

In a lab environment you do not need a perfect TLS story, but:

- use HTTPS if students log in with passwords,
- restrict access to your department / VPN where possible.

---

## 5. Running in ‚Äúproduction mode‚Äù

A simple default deployment is:

```bash
cd deploy
# Make sure .env contains API_SECRET and DB credentials
docker compose up --build -d
```

Once containers are healthy:

- Visit the frontend URL in a browser, e.g.:
  - `http://localhost:8080/` (direct)
  - or `https://your.domain.edu/` (via nginx).

Use the **Admin Cockpit** to:

- configure VLM engine,
- upload a small batch of images,
- run smoke tests.

---

## 6. Upgrade strategy

When a new Image Tagger version is released (e.g. v3.4.9 ‚Üí v3.4.9):

1. Back up the database and `data_store` volume.
2. Replace the backend/frontend image build context with the new repo.
3. Rebuild containers:

   ```bash
   cd deploy
   docker compose build backend frontend
   docker compose up -d
   ```

4. Run smoke tests:
   - `scripts/smoke_science.py`
   - `scripts/smoke_frontend.py`
   - basic manual exploration in the browser.

If anything fails:

- roll back to the previous images and code,
- restore from backup if necessary.

---

## 7. Security checklist

Before exposing Image Tagger outside a development machine:

- [ ] `API_SECRET` is long, random, and not the default.
- [ ] Database credentials are not the defaults shipped in the repo.
- [ ] Postgres is not exposed directly to the public internet.
- [ ] HTTPS is configured on the external endpoint (or access is VPN-only).
- [ ] VLM API keys are stored in environment variables or secrets, not in source.
- [ ] Docker volumes for DB and image data are backed up regularly.
- [ ] The version (`VERSION` file, backend, README) matches the deployed tag.

If you can honestly tick all of these boxes, you are in reasonable shape for a
departmental / lab deployment.
----- CONTENT END -----
----- FILE PATH: docs/SCIENCE_DEBUG_LAYERS.md
----- CONTENT START -----
# Science Debug Layers: Edges, Overlays, and Parameters

This document explains the "debug view" features in the Explorer app and how
they connect to the underlying computer vision pipeline.

The goal is to make the **science visible** to students: when the pipeline
computes complexity or edge-based measures, they should be able to see which
edges and structures the algorithm is operating on.

## 1. Where to find the debug controls

In the **Explorer** GUI, each image card has a **Debug** toggle in the toolbar
above the image:

- Click once: `Debug: Edges`
- Click twice: `Debug: Overlay`
- Click a third time: `Debug: Off`

When `Debug` is set to `Edges` or `Overlay`, a small control panel appears
with sliders for:

- **Edges**: low and high thresholds
- **Overlay**: opacity (only when in overlay mode)

These controls are lightweight and safe to adjust during exploration.

## 2. What the backend is doing

The debug views are powered by the `v1_debug` API in the backend:

- Endpoint: `GET /api/v1/debug/images/{image_id}/edges`
- Query parameters:
  - `t1`: low Canny threshold (default 50)
  - `t2`: high Canny threshold (default 150)
  - `l2`: whether to use the L2 gradient option (default `true`)

Under the hood, the backend:

1. Resolves the image path based on the `Image.storage_path` field and
   `IMAGE_STORAGE_ROOT`.
2. Loads the image and converts it to grayscale.
3. Runs OpenCV's Canny edge detector:

   ```python
   edges = cv2.Canny(gray, t1, t2, L2gradient=l2)
   ```

4. Converts the edge map to a PNG image and returns it as `image/png`.

### 2.1 Cache behaviour

Because Canny is CPU-intensive for repeated requests, the debug endpoint keeps
a small **on-disk cache** keyed by:

- `image_id`
- `t1`
- `t2`
- `l2`

If a PNG edge map already exists for that combination, it is served directly.
If not, it is computed once and then cached.

This makes classroom usage (many students experimenting with the same images)
fast and predictable.

## 3. How the frontend uses these parameters

In the Explorer React app, the debug state is:

```js
const [debugMode, setDebugMode] = useState('none');    // 'none' | 'edges' | 'overlay'
const [overlayOpacity, setOverlayOpacity] = useState(0.5);
const [edgeThresholds, setEdgeThresholds] = useState({ low: 50, high: 150 });
```

When `debugMode` is not `'none'`, the app:

- Builds a URL of the form:

  ```text
  /api/v1/debug/images/{image_id}/edges?t1={low}&t2={high}
  ```

- Either:
  - uses that URL directly as the `<img>` source in **Edges** mode, or
  - draws the edges as a second `<img>` on top of the original image in
    **Overlay** mode with `opacity = overlayOpacity`.

The sliders in the UI update `edgeThresholds.low`, `edgeThresholds.high`, and
`overlayOpacity`, so you can see in real time how the parameters affect the
edge map.

## 4. How to use this pedagogically

Some example classroom exercises:

### 4.1 Parameter sensitivity

- Choose a single image (e.g., a cluttered interior).
- Ask students to vary `t1` and `t2` and observe:
  - When do edges become too dense to be meaningful?
  - When do edges become too sparse (missing important structure)?
- Discuss how threshold choice influences downstream measures of
  **visual complexity**.

### 4.2 Overlay vs. full replacement

- In **Edges** mode, only the edges are visible.
- In **Overlay** mode, edges are drawn on top of the original image with
  adjustable opacity.

Ask students which mode makes it easier to understand *why* the algorithm is
detecting certain edges.

### 4.3 Compare different image types

- Low clutter vs high clutter.
- High contrast vs low contrast.
- Natural vs highly geometric spaces.

How do edge maps differ across these, and what does that imply for complexity
and legibility?

### 4.4 Linking to BN factors

When complexity is high (based on edges), how might this influence BN nodes
related to:

- cognitive load,
- preference,
- legibility?

Encourage students to think about which types of edges and patterns might
matter most for **human** perception, not just the algorithm.

## 5. Interpreting Canny thresholds

A few guiding intuitions you can give students:

- The **low threshold (`t1`)** controls how easy it is for a gradient to
  start an edge. Lower values ‚Üí more edges, including weak gradients.
- The **high threshold (`t2`)** controls how strong a gradient must be to be
  accepted as a definite edge. Higher values ‚Üí only the strongest contrast
  boundaries survive.
- The **ratio between `t1` and `t2`** often matters more than absolute values.

In architectural images:

- Very low thresholds can produce noisy edge maps where textures or sensor
  noise dominate.
- Very high thresholds can drop important structural lines (e.g., corners,
  door frames) and make the space look simpler than it really is.

Part of the scientific judgement is choosing a regime where the edge map
captures **structurally meaningful** lines without drowning in noise.

## 6. Takeaways

- The debug layers are there to make the pipeline **transparent**, not just
  pretty.
- Students should use them to connect:
  - raw pixels ‚Üí edges ‚Üí quantitative measures ‚Üí BN factors ‚Üí psychological
    interpretations.
- Experimenting with the thresholds and overlay is encouraged; it is a safe,
  reversible way to build intuition about the relationship between visual
  structure and computed complexity.
----- CONTENT END -----
----- FILE PATH: docs/SCIENCE_TAG_MAP.md
----- CONTENT START -----
# Science Tag Coverage Map (v1)

Autogenerated by `scripts/generate_tag_coverage.py`.

- Total known feature keys: 145
- Keys with at least one compute implementation: 76
- Stub-allowed keys: 69

## Breakdown by source_type

| source_type | count |
|------------|-------|
| math_or_deterministic | 61 |
| stub_only | 69 |
| stub_total | 69 |
| vlm_cognitive | 1 |
| vlm_semantic | 14 |

## Notes

- `math_or_deterministic`: numeric features computed by the L0/L1 engines
  (e.g., color histograms, texture, fractals, depth/spatial metrics).
- `vlm_cognitive`: high-level cognitive/affective dimensions estimated by
  the CognitiveStateAnalyzer VLM.
- `vlm_semantic`: semantic tags such as style.* and spatial.room_function.*
  estimated by the SemanticTagAnalyzer VLM.
- `stub_only`: keys that are intentionally present in the registry but do
  not yet have a compute implementation; they are tracked by
  `backend/science/feature_stubs.py`.
- `unassigned`: keys that are present in the union of registry/stub/computed
  keys but that currently have no detectable compute implementation and are
  not listed as stubs. In a healthy repository this count should be zero.
  New builds should fail if it drifts above zero.
----- CONTENT END -----
----- FILE PATH: docs/STUDENT_ONBOARDING.md
----- CONTENT START -----
# Student Onboarding Guide (Image Tagger v3.3.7)

This guide is for students who are joining the Image Tagger v3 project. It gives you a
practical sequence of steps so you can get productive quickly.

## Phase 0 ‚Äì Read the ground rules (30‚Äì60 minutes)

Before you touch code, skim:

- `PROJECT_CONSTITUTION.md` ‚Äì how this repo is expected to evolve.
- `docs/governance_guide.md` ‚Äì what Guardian is and why it matters.
- `docs/AI_COLLAB_WORKFLOW.md` ‚Äì how we work with AI tools on this project.

You do *not* need to memorise everything; just get a sense of:

- Why we avoid deleting files.
- Why each release ships as a ZIP + concatenated TXT.
- How Guardian is used to detect drift.

## Phase 1 ‚Äì Get the system running (1‚Äì2 sessions)

1. Make sure Docker Desktop is installed and running.
2. From the repo root:

   ```bash
   chmod +x install.sh
   ./install.sh
   ```

3. When `install.sh` finishes, check:

   - The API health endpoint: `http://localhost:8000/health`
   - The role portal (index page) ‚Äì you should see links to:
     - Tagger Workbench
     - Supervisor Monitor
     - Admin Cockpit
     - Research Explorer

If something fails, see `docs/devops_quickstart.md` for common issues.

## Phase 2 ‚Äì Explore the GUIs (1‚Äì2 sessions)

With the stack running:

1. **Tagger Workbench**
   - Open the Tagger app from the portal.
   - Load a small batch of images (if available) and try tagging a few.
   - Verify that your tags appear in the database by refreshing the Workbench and, later,
     by looking at Monitor.

2. **Supervisor Monitor**
   - Open the Monitor app.
   - You should see velocity and IRR summaries.
   - If there is no data yet, Monitor may show empty tables; once taggers have worked,
     it should display real stats.
   - Use the Tag Inspector to inspect disagreements for individual images.

3. **Admin Cockpit**
   - Inspect model and budget settings.
   - You do not need to change anything at first; just understand what controls exist.

4. **Research Explorer**
   - Open the Explorer app.
   - Load attributes and try simple searches or filters.
   - Notice how attribute keys are grouped; these map to the underlying attribute registry.

## Phase 3 ‚Äì Understand the science pipeline (2‚Äì3 sessions)

Read:

- `docs/science_overview.md` ‚Äì how the science pipeline is structured.
- Skim the code under `backend/science/`, especially:
  - `pipeline.py`
  - `summary.py`
  - A couple of analyzers (e.g. colour, texture, fractals).

Your goal in this phase:

- Understand what `AnalysisFrame` does.
- Know which attributes the pipeline writes into the `Validation` table.
- See how composite indices (for example `science.visual_richness`) and their bins are built.

If you have time, run or adapt `scripts/smoke_science.py` to confirm that the pipeline can
successfully write science attributes for at least one image.

## Phase 4 ‚Äì BN export and data for modelling (2‚Äì3 sessions)

Once you are comfortable with the science outputs:

1. Read `backend/api/v1_bn_export.py` and `backend/schemas/bn_export.py`.
2. Inspect `backend/science/index_catalog.py` to see which indices are considered for BN.
3. Ensure there are science `Validation` rows in the database (either by running the
   pipeline or by using a seeding script).

Then, either:

- Call the `/v1/export/bn-snapshot` endpoint from a notebook or script, or
- Use the `export_bn_snapshot` function directly inside a Python session.

Your goal is to produce a small CSV or JSONL of BN-ready rows that can be used in external
tools (for example PyMC, pgmpy, or a custom BN visualiser).

## Phase 5 ‚Äì Contribute safely (ongoing)

When you are ready to make changes:

1. Decide which area you are touching:
   - Science (new analyzers or indices).
   - UX (Workbench, Monitor, Explorer).
   - DevOps / tests.
2. Update the relevant code and docs *together*.
3. Run at least:
   - `python scripts/guardian.py verify`
   - `pytest -q`

When you are ready to make changes, run at least:

- `pytest tests/test_v3_api.py` ‚Äì API health and RBAC sanity.
- `pytest tests/test_guardian.py` ‚Äì governance (Guardian) sanity.
- `pytest tests/test_bn_export_smoke.py` ‚Äì BN export coupled to Validation.
- `pytest tests/test_workbench_smoke.py` ‚Äì Tagger Workbench basic flow.
- `pytest tests/test_explorer_smoke.py` ‚Äì Research Explorer basic flow.
- (Optional) `pytest -m slow` ‚Äì includes `test_science_pipeline_smoke.py`, which
  runs the full science pipeline on a synthetic image.

If Guardian reports drift and the changes are intentional, talk to the project lead about
updating the baseline (`freeze`) as part of the next release.

## When in doubt

- Ask a TA or project lead to sanity-check your plan.
- Use AI tools as helpers, but remember that this repo has governance rules ‚Äì changes
  should keep tests, docs and contracts aligned.

## Example Lab 1 ‚Äì End-to-end walkthrough (60‚Äì90 minutes)

This lab is designed as a first-session exercise:

1. Run `./install.sh` and confirm the role portal and `/health` endpoint work.
2. Open the Tagger Workbench and tag at least 5 images.
3. Open the Supervisor Monitor and observe:
   - Velocity table (see whether your user ID appears).
   - IRR table (if multiple taggers have worked on the same images).
4. Open the Research Explorer and:
   - Load attributes.
   - Filter images (if available) by a science attribute such as
     `science.visual_richness` or by a tag you just created.
5. (Optional) Run `pytest tests/test_bn_export_smoke.py` and inspect the output
   of `/v1/export/bn-snapshot` in a notebook to see how science attributes and
   bins are packaged for BN tools.


## A note on using AI tools

AI assistants (ChatGPT, Claude, Gemini, etc.) are welcome, but please:

- Do **not** ask an AI to regenerate this repository from scratch.
- Always start from a current ZIP and request incremental, governance-respecting
  changes (no deletions, keep Guardian and tests intact).
- Treat AI as a collaborator that proposes diffs you can review, rather than as
  an opaque code generator.

----- CONTENT END -----
----- FILE PATH: docs/UPLOAD_JOBS_README.md
----- CONTENT START -----
# Upload Jobs & Science Orchestrator

This document describes how the Image Tagger upload job system works in v3.4.51 and how to monitor or run jobs.

## Overview

- When an admin uploads images via the Admin Cockpit, the `/api/v1/admin/upload` endpoint:
  - Persists the images to disk.
  - Creates `Image` rows in the database.
  - Creates an `UploadJob` row plus one `UploadJobItem` per image.
  - Enqueues a background job to run the science pipeline for each image in the batch.

- The science pipeline call uses `SciencePipeline(config=SciencePipelineConfig(enable_all=True), db=session)` and:
  - Computes math-based metrics (fractal, texture, color, complexity).
  - Runs the cognitive VLM analyzer and architectural-patterns VLM analyzer as configured.
  - Logs VLM cost usage to the `ToolUsage` ledger.

## Monitoring in the Admin Cockpit

The Admin Cockpit now exposes an **Upload jobs** panel:

- Shows the most recent jobs with:
  - Job id
  - Status (`PENDING`, `RUNNING`, `COMPLETED`, `COMPLETED_WITH_ERRORS`, `FAILED`)
  - Progress (completed / total items)
  - Error count and short summary
- The panel has a **Refresh** button to pull the latest state from `/api/v1/admin/upload/jobs`.
- After a bulk upload completes, the panel will automatically refresh via the `handleUploadCompleted` callback.

This gives TAs and admins visibility into whether large batches are still running, have failed, or are fully processed.

## API Endpoints

- `GET /api/v1/admin/upload/jobs?limit=20`
  - Returns a list of the most recent jobs and high-level status counters.
- `GET /api/v1/admin/upload/jobs/{job_id}`
  - Returns detailed information for a single job, including per-item statuses.

Both endpoints require an admin role (`X-User-Role: admin`).

## Running Jobs from a Worker Process

The primary execution path is FastAPI's in-process `BackgroundTasks`. For more advanced deployments, you can run jobs from a separate worker using:

```bash
python -m scripts.run_upload_job <job_id>
```

This script:

- Creates its own database session.
- Calls `backend.services.upload_jobs.run_upload_job(job_id)`.
- Updates job and item statuses in-place.

In a real multi-process deployment, you would typically:

- Point a task queue (e.g. Celery, RQ, or a simple cron) at `scripts.run_upload_job`.
- Dispatch a job id from the API layer into the queue.
- Let the worker process handle the science pipeline execution.

For classroom use, the built-in `BackgroundTasks` runner is usually sufficient.
----- CONTENT END -----
----- FILE PATH: docs/VLM_CASE_STUDY_HUMAN_VS_AI.md
----- CONTENT START -----
# Case Study: Comparing Human vs AI Ratings (VLM Cognitive / Affective Analysis)

This case study is designed for a small "scientist role" exercise.
Students compare **human ratings** of architectural images with the
**AI-generated cognitive and affective ratings** produced by Image Tagger's
Visual Language Model (VLM) pipeline.

## 1. Goal

Given a small set of images (e.g., 20‚Äì40 interiors), students will:

- Collect **human ratings** on:
  - coherence
  - complexity
  - legibility
  - mystery
  - restoration
  - cozy, welcoming, tranquil, scary, jarring

- Run the **science pipeline** with a real VLM provider configured
  (Gemini / OpenAI / Anthropic).

- Export a **BN-ready dataset** from Image Tagger.

- Merge the two datasets in a notebook and compute:
  - correlations (Pearson / Spearman) between human and AI ratings
  - simple scatter plots and inspection of outliers.

## 2. Setup

1. Configure the VLM provider in the **Admin Cockpit ‚Üí VLM Engine** panel:

   - Choose a provider (Gemini, OpenAI, Anthropic, or Stub for dry runs).
   - Optionally set a *cognitive & affective prompt override* to customize
     the wording for this class or experiment.
   - Set a **max recommended batch size** (e.g., 50 images).
   - Set an approximate **VLM cost per 1000 images** (USD). This is used for
     rough cost estimates only.

2. Add a small dataset of images via:

   - **Admin Bulk Upload** (recommended), or
   - the existing seeding scripts.

3. Confirm that the science pipeline runs successfully on at least one image
   (e.g., using `scripts/run_science_on_sample.py` or the science harness).

## 3. Collecting Human Ratings

Create a simple survey (Qualtrics, Google Forms, or a custom experiment) that:

- Shows each image.
- Asks participants to rate each dimension from 0.0 (very low) to 1.0 (very high),
  or on a Likert scale that can be rescaled to [0, 1].

Export the human ratings as a CSV with at least:

- `image_id` or `image_filename`
- the ten rating columns:
  - `human.coherence`, `human.complexity`, `human.legibility`,
    `human.mystery`, `human.restoration`,
  - `human.cozy`, `human.welcoming`, `human.tranquil`,
    `human.scary`, `human.jarring`.

## 4. Exporting AI / VLM Ratings from Image Tagger

1. Run the science pipeline on the selected images so that the VLM-based
   **cognitive.* and affect.* attributes** are written to the database
   (or CSV, if using the science harness).

2. Use the BN export script to obtain a machine-readable dataset, e.g.:

```bash
python -m scripts.export_bn_ready_dataset --output bn_dataset.csv
```

3. Inspect `bn_dataset.csv` and confirm that it contains columns such as:

- `cognitive.coherence`, `cognitive.complexity`,
  `cognitive.legibility`, `cognitive.mystery`, `cognitive.restoration`
- `affect.cozy`, `affect.welcoming`, `affect.tranquil`,
  `affect.scary`, `affect.jarring`

## 5. Merging the Datasets

In a Jupyter notebook (Python), students can:

1. Load both CSV files:

```python
import pandas as pd

ai_df = pd.read_csv("bn_dataset.csv")
human_df = pd.read_csv("human_ratings.csv")

# Align on image identifier (image_id or filename)
merged = ai_df.merge(human_df, on="image_id", how="inner")
print("Merged shape:", merged.shape)
```

2. Compute correlations:

```python
cog_cols = [
    "coherence", "complexity", "legibility", "mystery", "restoration",
]
affect_cols = ["cozy", "welcoming", "tranquil", "scary", "jarring"]

for dim in cog_cols + affect_cols:
    ai_col = f"cognitive.{dim}" if dim in cog_cols else f"affect.{dim}"
    human_col = f"human.{dim}"
    corr = merged[[ai_col, human_col]].corr(method="pearson").iloc[0, 1]
    print(f"{dim:12s} Pearson r = {corr: .3f}")
```

3. Plot scatter plots for a few dimensions:

```python
import matplotlib.pyplot as plt

for dim in ["coherence", "complexity", "restoration"]:
    ai_col = f"cognitive.{dim}"
    human_col = f"human.{dim}"
    plt.figure()
    plt.scatter(merged[human_col], merged[ai_col])
    plt.xlabel(f"Human {dim}")
    plt.ylabel(f"AI {dim}")
    plt.title(f"Human vs AI: {dim}")
    plt.grid(True)
    plt.show()
```

## 6. Discussion Questions

- On which dimensions do human and AI ratings agree most strongly?
- Are there systematic biases (e.g., AI consistently overestimates "cozy"
  compared to human ratings)?
- Do disagreements cluster in particular kinds of images (e.g., high clutter,
  unusual lighting, ambiguous spaces)?
- How sensitive are the AI ratings to changes in the **prompt override**
  configured in the Admin Cockpit?

## 7. Variations

- Compare different VLM providers (Gemini vs OpenAI vs Anthropic) on the same
  set of images.
- Run a second cohort of students with a slightly different cognitive prompt
  and explore how the wording affects AI‚Äìhuman agreement.
- Use the BN tooling to discretize the AI ratings into bins and compare
  agreement at the categorical level rather than raw scores.
----- CONTENT END -----
----- FILE PATH: docs/VLM_INTEGRATION.md
----- CONTENT START -----
# VLM Integration Guide (v3.4.0)

Image Tagger v3.4.0 introduces a unified Visual Language Model (VLM) service ‚Äì
"the Brain" ‚Äì that can plug in different multimodal providers while keeping the
science pipeline code stable.

## 1. Supported providers

The backend currently knows about four logical providers:

- **Gemini** (Google): e.g. `gemini-1.5-flash`, `gemini-1.5-pro`.
- **OpenAI**: e.g. `gpt-4o-mini`, `gpt-4o`, `gpt-4.1`.
- **Anthropic**: e.g. `claude-3.5-sonnet`.
- **Stub**: local neutral placeholders; no network calls.

The Admin Cockpit exposes a **VLM Engine** card where you can:

- See which API keys are visible inside the container.
- Choose a preferred provider (`auto`, `gemini`, `openai`, `anthropic`, `stub`).
- Test the current configuration on a specific image ID.

## 2. Configuration: API keys

VLM providers are activated via standard environment variables when you start
the backend container:

- Gemini: `GEMINI_API_KEY` (or `GOOGLE_API_KEY`)
- OpenAI: `OPENAI_API_KEY`
- Anthropic: `ANTHROPIC_API_KEY`

If none of these are set, the system falls back to the `StubEngine`. The
**VLM Engine** panel will show that only the stub is available and cognitive
attributes will be filled with neutral 0.5 scores.

## 3. Configuration: Provider preference

At runtime, the preferred provider is resolved in this order:

1. The provider saved from the Admin Cockpit (`VLM Engine` card).
2. The `VLM_PROVIDER` environment variable, if set.
3. Automatic detection based on available keys, with priority:

   `Gemini ‚Üí OpenAI ‚Üí Anthropic ‚Üí Stub`.

The Admin Cockpit writes a small JSON file at:

- `backend/data/vlm_config.json`

This file is safe to commit if you want a default in a lab deployment, or you
can keep it in a local volume in production.

## 4. How the science pipeline uses the VLM

The `SciencePipeline` constructs an `AnalysisFrame` for each image. The
`CognitiveStateAnalyzer` then calls the unified VLM service:

- Module: `backend/science/context/cognitive.py`
- Service: `backend/services/vlm.py`

The VLM is asked to rate the scene on five dimensions derived from
environmental psychology (Kaplan & Kaplan + restoration):

- `coherence`
- `complexity`
- `legibility`
- `mystery`
- `restoration`

Scores are expected in `[0.0, 1.0]`. They are stored as attributes:

- `cognitive.coherence`
- `cognitive.complexity`
- `cognitive.legibility`
- `cognitive.mystery`
- `cognitive.restoration`

If the engine is a stub (no keys configured), the analyzer writes 0.5 for all
five attributes with confidence 0.0 so downstream tools can still assume the
keys exist.

## 5. Cost and safety notes

- VLM calls are network-bound and relatively expensive; start by testing on a
  single image from the Admin Cockpit before running large batches.
- For student or classroom use, you can run in **Stub** mode (no keys) and
  still exercise the full pipeline shape.
- When providing real keys, prefer the most cost-effective model (e.g.
  Gemini 1.5 Flash or OpenAI gpt-4o-mini) for bulk tagging; reserve more
  expensive models for research-grade runs.

## 6. Quick sanity test

1. Ensure DB is seeded and at least one image exists.
2. In the Admin Cockpit ‚Üí **VLM Engine**, set a provider and save.
3. Enter a small image ID (e.g. `1`) and click **Test VLM**.
4. Confirm that:
   - `engine` reports the expected backend (Gemini / OpenAI / Anthropic / Stub).
   - The response JSON includes the five keys above.

If that works, you can now treat the cognitive metrics as part of the standard
attribute set for export, BN construction, and dashboards.


## 7. Affective / experiential dimensions

In addition to the five core cognitive/environmental metrics, the VLM prompt
now asks for five affective tone dimensions:

- `cozy`      ‚Äì how cozy / snug / intimate the space feels
- `welcoming` ‚Äì how welcoming / socially inviting it feels
- `tranquil`  ‚Äì how calm / tranquil it feels
- `scary`     ‚Äì how scary / threatening it feels
- `jarring`   ‚Äì how visually or affectively jarring it feels

These are written into the attribute store as:

- `affect.cozy`
- `affect.welcoming`
- `affect.tranquil`
- `affect.scary`
- `affect.jarring`

In stub mode they are set to 0.5 with confidence 0.0; with a live VLM they are
clamped to [0.0, 1.0] with confidence 0.9 by default.
----- CONTENT END -----
----- FILE PATH: docs/WHATS_NEW_v3_3_x.md
----- CONTENT START -----
# What's New in Image Tagger v3.3.x

This note summarizes the main changes introduced in the 3.3.x series,
relative to the earlier 3.2.x line.

## 1. Four dedicated GUIs

The old single-HTML / iframe shell has been replaced by four separate apps:

- **Tagger Workbench** (`frontend/apps/workbench`): high-throughput labeling UI.
- **Supervisor Monitor** (`frontend/apps/monitor`): team metrics, IRR summaries, Tag Inspector.
- **Admin Cockpit** (`frontend/apps/admin`): model + cost settings, tool configs, bulk image upload.
- **Research Explorer** (`frontend/apps/explorer`): faceted search over validated images, export hooks.

Each app has its own `ApiClient` with a clear base path and role header.

## 2. Admin bulk image upload

Admins can now upload multiple images in one operation via the Admin Cockpit.
The backend endpoint is:

- `POST /api/v1/admin/upload`

The endpoint returns `created_count`, `image_ids`, and `storage_paths`. It is
restricted to users with `X-User-Role: admin`.

## 3. Explorer attribute descriptions

The Explorer now surfaces attribute descriptions (or notes) as hover tooltips
on attribute chips, when available in the attribute catalog.

## 4. Frontend smoketest

A new script, `scripts/smoke_frontend.py`, checks that the portal is reachable
and that at least one of the expected key phrases appears in the rendered HTML
(or in a Playwright-driven browser session if Playwright is installed). This
is wired into `install.sh` as a lightweight end-to-end sanity check.

## 5. Governance and packaging

The 3.3.x line standardizes governance and packaging:

- `v3_governance.yml` + `scripts/guardian.py` enforce no-deletion and other rules.
- Releases are shipped both as a ZIP and as a concatenated TXT with a `deconcat.py` helper.

## 6. v3.3.7 refinements

In v3.3.7, we additionally:

- Unified version strings across `VERSION`, `backend/main.py`, `install.sh`, and key docs.
- Made `scripts/smoke_science.py` tolerant of a zero-image database (it now skips with guidance
  instead of failing hard).
- Added pytest coverage for the Admin bulk upload endpoint and the Monitor Tag Inspector.
- Improved empty-state messaging in the Monitor and Explorer apps.
- Added this `WHATS_NEW_v3_3_x.md` summary and small documentation polish.

## 7. v3.3.10 debug layers and deployment guide

- Added a `/v1/debug/images/{{image_id}}/edges` endpoint and wired it into Explorer
  so that students can toggle between the original image and its edge-map view.
- Introduced `docs/PRODUCTION_DEPLOYMENT.md` describing how to rotate API secrets,
  mount persistent volumes, and front the system with HTTPS.

----- CONTENT END -----
----- FILE PATH: docs/devops_quickstart.md
----- CONTENT START -----
# DevOps Quickstart (v3.3.7)

This guide is for TAs and students who want to run the Image Tagger v3 stack on a laptop
or lab machine.

## 1. Prerequisites

- Docker Desktop (or an equivalent container runtime)
- Python 3.10 or 3.11 (for helper scripts and tests)
- Git (if you are cloning from a repository) or a copy of the ZIP release

You do **not** need a system-wide PostgreSQL install; the database is run in a Docker container.

## 2. First-time setup

1. Ensure Docker Desktop is running.
2. From the repository root, make the install script executable (once per clone):

   ```bash
   chmod +x install.sh
   ```

3. Run the installer:

   ```bash
   ./install.sh
   ```

   By default this will:

   - Build Docker images for the API, database and frontend (using `deploy/docker-compose.yml`).
   - Run seeding scripts:
     - `backend/scripts/seed_tool_configs.py`
     - `backend/scripts/seed_attributes.py`
   - Run smoke tests:
     - `scripts/smoke_api.py`
     - `scripts/smoke_science.py`

If any step fails, the script prints an error message and exits with a non-zero status code.

## 3. Verifying the system

After `./install.sh` completes successfully:

1. Open a browser and navigate to the frontend portal (for example):

   - `http://localhost:8000/index.html` or
   - `http://localhost:8080/index.html`

   The exact port depends on your `deploy/docker-compose.yml` configuration.

2. You should see the **Role Portal**, with links to:

   - Tagger Workbench
   - Supervisor Monitor
   - Admin Cockpit
   - Research Explorer

3. Check the API health endpoint in a browser or via `curl`:

   ```bash
   curl http://localhost:8000/health
   ```

   A simple JSON response indicates the API is up.

## 4. Authentication and roles in dev mode

The backend uses a simple header-based RBAC scheme (`backend/services/auth.py`):

- `X-User-Id` (default: `1`)
- `X-User-Role` (default: `"tagger"`)

Admin-only endpoints (e.g. Monitor and Admin routers) require `X-User-Role: admin`.

For local development, the v3.3.7 frontend wires this header into the Monitor and Admin
clients by constructing:

- `new ApiClient('/api/v1/monitor', { 'X-User-Role': 'admin' });`
- `new ApiClient('/api/v1/admin', { 'X-User-Role': 'admin' });`

If you deploy behind a reverse proxy or an auth gateway, you can remove or override these
defaults and have your infrastructure inject the appropriate headers.

## 5. Common issues

- **Docker not running**  
  Symptoms: `docker-compose` commands fail or hang.  
  Fix: Start Docker Desktop and re-run `./install.sh`.

- **Port already in use**  
  Symptoms: API or frontend container fails to bind to a port (for example 8000).  
  Fix: Stop any process using that port or adjust ports in `deploy/docker-compose.yml`.

- **Database container failing**  
  Symptoms: API logs show connection errors to the DB.  
  Fix: Check Docker Desktop logs for the DB container; ensure volumes and environment
  variables in `deploy/docker-compose.yml` are correct.

## 6. Running tests

The repository includes basic API smoketests under `tests/` plus smoke scripts under `scripts/`.

- To run Python tests (outside Docker):

  ```bash
  pip install -r requirements.txt
  pytest -q
  ```

- To run tests against the Dockerised API, you can use a pattern such as:

  ```bash
  docker-compose -f deploy/docker-compose.yml exec -T api pytest -q
  ```

## 7. CI workflow

The CI recipe (`.github/workflows/ci_v3.yml`) is designed to:

1. Install Python dependencies.
2. Run governance verification:
   - `python scripts/guardian.py verify`
3. Run API smoketests:
   - `pytest tests/test_v3_api.py`
4. Optionally run `scripts/smoke_science.py` as an advisory check.

Teams can adapt this workflow to their own CI provider. The key idea is that every
published release should at least pass basic health checks and governance verification.


## Frontend smoketest expectations

The script `scripts/smoke_frontend.py` is a lightweight end-to-end check
that the portal is reachable and rendering the main shell.

- If Playwright is installed, it will launch a headless Chromium instance
  and wait for the page to reach a network-idle state.
- Otherwise it falls back to a simple HTTP GET and inspects the HTML
  response body.

The smoketest looks for at least one of a small set of key phrases, such as
"image tagger", "tagger workbench", "admin cockpit", or "research explorer".
If you significantly rename the portal or landing page copy, update the
list of phrases in `smoke_frontend.py` to keep the smoketest aligned with
the UI text.

## 5. BN / DB Health and Legacy Migration (v3.4.63+)

These steps are intended for TAs or developers who are running a real PostgreSQL
database with non-trivial data (i.e. more than just a few test images).

### 5.1 BN / DB health check

The BN / DB health checker verifies two things:

- Every ``Validation.attribute_key`` has a corresponding row in ``attributes.key``.
- Every BN candidate key exposed by the science index catalog also has a
  corresponding Attribute row.

To run the checker inside the Docker stack:

```bash
cd deploy
docker-compose exec api python -m backend.scripts.bn_db_health
```

A healthy database will report ``"ok": true``. If you see orphan validation keys
or missing candidate keys, treat this as a configuration or data hygiene issue
before running large BN exports.

You can also wire this into the governance guardian by setting
``check_bn_db_health: true`` under ``constraints`` in ``v3_governance.yml``.
This is best done once your Postgres instance is stable and seeded.

### 5.2 Legacy FK migration for Validation.attribute_key

Databases created before v3.4.63 may lack a real foreign key from
``Validation.attribute_key`` to ``attributes.key``. For new databases created
via the current ``install.sh`` (which calls ``Base.metadata.create_all``),
the constraint should already be present.

For older databases, use the migration helper (idempotent and safe to run
multiple times):

```bash
cd deploy
docker-compose exec api python -m backend.scripts.migrate_3_4_63_add_validation_fk
```

The script will:

- Inspect the live schema to see if the FK already exists.
- If missing, issue an ``ALTER TABLE`` to add the constraint.
- Print a short, human-readable summary of what it did.

After running the migration, you may re-run the BN / DB health checker to confirm
that there are no remaining orphan keys.
----- CONTENT END -----
----- FILE PATH: docs/governance_guide.md
----- CONTENT START -----
# Governance & Guardian Guide (v3.2.39)

This document explains how repository governance is enforced in the v3 line and how to work
with the `Guardian` tool without getting stuck.

## 1. Governance goals

The v3 line is designed to be:

- Inspectable and teachable.
- Resistant to unintentional drift and file loss.
- Friendly to multi-agent AI development (multiple AIs + humans editing the repo).

Key principles:

1. **No silent deletions**  
   Once a file path has existed in a release, it should not disappear in later versions.
   If code is replaced, the old version is moved under `archive/` with a version tag.

2. **Minimal viable stubs**  
   Active modules should not contain `...` placeholders or pseudo-code. If a feature is not
   yet implemented, it should be clearly marked with comments, but the module must still
   import and run without crashing.

3. **Reproducible releases**  
   Each release should be shippable as:
   - A ZIP of the full tree, and
   - A single concatenated TXT with a `deconcat.py` header so the tree can be reconstructed.

## 2. v3_governance.yml

The file `v3_governance.yml` describes:

- **protected_scopes** ‚Äì directories and patterns that are considered critical
  (for example `backend/science/`, `backend/api/`, `deploy/`, `scripts/guardian.py`).
- **critical_files** ‚Äì specific files that must not be deleted or modified lightly
  (for example main app entrypoints and governance config).

You can read this YAML file to see which areas of the repo are most tightly controlled.

## 3. Guardian commands

The `scripts/guardian.py` tool provides two main operations:

- `freeze` (or `snapshot`) ‚Äì capture the current state of protected files into a lock file.
- `verify` ‚Äì compare the current tree against the lock file and report differences.

Typical workflow:

```bash
# First-time setup after a clean release
python scripts/guardian.py freeze

# Before proposing a new release
python scripts/guardian.py verify
```

If `verify` reports drift, you can either:

- Fix the changes (for example restore accidentally deleted files), or
- Intentionally update the baseline by running `freeze` again *after review*.

## 4. Using Guardian with install.sh and CI

- The `install.sh` script may invoke Guardian to detect drift in a local checkout.
- The CI workflow (`.github/workflows/ci_v3.yml`) is expected to run:

  ```bash
  python scripts/guardian.py verify
  ```

When Guardian fails in CI:

- Treat it as a signal that the repo's critical surfaces have changed.
- Review the diffs carefully before updating the baseline (`freeze`).

## 5. Guidelines for students

If you are a student or collaborator, please:

- **Do**:
  - Add new modules, scripts and docs as needed.
  - Modify existing files to improve science, UX or tests.
  - Use `archive/` to keep old versions when making major rewrites.

- **Do not**:
  - Delete files that shipped in a prior version without archiving them.
  - Remove governance-related files (for example `v3_governance.yml`, `scripts/guardian.py`).
  - Disable Guardian checks in CI without discussing it with the project lead.

When in doubt, ask a TA or project lead before modifying anything under a protected scope.

## 6. Drift Shield in practice

The intent is not to block experimentation but to make it explicit when the public surface
of the system changes. Guardian helps answer questions such as:

- ‚ÄúDid we lose any science module files between versions?‚Äù
- ‚ÄúHave API contracts changed without updating tests and docs?‚Äù

By respecting these governance rules, teams can safely iterate on the system while keeping
its history and public surface stable.


## Guardian in everyday work

A few common scenarios and what to do:

1. **You have made local changes and want to cut a new release**

   - Run `scripts/guardian.py verify` to confirm that you have not broken
     any governance rules (e.g., file deletions in protected scopes).
   - If you are satisfied with the state, bump `VERSION` and the visible
     version strings in the README and backend banner.
   - Run `scripts/guardian.py freeze` to mint a new `governance.lock`
     baseline for this release.

2. **Guardian verification fails after some edits**

   - Read the error message; it will typically say which file or scope
     violated the rules.
   - If you removed a protected file by mistake, restore it from version
     control or from the last release ZIP / concatenated TXT.
   - Re-run `scripts/guardian.py verify` until it passes.

3. **You are collaborating with an AI assistant**

   - Instruct the assistant not to delete files and to keep historical
     directories under `archive/` rather than removing them.
   - Ask it to provide both a ZIP and a concatenated TXT with a `deconcat.py`
     script so that the full repository can be reconstructed elsewhere.
   - Use `scripts/guardian.py verify` on the result before merging it into
     your main branch or release line.

Guardian is deliberately conservative: if it complains, treat that as a
signal to inspect the change rather than as an obstacle.
----- CONTENT END -----
----- FILE PATH: docs/science_overview.md
----- CONTENT START -----
# Science Overview (v3.2.39)

This document explains the heuristics-based science pipeline implemented in `backend/science/`
and how its outputs are used by the Explorer, Monitor, and downstream analysis tools.

## 1. Pipeline sketch

1. Images are stored in the database via the `Image` model (`backend/models/assets.py`), with a
   `storage_path` or similar file reference.
2. The science pipeline orchestrator (`backend/science/pipeline.py`) loads pixels into an
   `AnalysisFrame` abstraction.
3. A set of analyzers run over this frame, each responsible for a family of measures:

   - Color and luminance (`backend/science/color.py`)
   - Texture via gray-level co-occurrence matrix (GLCM) (`backend/science/texture.py`)
   - Fractal dimension (`backend/science/fractals.py`)
   - Visual complexity (`backend/science/complexity.py`)
   - Perceptual / depth cues (`backend/science/perception.py`)
   - Social disposition (rule-based) (`backend/science/context_social.py`)
   - Cognitive/restorative disposition (rule-based) (`backend/science/context_cognitive.py`)
   - Summary / composite indices (`backend/science/summary.py`)

4. Each analyzer adds attributes to the frame using a simple API such as:

   ```python
   frame.add_attribute("science.edge_density", value, confidence=0.9)
   ```

5. At the end of the pipeline, `_save_results` (in `pipeline.py`) persists these attributes
   into the `Validation` table (`backend/models/annotation.py`) as science-sourced rows
   (e.g. `source='science_pipeline_v3.2'`).

6. These persisted attributes can be:

   - Visualised and filtered in the Explorer GUI.
   - Used as moderators in the Monitor (Supervisor) GUI.
   - Exported as BN-ready rows via the `/v1/export/bn-snapshot` endpoint.

## 2. Primitive measures

The current v3.2.39 line implements a modest but coherent set of primitives, including:

- **Color primitives** (in `color.py`):

  - Mean luminance and saturation in a perceptually uniform colour space.
  - Warm/cool ratio based on hue ranges.
  - Simple colour entropy over a coarse histogram.

- **Texture / GLCM primitives** (in `texture.py`):

  - GLCM contrast, homogeneity, energy, correlation on a downsampled grayscale channel.

- **Fractal measures** (in `fractals.py`):

  - Global fractal dimension `D` estimated via box-counting over a binarised edge map.

- **Complexity primitives** (in `complexity.py`):

  - Edge density: proportion of edge pixels relative to total image pixels.
  - Organisation ratio: heuristic ratio of structured vs noisy regions, derived from local
    variance and edge continuity.

These primitives are deterministic for a given image and configuration and are intended as
transparent, inspectable building blocks rather than final psychological truths.

## 3. Composite indices and bins

`backend/science/summary.py` combines several primitives into composite indices that are easier
to interpret in teaching and exploratory analysis:

- **Visual richness** (`science.visual_richness`)
  - Combines colour entropy, edge density and texture variation into a 0‚Äì1 composite.
- **Organised complexity** (`science.organized_complexity`)
  - Combines fractal dimension and organisation ratio into a 0‚Äì1 composite.

Both composites are also discretised into bins for BN and UX:

- `science.visual_richness_bin`
- `science.organized_complexity_bin`

The binning rule is:

- `0` ‚Üí low
- `1` ‚Üí mid
- `2` ‚Üí high

The BN export layer (`backend/api/v1_bn_export.py`) maps these numeric codes to string labels
(`"low"`, `"mid"`, `"high"`) so that downstream BN tools work purely with categorical
values.

## 4. Index catalog

`backend/science/index_catalog.py` defines a canonical index catalog. It exposes:

- `INDEX_CATALOG`: a dict mapping attribute keys to metadata (label, description, type, bins).
- `get_candidate_bn_keys()`: returns attribute keys recommended as BN inputs.
- `get_index_metadata()`: returns the full catalog for metadata endpoints.

The catalog keeps the BN export layer, Explorer, Monitor and notebooks in sync about which
indices exist and how they should be interpreted.

## 5. Relation to Explorer and Monitor

- The **Explorer** GUI can show science attributes as columns and allow filtering/sorting
  by indices and bins. In v3.2.39 it primarily uses attribute keys; a future iteration may
  fetch labels/descriptions from the index catalog for richer tooltips.

- The **Monitor** GUI can use science indices (especially composites and bins) as moderators
  in its analyses of tagging velocity and inter-rater reliability. For example, you can
  ask whether difficult images (high organised complexity) systematically slow taggers or
  increase disagreement.

## 6. BN export

The BN export endpoint (`/v1/export/bn-snapshot`) produces a list of `BNRow` objects (`backend/schemas/bn_export.py`):

- `image_id`: primary key of the image.
- `source`: software version string (e.g. `"image_tagger_v3.2.39"`).
- `indices`: a dict of {attribute_key ‚Üí float or null} for continuous indices.
- `bins`: a dict of {bin_attribute_key ‚Üí label or null} for categorical bins.

The underlying implementation reads from the `Validation` table, restricted to science
pipeline sources, and uses the numeric‚Üílabel mapping described above.

## 7. Extending the pipeline

To add a new index:

1. Implement an analyzer in `backend/science/` with a function such as
   `analyze(frame: AnalysisFrame)` that calls `frame.add_attribute(...)`.
2. Add a corresponding entry to `backend/science/index_catalog.py` (with optional bins).
3. Register the analyzer in `backend/science/pipeline.py` so it runs for each image.
4. If the index should be part of BN export, tag it as a `"candidate_bn_input"` in the
   catalog and, if needed, define a binning strategy.

The goal of this v3.2.39 line is to be modest, explicit and inspectable: science code is
short enough to read in a seminar, and all heuristics are visible and adjustable.
----- CONTENT END -----
----- FILE PATH: docs/ops/Cloud_AntiGravity_Quickstart.md
----- CONTENT START -----
# ‚òÅÔ∏è Cloud "Anti-Gravity" Quickstart (3.4.74_vlm_lab_notebook_TL_runbook)

This guide explains how to run **Image Tagger 3.4.74_vlm_lab_notebook_TL_runbook** in the cloud with **minimal local setup**.
It offers two tracks:

- **TRACK A ‚Äì Full Stack (persistent)**: full application with Workbench / Explorer / Admin.
- **TRACK B ‚Äì Science Notebook (ephemeral)**: a lightweight Colab-based lab that focuses on the
  science pipeline and VLM health checks.

---

## 1. Persistent vs. Ephemeral Environments

- **Persistent environments** (TRACK A):
  - Example: GitHub Codespaces, a cloud VM (AWS/GCP/Azure), or a lab server.
  - When you stop and restart the machine, your files and database state **are still there**
    (unless you explicitly delete them).
  - Ideal for multi-week projects, teaching labs, and anything that needs continuity.

- **Ephemeral environments** (TRACK B / Colab):
  - Example: Google Colab free-tier notebook sessions.
  - The runtime is **temporary**: when the notebook disconnects, times out, or you close it,
    anything stored on the notebook filesystem (e.g. `/content/...`) **disappears**.
  - Only things you explicitly save to **Google Drive** or download to your own computer survive.
  - Ideal for short experiments, demos, and self-contained labs.

In this guide:

- TRACK A gives you a **persistent full Image Tagger instance**.
- TRACK B gives you an **ephemeral "Science Lab"** that you can re-run quickly, but should not
  be treated as long-term storage.

---

## 2. TRACK A ‚Äì Full Stack (for TAs & Admins)

**Goal:** Run the full app (Workbench, Explorer, Admin, API) with a GUI.

**Best for:**
- Architecture demos and tagging sessions.
- Admin / TA configuration and testing.
- Multi-week projects that benefit from persistent data.

**Requires:** A cloud environment with Docker (e.g., GitHub Codespaces, AWS EC2, GCP VM, lab server).

### 2.1 GitHub Codespaces (Recommended)

1. Push or upload this repo (`Image_Tagger_3.4.74_vlm_lab_notebook_TL_runbook`) to GitHub.
2. In GitHub, open the repository page.
3. Click **Code ‚Üí Codespaces ‚Üí Create codespace on main**.
4. When the Codespace terminal is ready, run:

   ```bash
   ./auto_install.sh
   ```

5. In the **Ports** panel in Codespaces:
   - Find the port serving the main frontend (for example `8080`).
   - Click the globe icon to open the forwarded URL in your browser.

When you stop and restart the Codespace later, your data (Postgres, files under the repo) will still be there,
unless you delete the Codespace.

### 2.2 Generic Cloud VM (AWS/GCP/Azure or lab server)

1. Provision an Ubuntu 22.04+ VM (or use an existing lab machine).
2. Copy the `Image_Tagger_3.4.74_vlm_lab_TL_runbook_full.zip` artifact onto the machine, or clone from Git.
3. SSH into the machine and run:

   ```bash
   chmod +x infra/cloud/full_stack_vm_setup.sh
   ./infra/cloud/full_stack_vm_setup.sh
   ```

   This helper script will:

   - install Docker if needed,
   - unpack the repo ZIP into an `image_tagger` directory (if the ZIP is present),
   - run `./auto_install.sh` from inside the repo.

4. Access the UI:

   - If you are working over SSH with a browser on your local machine, you can:
     - forward ports via SSH, or
     - set up a reverse proxy / ingress.
   - For simple setups, the script prints instructions to use **ngrok**:

     ```bash
     ngrok http 8080
     ```

     Once ngrok is running, it shows a public URL you can give to students for short-term demos.

**Important:** A VM is **persistent** as long as you keep it running (or stop/start it without deleting its disk).
Do not treat it as disposable unless you intend to lose all stored images and tags.

---

## 3. TRACK B ‚Äì Science Notebook (for Students and Labs)

**Goal:** Run the **science pipeline + VLM health checks** on a small example dataset without
installing Docker or the full stack.

**Best for:**

- Data analysis and method demonstrations.
- Labs focused on VLM variance, psychometrics, and Turing-style evaluation.
- Students working on personal laptops that cannot run Docker.

**Requires:** A Google account and access to **Google Colab (Free Tier)**.

### 3.1 Running the VLM Health Lab notebook

1. Obtain the artifact:

   - Download the repository zip specified by your instructor, for example:
     - `Image_Tagger_3.4.74_vlm_lab_TL_runbook_full.zip`.

   - Locate the Colab notebook in the repo:
     - `notebooks/VLM_Health_Lab.ipynb`.

2. Open Colab:

   - Go to https://colab.research.google.com/
   - Click **File ‚Üí Upload notebook** and select `VLM_Health_Lab.ipynb`.

3. Follow the notebook cells:

   - **Step 1: Setup environment**  
     Installs Python libraries and a lightweight PostgreSQL instance.

   - **Step 2: Upload repo zip**  
     Unpacks the repo into `/content/repo` and switches to that directory.

   - **Step 3: Seed tiny image set**  
     Creates database tables, runs seed scripts (if present), and generates synthetic architectural images.

   - **Step 4: Run science pipeline**  
     Runs the Image Tagger science pipeline on the toy images.  
     In stub mode, the VLM returns neutral outputs; this is enough to exercise the pipeline.

   - **Step 5: Run VLM health audit**  
     Runs `scripts/audit_vlm_variance.py` and loads the resulting CSV using pandas.

4. Saving results:

   - Download the variance CSV(s) and any other outputs you care about.
   - Or write them to a mounted Google Drive folder.

**Reminder:** The Colab filesystem is **ephemeral**. Do not store anything important only in `/content`.

---

## 4. Which Track Should I Use?

- **Instructors / TAs / Technical Lead:**
  - Use **TRACK A (Full Stack)** for:
    - running the live system in class,
    - letting students tag real images via Workbench/Explorer,
    - running ongoing experiments where data persistence matters.
  - Use **TRACK B (Notebook)** when:
    - you need a low-friction lab on the science/VLM side,
    - students cannot run Docker,
    - or you want a standardised small experiment.

- **Students:**
  - Follow the instructions your TA gives.
  - Track A feels like a web app; Track B feels like a notebook-based lab.

---

## 5. Technical Lead Checklist

Before the course begins, the Technical Lead should:

- [ ] Bring up at least one **TRACK A** instance (Codespaces or VM) and confirm:
      - `./auto_install.sh` completes successfully.
      - Workbench, Explorer, and Admin load.
- [ ] Run the **TRACK B** notebook once end-to-end in Colab and confirm:
      - all five steps execute without error,
      - at least one variance CSV is produced.
- [ ] Update lab handouts to:
      - specify which track is in use,
      - point to the correct URL (Track A) or notebook + zip (Track B),
      - remind students about persistence vs. ephemerality.

With these steps done, your ‚Äúanti-gravity‚Äù deployment is ready for teaching.
----- CONTENT END -----
----- FILE PATH: docs/ops/Student_Quickstart_v3.4.73.md
----- CONTENT START -----
# üéì Student Quickstart ‚Äì Image Tagger 3.4.74_vlm_lab_notebook_TL_runbook

This page explains **how you will actually use Image Tagger in this course**.

You do not need to understand every internal detail.
You just need to know which path you are on and what to run or click.

---

## 1. What Image Tagger is (for you)

Image Tagger is a system that:

- stores architectural images,
- runs a science pipeline (visual and spatial metrics, and optionally VLMs),
- and lets you explore the results through web interfaces.

In this course you will use Image Tagger to:

- run defined experiments,
- inspect tags and metrics,
- and connect them to theoretical ideas about space, perception, and affect.

You are **not** expected to maintain the infrastructure.

---

## 2. Two tracks: Full App vs Colab Lab

Your instructor or TA will tell you which track you are using:

### Track A ‚Äì Full App (persistent, Docker-based)

- You access Image Tagger as a normal web app (via a URL or localhost).
- The system runs in Docker containers on:
  - a lab machine, or
  - a cloud VM / GitHub Codespace, or
  - occasionally your own laptop (if you have Docker and are comfortable using it).
- The database and image store are **persistent**:
  - if the machine is stopped and restarted, your data remains available (unless explicitly reset).

**You will:**

- visit a URL your TA provides,
- use Workbench to launch tagging jobs (if part of your assignment),
- use Explorer to view and compare images and tags,
- possibly look at Admin views if the assignment asks you to.

### Track B ‚Äì Colab Science Notebook (ephemeral VLM Health Lab)

- You work in **Google Colab** using the notebook:
  - `notebooks/VLM_Health_Lab.ipynb`
- The notebook:
  - unpacks a copy of the Image Tagger repository,
  - sets up a small local database,
  - generates a tiny synthetic image set,
  - runs the science pipeline,
  - and then runs a VLM variance audit.

**Important:** Colab is **ephemeral**.

- When the Colab runtime disconnects, everything under `/content` is wiped:
  - the unpacked repo,
  - any CSVs,
  - any generated reports.

**How to keep your work:**

- Save the notebook itself to Google Drive.
- Download CSVs and text summaries you need for your writeup.
- Optionally mount Google Drive and save outputs there.

---

## 3. Track A ‚Äì How to use the Full App

If your TA says you are using **Track A**, they will usually provide a URL for you to open.

### 3.1 If you are given a URL

1. Open the URL in a modern browser (Chrome, Edge, Firefox, Safari).
2. Follow your lab or assignment sheet, which will tell you:
   - whether to start in **Workbench** (running a job),
   - or **Explorer** (inspecting existing results),
   - or a specific view in **Admin**.

You should not need to run any terminal commands in this mode.

### 3.2 If you are asked to run locally with Docker

This will only happen if you are comfortable with Docker and your machine can support it.

1. Install Docker Desktop (or Docker Engine) if you do not already have it.
2. Unzip the Image Tagger repository into a folder on your machine.
3. Open a terminal in that folder and run:

   ```bash
   ./auto_install.sh
   ```

   The first run may take several minutes.

4. When instructed (by your TA or the script), start the stack (for example):

   ```bash
   docker compose up
   ```

5. Open the URL indicated by your TA (often `http://localhost:8080`).

If you hit errors in this mode, provide the terminal output and any error messages to your TA.

---

## 4. Track B ‚Äì How to use the Colab VLM Health Lab

If your TA says you are using **Track B**, you will work primarily in Google Colab.

1. Make sure you have:
   - a Google account, and
   - the Image Tagger zip specified by your instructor (for example: `Image_Tagger_3.4.74_vlm_lab_TL_runbook_full.zip`).

2. Open Google Colab:
   - Go to https://colab.research.google.com/
   - Choose **File ‚Üí Upload notebook**.
   - Select `notebooks/VLM_Health_Lab.ipynb` from the repository.

3. In the notebook, run the cells in order:

   - **Step 1 ‚Äì Setup Environment**  
     Installs required libraries and starts a local PostgreSQL database inside Colab.

   - **Step 2 ‚Äì Upload Repository Zip**  
     You will be prompted to upload the Image Tagger zip.
     The notebook unpacks it into `/content/repo`.

   - **Step 3 ‚Äì Seed Database & Generate Toy Images**  
     Creates database tables, seeds configuration (if seed scripts are present), and generates a handful of synthetic ‚Äúarchitectural‚Äù images.

   - **Step 4 ‚Äì Run Science Pipeline**  
     Runs the Image Tagger science pipeline on the toy images.
     If no API keys are set, the system uses a stub engine for VLM calls (which is fine for plumbing tests).

   - **Step 5 ‚Äì Run VLM Variance Audit**  
     Runs `scripts/audit_vlm_variance.py` and then loads and displays the resulting CSV.

4. At the end of the lab:

   - Download the CSV(s) and any text summaries the lab asks you to submit.
   - Optionally save them to Google Drive.

**Reminder:** if you simply close the tab or leave the notebook idle until it disconnects, all files in `/content` vanish.

---

## 5. When something breaks

Before contacting your TA:

- Note **which track** you are using (Track A vs Track B).
- Take a screenshot of the error.
- Capture:
  - the URL and which page you were on (Track A), or
  - the notebook cell you just executed (Track B).
- Write one‚Äìtwo sentences describing:
  - what you were trying to do,
  - what you expected,
  - what actually happened.

Send this to your TA. This will greatly accelerate debugging.

---

## 6. One-sentence summary

- Track A: **web app**, persistent, usually accessed via a URL your TA provides.  
- Track B: **Colab notebook**, ephemeral, used for a small, scripted VLM health experiment.

If you are unsure which track you‚Äôre on, ask your TA **before** trying to install or run anything yourself.
----- CONTENT END -----
----- FILE PATH: docs/ops/Technical_Lead_Runbook_v3.4.74.md
----- CONTENT START -----
# üõ†Ô∏è Technical Lead Runbook ‚Äì Image Tagger 3.4.74_vlm_lab_notebook_TL_runbook

This document is for the **Technical Lead (TL)** responsible for:

- getting Image Tagger up and running,
- verifying that both teaching tracks work, and
- making the system easy to share with students and collaborators.

It assumes you are comfortable with basic command-line work and Docker.

---

## 1. What you are supporting

The repository supports two main teaching modes:

- **Track A ‚Äì Full App (persistent)**  
  - Full stack with Workbench, Explorer, Admin.
  - Runs in Docker on a server, Codespace, or lab machine.
  - Suitable for multi-week projects and demos with persistent data.

- **Track B ‚Äì Colab ‚ÄúVLM Health Lab‚Äù (ephemeral)**  
  - Google Colab notebook: `notebooks/VLM_Health_Lab.ipynb`.
  - Sets up a small DB and toy image set inside Colab.
  - Runs the science pipeline and VLM variance audit.
  - Ideal for short, self-contained labs that do not require persistence.

Your goal is to ensure that **at least one Track A deployment** and the **Track B notebook** are working
and documented before students arrive.

---

## 2. Artifacts you need

- The repository zip, for example:
  - `Image_Tagger_3.4.74_vlm_lab_notebook_TL_runbook_full.zip`
- The core docs inside the repo:
  - `STUDENT_START_HERE.md`
  - `docs/ops/Student_Quickstart_v3.4.73.md`
  - `docs/ops/Cloud_AntiGravity_Quickstart.md`
  - `docs/ops/VLM_Health_Quickstart.md` (if present)
  - `docs/ops/VLM_Health_SOP.md` (if present)
  - `docs/ops/Technical_Lead_Runbook_v3.4.74.md` (this file)

Optional but recommended external docs (if provided):

- A **Repo Overview** one-pager (DOCX or PDF).
- A **TA & Student Guide** (DOCX or PDF).

---

## 3. Track A ‚Äì Full App Deployment

You have three main options: local machine, GitHub Codespaces, or cloud VM / lab server.

### 3.1 Local machine (for your own development / testing)

1. Install Docker and docker-compose.
2. Unzip the repository into a folder on your machine.
3. Open a terminal in that folder and run:

   ```bash
   ./auto_install.sh
   ```

   - First run: builds images, runs migrations and seeds.
   - Subsequent runs: should be quicker, mainly verifying that things still work.

4. Start the stack (command may vary slightly by repo config):

   ```bash
   docker compose up
   ```

5. Open the main frontend URL (often `http://localhost:8080` or as documented in the code).

**Smoke test:**

- Confirm you can:
  - open Workbench,
  - open Explorer,
  - access Admin (and see VLM Health views).

### 3.2 GitHub Codespaces (recommended for remote teaching)

1. Ensure the repository is on GitHub.
2. From the GitHub repo page:
   - Click **Code ‚Üí Codespaces ‚Üí Create codespace on main**.
3. Once the Codespace is ready, open the integrated terminal and run:

   ```bash
   ./auto_install.sh
   ```

4. Use the **Ports** panel in Codespaces:
   - Find the port bound to the main frontend.
   - Click the globe icon to open it in your browser.
   - Optionally make the port public for temporary sharing in class.

**Advantages:**

- All students with a browser can access the same instance (if you open the URL).
- No local installation on student machines.
- You can snapshot or re-create environments as needed.

### 3.3 Cloud VM or lab server

1. Provision an Ubuntu 22.04+ VM (or choose an existing lab server).
2. Copy the repository zip to the VM.
3. SSH into the VM and run:

   ```bash
   chmod +x infra/cloud/full_stack_vm_setup.sh
   ./infra/cloud/full_stack_vm_setup.sh
   ```

   This script will:

   - install Docker if necessary,
   - unpack the repo zip into an `image_tagger` directory (if present),
   - run `./auto_install.sh`.

4. Expose the frontend:

   - For quick demos, use `ngrok http 8080` and share the generated URL.
   - For longer-term use, configure a proper reverse proxy or load balancer.

**Checklist for Track A:**

- [ ] `./auto_install.sh` completes without errors.
- [ ] Workbench loads.
- [ ] Explorer loads.
- [ ] Admin loads and VLM Health pages are reachable.
- [ ] Your chosen sharing mechanism (Codespaces URL, VM+ngrok) is documented for students.

---

## 4. Track B ‚Äì Colab VLM Health Lab

The notebook `notebooks/VLM_Health_Lab.ipynb` is a self-contained ‚Äúscience lab‚Äù path.

### 4.1 TL verification

1. Download `Image_Tagger_3.4.74_vlm_lab_notebook_TL_runbook_full.zip` to your local machine.
2. Open the notebook in Google Colab:
   - Go to https://colab.research.google.com/
   - Choose **File ‚Üí Upload notebook**, select `notebooks/VLM_Health_Lab.ipynb`.
3. Run each cell in order:
   - Step 1: environment setup (libraries + Postgres).
   - Step 2: upload the repo zip.
   - Step 3: DB init + seeds + synthetic images.
   - Step 4: run science pipeline.
   - Step 5: run VLM variance audit and view the CSV.

4. Confirm that:
   - the notebook runs end-to-end without crashing,
   - at least one variance CSV is produced and displayed.

### 4.2 Sharing instructions with TAs and students

- Make sure the **Student Quickstart** and any lab handouts:
  - clearly label this as **Track B ‚Äì Colab VLM Health Lab**,
  - mention its **ephemeral** nature,
  - and provide the correct zip and notebook to use.

- Optionally create:
  - a short screencast or screenshot sequence,
  - or a one-page PDF summarising the steps.

---

## 5. Minimal GO/NO-GO Gate for the Technical Lead

Before the course starts, the TL should be able to answer **YES** to:

1. **Governance / guards:**
   - [ ] `python scripts/syntax_guard.py` passes.
   - [ ] `python scripts/program_integrity_guard.py` passes.
   - [ ] `python scripts/critical_import_guard.py` passes.
   - [ ] `python scripts/canon_guard.py` passes.

2. **Track A:**
   - [ ] I can run `./auto_install.sh` to completion in at least one environment (local, Codespace, VM).
   - [ ] I can open Workbench, Explorer, and Admin in a browser.
   - [ ] I know what URL to give students (and under what conditions).

3. **Track B:**
   - [ ] I can run the full `VLM_Health_Lab.ipynb` notebook in Colab without errors.
   - [ ] I know which zip students should upload.
   - [ ] I have told TAs which parts of the notebook matter for their assignments.

4. **Documentation:**
   - [ ] `STUDENT_START_HERE.md` and `docs/ops/Student_Quickstart_v3.4.73.md` exist and reflect our actual teaching plan.
   - [ ] `docs/ops/Cloud_AntiGravity_Quickstart.md` is accurate for our deployment strategy.
   - [ ] TAs know where to find any external guides (Repo Overview, TA & Student Guide).

If any of these are ‚Äúno,‚Äù treat that as a **pre-course bug** and resolve it before students touch the system.

---

## 6. Communication with the teaching team

Share with TAs:

- where the Track A instance lives (URL, credentials if any),
- whether Track B will be used and how,
- what *not* to change (e.g., governance files, guard scripts),
- how to escalate issues (what logs to send you, what screenshots to collect).

With this runbook and the in-repo quickstarts, the Technical Lead should be able to maintain a
stable teaching instance and support both the full application and the Colab lab.
----- CONTENT END -----
----- FILE PATH: docs/ops/VLM_Health_Quickstart.md
----- CONTENT START -----
# VLM Health Quickstart

This is the short version of the VLM health procedure for TAs and students.

Use this when you just need to run the standard checks and hand the results
to the PI, without reading the full SOP.

---

## Prerequisites

- You can run the normal Image Tagger pipeline end-to-end.
- The following CSVs have already been generated for the run you care about:
  - `reports/bn_validations_flat.csv`
  - `reports/vlm_validations.csv`
  - `reports/human_validations.csv`

If you are not sure whether these exist, ask your supervisor before running
the health checks.

---

## Step 1 ‚Äì Initialise a health run

From the repo root:

```bash
make vlm-health-init
```

This will:

- Create a new folder under `reports/vlm_health/` named with today‚Äôs date,
  the current VERSION file, and a default VLM profile (e.g. `main_vlm`).
- Copy the three input CSVs into `raw/` inside that folder.

You should see a message telling you which `RUN_ID` it used.

---

## Step 2 ‚Äì Run the variance audit

```bash
make vlm-health-audit
```

This will run `scripts/audit_vlm_variance.py` on the flat BN validations
export and write a CSV of potentially problematic attributes to:

```text
reports/vlm_health/<RUN_ID>/derived/vlm_variance_audit.csv
```

At minimum:

- Open this CSV.
- Sort by `dominant_ratio` and `n`.
- Note any attributes that look suspicious (e.g. almost always the same value).

You do not have to fix anything; just flag them in the log.

---

## Step 3 ‚Äì Prepare the Turing panel

```bash
make vlm-health-panel
```

This will create:

```text
reports/vlm_health/<RUN_ID>/raw/vlm_turing_panel.csv
```

Send this file (or a Google Sheet based on it) to the human judges who will
compare VLM vs human labels. They should fill in at least:

- `rater_id`
- `guess_is_ai` (A or B)
- optionally `rating_A` and `rating_B` if you are using numeric ratings.

When they are done, save the completed CSV as:

```text
reports/vlm_health/<RUN_ID>/raw/vlm_turing_panel_completed.csv
```

---

## Step 4 ‚Äì Score the Turing panel

```bash
make vlm-health-score
```

This will run `scripts/vlm_turing_test_score.py` on the completed panel and
write a text summary to:

```text
reports/vlm_health/<RUN_ID>/derived/vlm_turing_summary.txt
```

Open this file and check that it contains:

- Total number of judgments.
- Overall accuracy of `guess_is_ai` vs the hidden ground truth.
- Per-rater accuracy.
- Mean ratings for AI vs human labels (if ratings were provided).

---

## Step 5 ‚Äì Fill in the log

In the same folder, open or create:

```text
reports/vlm_health/<RUN_ID>/log.md
```

Add a short note with:

- Date, repo version, VLM profile (copy the RUN_ID at the top).
- Where the audit and Turing files live.
- Any attributes you think look suspicious in the variance audit.
- Whether the Turing summary ‚Äúlooks okay‚Äù (no need for deep interpretation).

Example sketch:

```markdown
# VLM health log ‚Äì 2025-11-27_v3.4.69_main_vlm

- Audit CSV: derived/vlm_variance_audit.csv
- Turing summary: derived/vlm_turing_summary.txt

Noted attributes:
- `ceiling_height` has very high dominance; flagged for PI review.

Turing headline:
- Overall accuracy ~0.53; nothing obviously broken.

```

Once this is done, send the RUN_ID and the log file to the PI or lead
researcher as instructed.
----- CONTENT END -----
----- FILE PATH: docs/ops/VLM_Health_SOP.md
----- CONTENT START -----
# VLM Health & Turing SOP

This SOP defines the standard checks we run on the Vision-Language Model (VLM) tagging pipeline and its integration with the BN for CNfA experiments.

It has two main components:

1. A **variance audit**, to detect mode-collapsed or low-variance attributes in VLM outputs.
2. A **Turing-style panel**, where human judges compare anonymised VLM vs human labels.

If you just want the short, operational version of this procedure,
see **`docs/ops/VLM_Health_Quickstart.md`**.

---

## 1. When to run this SOP

Run the VLM Health SOP in each of these situations:

1. **New VLM weights or prompt.**
2. **New BN version that changes observables or binning.**
3. **New major dataset or domain shift.**
4. **Pre-release check** before a public or classroom deployment.

We identify each run by: `YYYY-MM-DD_<repo_version>_<vlm_profile>` (the RUN_ID).

---

## 2. Required scripts

The SOP assumes these scripts exist under `scripts/`:

- `audit_vlm_variance.py`
- `vlm_turing_test_prep.py`
- `vlm_turing_test_score.py`

Each script has its own `--help` describing CLI options.

---

## 3. Folder structure

For each run, create:

```text
reports/
  vlm_health/
    RUN_ID/
      raw/
      derived/
      log.md
```

where `RUN_ID = YYYY-MM-DD_<repo_version>_<vlm_profile>`.

- `raw/`   holds input CSVs and the Turing panel files.
- `derived/` holds audit outputs and Turing summaries.
- `log.md` summarises decisions and follow-ups.

---

## 4. Inputs

The SOP expects three CSVs:

1. `bn_validations_flat.csv`  
2. `vlm_validations.csv`  
3. `human_validations.csv`  

The actual filenames may differ, but they should provide:

- `image_id`
- `attribute_key`
- `source` (for the flat BN export)
- `value` (numeric or ordinal label)

---

## 5. Operational steps

Assume you are at repo root and have already produced the three CSVs.

### 5.1 Initialise the run folder

```bash
RUN_ID="YYYY-MM-DD_vX.Y.Z_profile"

mkdir -p "reports/vlm_health/${RUN_ID}/raw"
mkdir -p "reports/vlm_health/${RUN_ID}/derived"

cp reports/bn_validations_flat.csv "reports/vlm_health/${RUN_ID}/raw/"
cp reports/vlm_validations.csv    "reports/vlm_health/${RUN_ID}/raw/"
cp reports/human_validations.csv  "reports/vlm_health/${RUN_ID}/raw/"
```

### 5.2 Variance audit

```bash
python scripts/audit_vlm_variance.py       "reports/vlm_health/${RUN_ID}/raw/bn_validations_flat.csv"       --out "reports/vlm_health/${RUN_ID}/derived/vlm_variance_audit.csv"       --source-column source       --attribute-column attribute_key       --value-column value       --source-prefix science_pipeline.vlm
```

Review the resulting CSV and note:

- Attributes with high `dominant_ratio` and low variance.
- Whether each flagged attribute is:
  - Acceptable (true domain skew),
  - Needs investigation,
  - Blocking for release.

Document your judgment in `log.md`.

### 5.3 Build the Turing panel

```bash
python scripts/vlm_turing_test_prep.py       --vlm   "reports/vlm_health/${RUN_ID}/raw/vlm_validations.csv"       --human "reports/vlm_health/${RUN_ID}/raw/human_validations.csv"       --out   "reports/vlm_health/${RUN_ID}/raw/vlm_turing_panel.csv"       --max-trials 400       --seed 42
```

Give `vlm_turing_panel.csv` to human judges (e.g., via Google Sheets or a small UI) and ask them to complete:

- `rater_id`
- `rating_A`, `rating_B` (if used)
- `guess_is_ai` (A or B)
- optional `notes`

Save the completed file as:

```text
reports/vlm_health/${RUN_ID}/raw/vlm_turing_panel_completed.csv
```

### 5.4 Score the Turing panel

```bash
python scripts/vlm_turing_test_score.py       --panel "reports/vlm_health/${RUN_ID}/raw/vlm_turing_panel_completed.csv"       > "reports/vlm_health/${RUN_ID}/derived/vlm_turing_summary.txt"
```

The summary includes:

- Overall guess accuracy vs chance.
- Per-rater accuracy.
- Mean ratings of AI vs human labels (if rating fields are present).

Decide whether the VLM is acceptable for the intended use, and document in `log.md`.

---

## 6. Logging

For each run, fill in `reports/vlm_health/${RUN_ID}/log.md` with:

- Date, repo version, VLM profile.
- Pointer to:
  - `derived/vlm_variance_audit.csv`
  - `derived/vlm_turing_summary.txt`
- Short narrative of:
  - Which attributes, if any, are problematic.
  - Whether human judges can reliably distinguish AI vs human labels.
  - Whether apparent quality is acceptable.
- A small checklist of follow-up actions.

Example sections:

- `## 1. Variance audit`
- `## 2. VLM Turing Test`
- `## 3. Actions / follow-ups`

---

## 7. Makefile targets (optional)

For convenience, you can add Makefile targets:

- `vlm-health-init`
- `vlm-health-audit`
- `vlm-health-panel`
- `vlm-health-score`

so that operators can run each stage with a single command.

This SOP should be kept under version control and updated as the VLM, BN,
or evaluation protocol evolves.
----- CONTENT END -----
----- FILE PATH: frontend/index.html
----- CONTENT START -----
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Image Tagger v3.2.36 ‚Äî Role Portal</title>
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 2rem;
      background: #f7f7f9;
      color: #222;
    }
    h1 {
      font-size: 1.8rem;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: #555;
      margin-bottom: 1.5rem;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 1rem;
    }
    .card {
      background: #fff;
      border-radius: 0.75rem;
      padding: 1rem 1.25rem;
      box-shadow: 0 2px 6px rgba(0,0,0,0.06);
      border: 1px solid #e2e2ea;
    }
    .card h2 {
      margin: 0 0 0.25rem 0;
      font-size: 1.1rem;
    }
    .role {
      font-size: 0.9rem;
      color: #777;
      margin-bottom: 0.4rem;
    }
    .card p {
      margin: 0 0 0.6rem 0;
      font-size: 0.9rem;
      line-height: 1.35;
    }
    .link {
      display: inline-block;
      margin-top: 0.4rem;
      font-size: 0.9rem;
      text-decoration: none;
      padding: 0.3rem 0.6rem;
      border-radius: 999px;
      border: 1px solid #0070f3;
      color: #0070f3;
    }
    .link:hover {
      background: #0070f3;
      color: #fff;
    }
    .footnote {
      margin-top: 1.5rem;
      font-size: 0.8rem;
      color: #777;
    }
  </style>
</head>
<body>
  <h1>Image Tagger v3.2.36 ‚Äî Role Portal</h1>
  <div class="subtitle">
    Choose the interface that matches your role. All apps expect the FastAPI backend to be running with <code>/api/v1/...</code> routes enabled.
  </div>
  <div class="grid">
    <div class="card">
      <h2>Tagger Workbench</h2>
      <div class="role">Role: Annotators / junior researchers</div>
      <p>High-throughput labeling of images and regions. Optimized for focus and speed.</p>
      <a class="link" href="/apps/workbench/index.html">Open Tagger Workbench</a>
    </div>
    <div class="card">
      <h2>Supervisor Monitor</h2>
      <div class="role">Role: Leads / QA supervisors</div>
      <p>Monitor tagging velocity, inter-rater reliability, and inspect disagreements at the image level.</p>
      <a class="link" href="/apps/monitor/index.html">Open Supervisor Monitor</a>
    </div>
    <div class="card">
      <h2>Admin Cockpit</h2>
      <div class="role">Role: System owners / budget admins</div>
      <p>Control model selection, tagging budgets, and the global cost kill-switch for API usage.</p>
      <a class="link" href="/apps/admin/index.html">Open Admin Cockpit</a>
    </div>
    <div class="card">
      <h2>Research Explorer</h2>
      <div class="role">Role: Scientists / analysts</div>
      <p>Search the tagged corpus, filter by attributes, and export datasets for training or analysis.</p>
      <a class="link" href="/apps/explorer/index.html">Open Research Explorer</a>
    </div>
  </div>
  <div class="footnote">
    If you see empty dashboards, run the seeding scripts and the science pipeline first (see <code>README_v3.md</code> ‚ÄúQuickstart &amp; Seeding‚Äù).
  </div>
</body>
</html>
----- CONTENT END -----
----- FILE PATH: frontend/package.json
----- CONTENT START -----
{
  "name": "image-tagger-v3-monorepo",
  "private": true,
  "version": "3.0.0",
  "type": "module",
  "workspaces": [
    "apps/*",
    "shared"
  ],
  "scripts": {
    "dev:workbench": "vite apps/workbench --port 3001",
    "dev:monitor": "vite apps/monitor --port 3002",
    "dev:admin": "vite apps/admin --port 3003",
    "dev:explorer": "vite apps/explorer --port 3004",
    "dev:all": "concurrently \"npm run dev:workbench\" \"npm run dev:monitor\" \"npm run dev:admin\" \"npm run dev:explorer\"",
    "build": "npm run build -w apps/workbench && npm run build -w apps/monitor && npm run build -w apps/admin && npm run build -w apps/explorer"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "lucide-react": "^0.292.0",
    "recharts": "^2.9.0"
  },
  "devDependencies": {
    "@vitejs/plugin-react": "^4.2.0",
    "autoprefixer": "^10.4.16",
    "concurrently": "^8.2.2",
    "postcss": "^8.4.31",
    "tailwindcss": "^3.3.5",
    "vite": "^5.0.0"
  }
}----- CONTENT END -----
----- FILE PATH: frontend/postcss.config.js
----- CONTENT START -----
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}----- CONTENT END -----
----- FILE PATH: frontend/tailwind.config.js
----- CONTENT START -----
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./apps/*/index.html",
    "./apps/*/src/**/*.{js,ts,jsx,tsx}",
    "./shared/src/**/*.{js,ts,jsx,tsx}"
  ],
  theme: {
    extend: {
        colors: {
            'enterprise-blue': '#0f172a',
            'action-primary': '#3b82f6',
            'surface-dark': '#1e293b'
        }
    },
  },
  plugins: [],
}----- CONTENT END -----
----- FILE PATH: frontend/vite.config.base.js
----- CONTENT START -----
import react from '@vitejs/plugin-react';
import path from 'path';

/**
 * Shared Vite Configuration
 * Ensures all 4 GUIs use the same aliases and build settings.
 */
export default function getBaseConfig(dirname) {
  return {
    plugins: [react()],
    resolve: {
      alias: {
        '@shared': path.resolve(dirname, '../../shared/src'),
        '@': path.resolve(dirname, './src'),
      },
    },
    server: {
      proxy: {
        '/api': {
          target: 'http://127.0.0.1:8000',
          changeOrigin: true,
          rewrite: (path) => path.replace(/^\/api/, '/v1'),
        },
        '/static': {
            target: 'http://127.0.0.1:8000',
            changeOrigin: true
        }
      },
    },
    build: {
        outDir: `../../dist/${path.basename(dirname)}`,
        emptyOutDir: true
    }
  };
}----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Admin | Image Tagger v3</title>
  </head>
  <body class="bg-gray-50 h-screen">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/package.json
----- CONTENT START -----
{
  "name": "admin",
  "private": true,
  "version": "3.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build"
  }
}----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/vite.config.js
----- CONTENT START -----
import { defineConfig } from 'vite';
import getBaseConfig from '../../vite.config.base';
import path from 'path';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));
export default defineConfig(getBaseConfig(__dirname));----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/public/feature_navigator.html
----- CONTENT START -----
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Repository Explorer</title>
    <!-- 1. Include Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 2. Include PapaParse to parse the CSV -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.4.1/papaparse.min.js"></script>
    <style>
        /* Simple bar chart styles */
        .chart-bar {
            transition: all 0.3s ease;
        }
        .chart-bar:hover {
            opacity: 0.8;
        }
    </style>
</head>
<body class="bg-gray-100 font-sans antialiased">
    <div class="flex h-screen overflow-hidden">
        
        <!-- Sidebar: Filters -->
        <aside class="w-72 bg-white p-6 overflow-y-auto border-r border-gray-200">
            <h2 class="text-lg font-semibold text-gray-800 mb-4">Filter by Category</h2>
            <div id="filter-container" class="space-y-2">
                <!-- Filter checkboxes will be dynamically inserted here -->
                <div class="flex items-center">
                    <input id="filter-all" type="checkbox" checked class="h-4 w-4 rounded border-gray-300 text-blue-600 focus:ring-blue-500" onchange="filterData()">
                    <label for="filter-all" class="ml-3 text-sm font-medium text-gray-700">All Categories</label>
                </div>
            </div>
        </aside>

        <!-- Main Content Area -->
        <main class="flex-1 overflow-y-auto p-8">
            <header class="mb-8">
                <h1 class="text-3xl font-bold text-gray-900">Architectural Feature Repository Explorer</h1>
                <p class="text-gray-600 mt-1">A dashboard to explore, filter, and analyze the feature database.</p>
            </header>

            <!-- Stats & Chart -->
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
                <!-- Summary Stat -->
                <div class="bg-white p-6 rounded-lg shadow">
                    <h3 class="text-sm font-medium text-gray-500">Total Features</h3>
                    <p id="total-features" class="mt-1 text-3xl font-semibold text-gray-900">0</p>
                </div>
                <!-- Summary Stat -->
                <div class="bg-white p-6 rounded-lg shadow">
                    <h3 class="text-sm font-medium text-gray-500">Total Categories</h3>
                    <p id="total-categories" class="mt-1 text-3xl font-semibold text-gray-900">0</p>
                </div>

                <!-- Frequency Chart -->
                <div class="bg-white p-6 rounded-lg shadow md:col-span-3">
                    <h3 class="text-lg font-medium text-gray-900 mb-4">Feature Frequency by Category</h3>
                    <div id="chart-container" class="space-y-2">
                        <!-- Chart bars will be dynamically inserted here -->
                    </div>
                </div>
            </div>

            <!-- Filtered Feature List -->
            <div class="bg-white rounded-lg shadow overflow-hidden">
                <div class="p-6">
                    <h2 class="text-xl font-semibold text-gray-900">Filtered Feature List</h2>
                </div>
                <div class="overflow-x-auto">
                    <table class="min-w-full divide-y divide-gray-200">
                        <thead class="bg-gray-50">
                            <tr>
                                <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Feature Name</th>
                                <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Category</th>
                                <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">CNfA Relevance</th>
                            </tr>
                        </thead>
                        <tbody id="feature-list" class="bg-white divide-y divide-gray-200">
                            <!-- Feature rows will be dynamically inserted here -->
                        </tbody>
                    </table>
                </div>
            </div>
        </main>
    </div>

    <script>
        // Global variable to hold the parsed data
        let allFeatures = [];
        const categoryColors = [
            'bg-blue-500', 'bg-green-500', 'bg-red-500', 'bg-yellow-500', 'bg-purple-500', 
            'bg-pink-500', 'bg-indigo-500', 'bg-teal-500', 'bg-orange-500', 'bg-gray-500',
            'bg-blue-700', 'bg-green-700', 'bg-red-700', 'bg-yellow-700', 'bg-purple-700',
        ];

        // *** FIX: Embed the CSV data directly as a string ***
        const csvData = `
"Feature_Category","Feature_Name","Human_Readable_Label","Method_1_Retrieval_Source","Method_1_Retrieval_Query","Method_2_Extraction_Tool","Method_2_Extraction_Algorithm_or_Query","CNfA_Relevance"
"Style","style.mid_century_modern","Mid-Century Modern","Houzz, Pinterest, ArchDaily, Unsplash API","""mid-century modern"", ""MCM""","VLM (Gemini), CLIP","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Mid-Century Modern? (yes/no)""\`","Cues for 'Eames' schema, 'warm' woods, 'classic' design, low 'ornament'."
"Style","style.japandi","Japandi","Houzz, Pinterest","""japandi""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Japandi? (yes/no)""\`","Cues for 'minimalism', 'biophilia' (wood/rattan), 'calm', 'wabi-sabi' (imperfection)."
"Style","style.scandinavian","Scandinavian","Houzz, Pinterest, Unsplash API","""scandinavian"", ""scandi""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Scandinavian? (yes/no)""\`","Cues for 'lightness' (light woods, white walls), 'minimalism', 'coziness' (hygge), 'functionality'."
"Style","style.industrial","Industrial","Houzz, Pinterest, Unsplash API","""industrial interior""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Industrial? (yes/no)""\`","Cues for 'rawness' (exposed brick/ducts), 'authenticity', 'cold' materials (metal, concrete)."
"Style","style.farmhouse","Farmhouse / Modern Farmhouse","Houzz, Pinterest","""farmhouse"", ""modern farmhouse""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Farmhouse or Modern Farmhouse? (yes/no)""\`","Cues for 'comfort', 'nostalgia', 'rustic' materials (shiplap, barn doors), 'hearth' (social center)."
"Style","style.traditional","Traditional","Houzz, Pinterest","""traditional interior""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Traditional? (yes/no)""\`","Cues for 'formality', 'symmetry', 'ornament', 'heirloom' (status, history)."
"Style","style.minimalist","Minimalist","Houzz, Pinterest, Unsplash API","""minimalist interior""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Minimalist? (yes/no)""\`","Low 'clutter', 'low complexity', 'low cognitive load'. Can be 'calming' or 'sterile'."
"Style","style.rustic","Rustic","Houzz, Pinterest","""rustic interior""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Rustic? (yes/no)""\`","Cues for 'refuge' (cabin), 'biophilia' (raw wood, stone), 'warmth', 'escape from technology'."
"Style","style.bohemian","Bohemian / Boho","Houzz, Pinterest, Unsplash API","""bohemian"", ""boho interior""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is the primary style of this interior Bohemian? (yes/no)""\`","High 'clutter_density', 'personalization', 'soft_goods_ratio' (haptics), 'informal', 'creative'."
"Provenance","provenance.architect_le_corbusier","Architect: Le Corbusier","ArchDaily, Dezeen, Academic Archives","""architect: Le Corbusier""","VLM (Gemini), CLIP","(Vector Search) \`cosine_similarity(image_vector, known_corbusier_vector) > 0.8\` (VLM Query) \`Q: ""Does this interior strongly resemble the work of Le Corbusier? (yes/no)""\`","High 'schema' activation for 'Modernism'. Cues for 'concrete', 'primary colors', 'machine for living'."
"Provenance","provenance.architect_kengo_kuma","Architect: Kengo Kuma","ArchDaily, Dezeen","""architect: Kengo Kuma""","VLM (Gemini), CLIP","(Vector Search) \`cosine_similarity(image_vector, known_kuma_vector) > 0.8\` (VLM Query) \`Q: ""Does this interior strongly resemble the work of Kengo Kuma? (yes/no)""\`","High 'biophilia' cue, 'wood' (slats), 'lightness', 'dematerialization', 'craft'."
"Provenance","provenance.iconic_chair_eames","Iconic Chair: Eames Lounge Chair","Houzz (Visual Match), Pinterest, Design Museums","""Eames Lounge Chair""","VLM (Gemini), Google Cloud Vision API","(Object Detection) \`GCV\` has a 'mid' for 'Eames Lounge Chair'. (VLM Query) \`Q: ""Is there an Eames Lounge Chair in this image? (yes/no)""\`","'Mid-Century Modern' schema. 'High-status' cue. 'Perceived comfort' (affordance)."
"Provenance","provenance.iconic_chair_barcelona","Iconic Chair: Barcelona Chair","Houzz (Visual Match), Pinterest, Design Museums","""Barcelona Chair""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is there a Barcelona Chair in this image? (yes/no)""\`","'Mies van der Rohe' schema. 'Modernist' cue. 'Formal' seating (less 'lounge' than Eames)."
"Spatial & Geometric","spatial.room_function.living_room","Room Function: Living Room","Houzz, ArchDaily, ADE20K","""Living Room""","VLM (Gemini), ADE20K Dataset","(VLM Query - Classification) \`Q: ""What room is this?""\` -> \`(label == 'living_room')\`","Primary 'social' and 'relaxation' space. High 'sociopetal' potential."
"Spatial & Geometric","spatial.room_function.kitchen","Room Function: Kitchen","Houzz, ArchDaily, ADE20K","""Kitchen""","VLM (Gemini), ADE20K Dataset","(VLM Query - Classification) \`Q: ""What room is this?""\` -> \`(label == 'kitchen')\`","'Task-oriented' (focus). 'Social' (hearth). 'Olfactory' cues (food)."
"Spatial & Geometric","spatial.room_function.bedroom","Room Function: Bedroom","Houzz, ArchDaily, ADE20K","""Bedroom""","VLM (Gemini), ADE20K Dataset","(VLM Query - Classification) \`Q: ""What room is this?""\` -> \`(label == 'bedroom')\`","Primary 'refuge' and 'rest' space. Low-arousal, low-light, high 'softness' desired."
"Spatial & Geometric","spatial.room_function.home_office","Room Function: Home Office","Houzz, ArchDaily, ADE20K","""Home Office""","VLM (Gemini), ADE20K Dataset","(VLM Query - Classification) \`Q: ""What room is this?""\` -> \`(label == 'home_office')\`","'Task-oriented' (focus). Requires 'ergonomic' affordances, 'low-distraction', 'good task lighting'."
"Spatial & Geometric","spatial.room_function.bathroom","Room Function: Bathroom","Houzz, ArchDaily, ADE20K","""Bathroom""","VLM (Gemini), ADE20K Dataset","(VLM Query - Classification) \`Q: ""What room is this?""\` -> \`(label == 'bathroom')\`","'Private' space. 'Spa-like' (restorative) or 'functional' (task-based). 'Haptic' (water, steam)."
"Component: Ceiling","component.ceiling.exposed_beam","Ceiling: Exposed Beam","Houzz, Pinterest","""exposed beam ceiling""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Are there exposed structural beams on the ceiling? (yes/no)""\`","Cues for 'rustic', 'craft', 'structure', 'shelter'. Can increase 'visual complexity'."
"Component: Ceiling","component.ceiling.coffered","Ceiling: Coffered","Houzz, Pinterest","""coffered ceiling""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is this a coffered (sunken panel) ceiling? (yes/no)""\`","Cues for 'traditional', 'formal', 'high-status', 'high-complexity', 'rhythm'."
"Component: Ceiling","component.ceiling.vaulted","Ceiling: Vaulted","Houzz, Pinterest","""vaulted ceiling""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is this a vaulted or cathedral ceiling? (yes/no)""\`","'Cathedral Effect'. Primes 'abstract thought', 'awe', 'openness'. Can feel 'less cozy'."
"Component: Ceiling","component.ceiling.tray","Ceiling: Tray","Houzz","""tray ceiling""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is this a tray ceiling (recessed center)? (yes/no)""\`","'Formal' cue (often in dining/bedrooms). 'Softly' defines a zone. Good for 'cove lighting'."
"Component: Ceiling","component.ceiling.cove_lighting","Ceiling: Cove Lighting","Houzz, Pinterest, Lighting Catalogs","""cove lighting""","VLM (Gemini)","(VLM Query - Binary Classification) \`Q: ""Is indirect cove lighting visible at the ceiling edge? (yes/no)""\`","'Indirect light' = low glare, 'soft' shadows, 'calming' atmosphere. Reduces 'visual hot spots'."
"Component: Wall","component.wall.material.brick_exposed","Wall: Exposed Brick","Houzz, Pinterest, ArchDaily","""exposed brick wall"", ""brick wall interior""","VLM (Gemini), SAM","1. Get \`wall\` segment (SAM). 2. Pass to VLM: \`Q: ""Is this wall exposed brick? (yes/no)""\`","'Industrial' or 'historic' schema. 'Authenticity'. 'Rough' texture. 'Warm' color. High 'fractal_dimension'."
"Component: Wall","component.wall.material.stone_wall","Wall: Stone Wall","Houzz, Pinterest, ArchDaily","""stone wall interior"", ""stacked stone wall"" (User's 'stone walls')","VLM (Gemini), SAM","1. Get \`wall\` segment (SAM). 2. Pass to VLM: \`Q: ""Is this wall made of stacked stone? (yes/no)""\`","'Refuge' cue (cave-like, solid). 'Biophilia' (natural material). 'High-mass' thermal cue. 'Rustic' schema."
"Component: Wall","component.wall.material.wood_slat","Wall: Wood Slat / Acoustic Wood","Houzz, Pinterest, ArchDaily (Products)","""wood slat wall"", ""acoustic wood panel""","VLM (Gemini), SAM","1. Get \`wall\` segment (SAM). 2. Pass to VLM: \`Q: ""Is this a wood slat wall? (yes/no)""\`","'Biophilia'. High 'rhythm'. 'Modern' style. Implies 'acoustic damping' (CNfA)."
"Component: Wall","component.wall.treatment.wallpaper_patterned","Wall: Patterned Wallpaper","Houzz, Wallpaper Catalogs","""patterned wallpaper""","VLM (Gemini), SAM","1. Get \`wall\` segment (SAM). 2. Pass to VLM: \`Q: ""Is this wall covered in patterned wallpaper? (yes/no)""\`","High 'visual complexity'. 'Biophilic' (if floral/natural). 'Formal' or 'playful' style. Can be 'high-load' or 'fascinating'."
"Component: Wall","component.wall.treatment.wainscoting","Wall: Wainscoting","Houzz, Pinterest","""wainscoting""","VLM (Gemini), SAM","1. Get \`wall\` segment (SAM). 2. Pass to VLM: \`Q: ""Does this wall have wainscoting (lower-half paneling)? (yes/no)""\`","'Traditional' or 'Formal' schema. Creates a 'horizontal datum line'. 'Human-scale' element."
"Component: Kitchen","component.kitchen.hardware.material.brass","Hardware: Brass","Houzz, Pinterest, Hardware Catalogs (e.g., Rejuvenation)","""brass hardware"", ""brass cabinet pulls""","VLM (Gemini), SAM","1. Get \`cabinet\` segments (SAM). 2. Find small anomalies (hardware). 3. Pass hardware segment to VLM: \`Q: ""Is this hardware brass? (yes/no)""\`","'Warm' metal. 'Classic' or 'high-status' cue. 'Haptic' affordance."
"Component: Kitchen","component.kitchen.hardware.material.matte_black","Hardware: Matte Black","Houzz, Pinterest, Hardware Catalogs","""matte black hardware"", ""matte black faucet""","VLM (Gemini), SAM","1. Get \`cabinet\` segments (SAM). 2. Find hardware. 3. Pass hardware segment to VLM: \`Q: ""Is this hardware matte black? (yes/no)""\`","'Modern', 'Industrial', or 'Farmhouse' style. 'High-contrast' graphic element."
"Component: Kitchen","component.kitchen.hardware.material.chrome","Hardware: Chrome","Houzz, Pinterest, Hardware Catalogs","""chrome hardware"", ""chrome faucet""","VLM (Gemini), SAM","1. Get \`cabinet\` segments (SAM). 2. Find hardware. 3. Pass hardware segment to VLM: \`Q: ""Is this hardware chrome/polished nickel? (yes/no)""\`","'Standard', 'clean', 'functional' cue. 'Cool' metal. Can create 'glare' (visual hot spots)."
"Component: Kitchen","component.kitchen.hardware.type.bar_pull","Hardware: Bar Pull","Houzz, Hardware Catalogs","""bar pull"", ""cabinet bar pull""","VLM (Gemini), SAM","1. Get \`cabinet hardware\` segments. 2. Pass segment to VLM: \`Q: ""Is this hardware a 'bar pull' or a 'knob'?""\` -> \`(label == 'bar_pull')\`","'Modern', 'minimal', 'rectilinear' cue. 'Clear' haptic affordance."
"Component: Kitchen","component.kitchen.hardware.type.knob","Hardware: Knob","Houzz, Hardware Catalots","""cabinet knob""","VLM (Gemini), SAM","1. Get \`cabinet hardware\` segments. 2. Pass segment to VLM: \`Q: ""Is this hardware a 'bar pull' or a 'knob'?""\` -> \`(label == 'knob')\`","'Traditional' or 'vintage' cue. 'Small-scale' haptic affordance."
"Component: Kitchen","component.kitchen.hardware.type.handleless","Hardware: Handleless / Integrated","Houzz, Pinterest, Kitchen Catalogs","""handleless cabinets"", ""push-to-open""","VLM (Gemini), SAM","1. Get \`cabinet\` segments. 2. \`count(hardware segments) == 0\`. (VLM Query) \`Q: ""Are these cabinets handleless? (yes/no)""\`","'Minimalist' schema. Low 'visual complexity'. 'Ambiguous' haptic affordance (less obvious)."
"Component: Bathroom","component.bathroom.fixture.material.brass","Fixture: Brass","Houzz, Plumbing Catalogs (e.g., Kohler)","""brass faucet"", ""brass showerhead""","VLM (Gemini), SAM","1. Get \`faucet\` or \`shower\` segment. 2. Pass to VLM: \`Q: ""Is this fixture's material brass? (yes/no)""\`","'Warm' metal. 'Classic' or 'high-status'. Cues 'wabi-sabi' if 'unlaquered brass' (patina)."
"Component: Bathroom","component.bathroom.fixture.material.matte_black","Fixture: Matte Black","Houzz, Plumbing Catalogs","""matte black faucet"", ""matte black showerhead""","VLM (Gemini), SAM","1. Get \`faucet\` or \`shower\` segment. 2. Pass to VLM: \`Q: ""Is this fixture's material matte black? (yes/no)""\`","'Modern', 'graphic', 'high-contrast'. Can be 'calming' (low-reflection) or 'severe'."
"Component: Bathroom","component.bathroom.fixture.type.rain_showerhead","Fixture: Rain Showerhead","Houzz, Plumbing Catalogs","""rain showerhead""","VLM (Gemini)","1. Get \`showerhead\` bounding box. 2. Pass to VLM: \`Q: ""Is this a rain showerhead (ceiling-mounted or large-diameter)? (yes/no)""\`","'Spa-like' affordance. Cues 'relaxation', 'luxury', 'enveloping' (full-body haptic)."
"Computed CNfA: Light","cnfa.light.diffuse_vs_direct_ratio","Quantifying 'softness' of light.","'Diffuse' (soft shadows) = 'calming', 'low-load', 'even' illumination. 'Direct' (hard shadows) = 'dramatic', 'high-contrast', 'focal', can be 'harsh'.","N/A (Computed)","N/A (Computed)","OpenCV, VLM (Gemini)","(CV Method) 1. Find all \`shadow\` segments. 2. Calculate the 'entropy' of the shadow edge gradient (penumbra). High entropy = blurry/soft. Low entropy = sharp/hard. (VLM Query) \`Q: ""Are the shadows in this room 'soft and diffuse' or 'hard and sharp'?""\`"
"Computed CNfA: Light","cnfa.light.warm_vs_cool_ratio","Quantifying overall light 'temperature'.","(Circadian) 'Cool' light (>5000K) = 'alertness'. 'Warm' light (<3000K) = 'relaxation'.","N/A (Computed)","N/A (Computed)","OpenCV, SAM","1. Get \`light_source\` (window, lamp) and \`illuminated_surface\` (white wall) segments (SAM). 2. Calculate the average BGR color of these pixels. 3. Convert BGR to Color Temperature (CCT) in Kelvin. 4. \`Ratio = (pixels > 4000K) / (pixels < 4000K)\`."
"Computed CNfA: Light","cnfa.light.vertical_illuminance_proxy","Quantifying light on vertical surfaces (walls).","'Vertical illuminance' (lit walls) is key to making a space feel 'bright' and 'open', more so than just floor/ceiling light.","N/A (Computed)","N/A (Computed)","SAM, OpenCV","1. Get \`wall\` segments (SAM). 2. Calculate the \`mean(pixel_brightness)\` of *only* the wall segments. 3. Compare \`brightness(wall)\` to \`brightness(floor)\`."
"Computed CNfA: Spatial","cnfa.spatial.prospect_to_refuge_ratio","Quantifying the 'Prospect-Refuge' balance (Appleton).","The 'ideal' state. High prospect ('I can see') + High refuge ('I cannot be seen'). The 'captain's chair' or 'cozy nook with a view'.","N/A (Computed)","N/A (Computed)","Apple RoomPlan API, 3D Mesh Library","(Complex Calculation) 1. Get 3D mesh. 2. For each \`seat\` (refuge spot): 3. Calculate \`prospect_score\` (isovist area from seat). 4. Calculate \`refuge_score\` (1 - visibility *from* main paths). 5. \`Ratio = max(prospect_score * refuge_score)\`."
"Computed CNfA: Spatial","cnfa.spatial.enclosure_index","Quantifying the 'sense of enclosure'. (User's 'privacy.enclosure')","'High Enclosure' = 'refuge', 'privacy', 'cozy', 'intimate' (can be 'claustrophobic'). 'Low' = 'open', 'social', 'public' (can be 'exposed').","N/A (Computed)","N/A (Computed)","Apple RoomPlan API, VLM (Gemini)","(Strong Method) 1. Get 3D mesh (RoomPlan). 2. \`Index = 1 - (area(openings) / area(total_wall_surface))\`. (VLM Query) \`Q: ""On a scale of 1-10, how 'enclosed' or 'cozy' does this space feel?""\`"
"Computed CNfA: Spatial","cnfa.spatial.ceiling_height_avg","Quantifying the 'Cathedral Effect'.","'High Ceiling' (>3m) = primes 'abstract' thought, 'awe'. 'Low Ceiling' (<2.5m) = primes 'concrete' thought, 'focus', 'coziness'.","Apple RoomPlan API","N/A (Computed)","Apple RoomPlan API","(Strong Method) This is a *direct output* of the \`RoomPlan\` API, \`room.height\`."
"Computed CNfA: Cognitive","cnfa.cognitive.legibility_score","Quantifying 'wayfinding' ease (Lynch).","'High Legibility' = 'low cognitive load', 'easy to map', 'comfortable'. 'Low' (ambiguous paths) = 'stress', 'confusion', 'mystery'."
"N/A (Computed)","N/A (Computed)","VLM (Gemini), Apple RoomPlan API","(Strong Method) 1. Get 2D floor plan (RoomPlan). 2. Run 'Space Syntax' analysis (calculate 'integration' and 'intelligibility' of the graph). (VLM Query) \`Q: ""Does this space look 'easy' or 'confusing' to navigate?""\`"
"Computed CNfA: Cognitive","cnfa.cognitive.landmark_salience","Quantifying 'landmarks' for navigation (Lynch).","A 'salient landmark' (e.g., fireplace, sculpture, unique window) 'anchors' the mental map, reducing cognitive load.","N/A (Computed)","N/A (Computed)","Salience Models (FASA, etc.), VLM (Gemini)","(CV Method) 1. Generate a 'visual salience map'. 2. \`Score = max(salience_peak_value)\`. A high peak = a clear landmark. (VLM Query) \`Q: ""What is the single most memorable object or feature in this room?""\`"
"Computed CNfA: Cognitive","cnfa.cognitive.activity_zones_count","Quantifying 'functional density'.","'High Count' (many zones) = 'complex', 'high-function' (e.g., studio apt). 'Low Count' = 'simple', 'focused' (e.g., bedroom).","N/A (Computed)","N/A (Computed)","VLM (Gemini)","(VLM Query - Count) \`Q: ""How many distinct activity zones can you see (e.g., a 'reading zone', a 'dining zone', a 'TV zone')? List them.""\` -> \`count(list)\`"
"Computed CNfA: Haptic","cnfa.haptic.soft_surface_ratio","Quantifying 'haptic comfort' and 'acoustic softness'.","'High Ratio' = 'soft haptics', 'acoustic damping', 'cozy', 'low-reverb'. 'Low' (e.g., all hard surfaces) = 'cold haptics', 'high-reverb', 'loud'.","N/A (Computed)","N/A (Computed)","VLM (Gemini), SAM","1. Use SAM to segment all major surfaces. 2. For each, ask VLM: \`Q: ""Is this surface 'soft' (e.g., rug, curtain, sofa, plant) or 'hard' (e.g., wood, concrete, glass, plaster)?""\` 3. \`Ratio = area(Soft) / area(Total)\`."
"Computed CNfA: Haptic","cnfa.haptic.texture_variation_index","Quantifying 'haptic richness'.","'High Variation' (e.g., wood, stone, wool, glass) = 'rich sensory' experience (Barsalou), 'high-interest', 'craft'. 'Low' (e.g., all drywall) = 'simple', 'low-load', 'sterile'."
"N/A (Computed)","N/A (Computed)","VLM (Gemini), SAM, OpenCV","(CV Method) 1. Get all surfaces (SAM). 2. For each, calculate a 'texture vector' (Gabor, GLCM). 3. \`Index = variance(all_texture_vectors)\`. (VLM) \`Q: ""Count the number of different textures you can see (e.g., 'smooth glass', 'rough wood', 'soft fabric').""\`"
"Computed CNfA: Dynamic","cnfa.dynamic.optic_flow_magnitude","Optic Flow Magnitude (Avg)","N/A (Computed)","N/A (Computed)","3D Model (RoomPlan) + 3D Engine (three.js) OR Video (OpenCV)","(3D Model) 1. Simulate camera move from 'entry' to 'focal_point'. 2. Run OpenCV's 'calcOpticalFlowFarneback' on the rendered frames. 3. Average the vector magnitudes. (Video) 1. Run 'calcOpticalFlowFarneback' directly on the video. 2. Average magnitudes.","Gibson's 'optic flow'. High magnitude (e.g., 'tunneling') increases arousal/stress. Low magnitude ('open lobby') is calmer. This is the brain's primary cue for 'speed' and 'space'."
"Computed CNfA: Dynamic","cnfa.dynamic.revelation_rate","Revelation Rate (Mystery)","N/A (Computed)","N/A (Computed)","3D Model (RoomPlan) + 3D Engine (three.js)","(3D Model) 1. Simulate camera move along 'primary_path'. 2. At each step(t), calculate \`total_visible_surface_area\`. 3. \`Rate = d(Area) / d(t)\`. A high, sustained rate = high 'Mystery'.","Kaplan's 'Mystery'. The rate at which new information is revealed by moving. 'High' rate is 'engaging' and 'fascinating'. 'Zero' rate is 'boring'. 'Infinite' (e.g., a corner) is 'surprising'."
"Computed CNfA: Dynamic","cnfa.dynamic.texture_gradient","Dynamic Texture Gradient (slat example)","N/A (Computed)","N/A (Computed)","3D Model (RoomPlan) + 3D Engine + OpenCV OR VLM (Depth Map) + OpenCV","(3D Model) 1. Simulate camera *approaching* a textured wall. 2. For each frame, run FFT on the texture to get \`dominant_spatial_frequency\`. 3. \`Gradient = d(Frequency) / d(distance)\`. (Weak Method) 1. Get 2.5D depth map. 2. Calculate FFT on texture at 'point A' vs. 'point B'. 3. Get gradient.","Gibson's 'texture gradient'. This is a primary cue for 'scale', 'distance', and 'speed'. A 'steep' gradient (e.g., a fine-grained texture) provides a powerful cue for your own motion."
"Computed CNfA: Dynamic","cnfa.dynamic.path_glare_max","Path-Based Glare (Max)","N/A (Computed)","N/A (Computed)","3D Model (RoomPlan) + 3D Engine (Unity, Unreal)","(3D Model) 1. Simulate camera move along 'primary_path' *with a light source (sun)*. 2. Use ray-tracing to render reflections. 3. At each frame, run the *static* 'cnfa.light.glare_probability' algorithm. 4. \`Result = max(glare_prob_along_path)\`.","Measures *experiential* visual discomfort. A static photo might miss the blinding glare that only happens when you walk past a window. This is a key 'stressor' that static analysis fails to capture."
"Computed CNfA: Dynamic","cnfa.dynamic.reflection_flow","Reflection Flow (Shimmer)","N/A (Computed)","N/A (Computed)","3D Model (RoomPlan) + 3D Engine OR Video (OpenCV)","(Video/3D Render) 1. Identify 'glossy' surfaces (e.g., polished floor, water). 2. Run 'Optical Flow' *only* on those surfaces. 3. \`Flow = avg_magnitude(vectors_on_glossy_surface)\`.","The 'shimmer' of light on water or a polished floor. Can be a 'Soft Fascination' (restorative, biophilic) or a 'Distraction' (high-load, annoying)."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.edge_clarity_mean","Edge Clarity / Predictability","N/A (Computed)","N/A (Computed)","OpenCV","(CV Method) 1. Run Canny or Sobel edge detection. 2. Calculate the *average gradient (sharpness)* of all detected edge pixels. High avg = 'crisp', 'high-contrast', 'easy-to-process' edges. Low avg = 'blurry', 'ambiguous' edges.","'Predictive Coding'. Our brain 'predicts' edges. Sharp, clear edges fulfill these predictions easily, leading to 'high fluency' (feels good, low-load). Ambiguous edges create 'prediction error' (higher load, 'uneasy' feeling)."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.pattern_rhythm_regularity","Pattern Rhythm / Regularity (User's 'slats')","N/A (Computed)","N/A (Computed)","OpenCV, SAM","(CV Method) 1. Segment a repeating pattern (e.g., 'wood slats', 'tile grid') using SAM. 2. Take a 1D slice across the pattern. 3. Run a Fast Fourier Transform (FFT) on this signal. A single, sharp peak = high regularity. A noisy, broad signal = low regularity.","'Easy to Gestalt'. A regular rhythm (like wood slats) is perceptually 'simple' because the brain can model it with one simple rule (e.g., 'repeat every 5cm'). This is 'high fluency' and can be 'calming' or 'hypnotic'."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.symmetry_score_horizontal","Horizontal Symmetry Score","N/A (Computed)","N/A (Computed)","OpenCV","(CV Method) 1. \`image_flipped = cv2.flip(image, 1)\` (horizontal flip). 2. Calculate the 'Structural Similarity Index' (SSIM) between \`image\` and \`image_flipped\`. 3. \`Symmetry Score = SSIM_result\`.","'High Fluency' / 'Pr√§gnanz'. Symmetry is the 'simplest' visual form. It is processed *very* quickly and easily by the brain. Often associated with 'formality', 'stability', and 'beauty', but also 'static' or 'boring'."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.visual_entropy_spatial","Visual Entropy (Spatial Disorder)","N/A (Computed)","N/A (Computed)","Google Cloud Vision API, OpenCV","(CV Method) 1. Get bounding boxes for all objects (GCV). 2. Create a 2D spatial histogram (grid) of the object centers. 3. Calculate the Shannon entropy of this distribution. High entropy = high disorder (objects scattered randomly).","This is 'disfluency'. 'High Entropy' = 'chaotic', 'disorganized', 'high-load'. 'Low Entropy' (e.g., everything on a grid) = 'orderly', 'low-load', 'legible'."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.clutter_density_count","Clutter Density (Object Count)","N/A (Computed)","N/A (Computed)","Google Cloud Vision API","(CV Method) 1. Use GCV \`objectLocalization\` to get a list of *all* objects. 2. \`Density = count(objects) / area(room_floor)\`. 3. Normalize.","'High Cognitive Load'. This is a *count* of items the brain must process. Differs from 'entropy' (which is 'disorder'). A full bookshelf is 'dense' but not 'entropic'. High density = high load."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.color_palette_entropy","Color Palette Entropy (Complexity)","N/A (Computed)","N/A (Computed)","Google Cloud Vision API, OpenCV","(CV Method) 1. Get the \`dominant_palette\` (from GCV or OpenCV k-means clustering). 2. Calculate the Shannon entropy of the palette's pixel-fraction distribution. Low entropy = monochromatic (fluent). High entropy = many colors (complex).","'High Fluency' (low entropy) = 'calming', 'serene', 'monochromatic'. 'Disfluency' (high entropy) = 'vibrant', 'chaotic', 'playful', 'high-arousal'."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.figure_ground_clarity","Figure-Ground Clarity","N/A (Computed)","N/A (Computed)","VLM (Gemini), SAM","(VLM Query) \`Q: ""On a scale of 1-10, how easy is it to distinguish objects from their background in this image?""\` (CV Method) 1. Get \`object\` segments (SAM). 2. Get \`wall/floor\` segments (SAM). 3. Calculate the avg. color/texture *contrast* between object edges and their background.","'Gestalt - Figure/Ground'. 'High Clarity' = 'legible', 'low-load'. 'Low Clarity' (e.g., a beige chair against a beige wall) = 'ambiguous', 'high-load', but can also feel 'serene' or 'blended'."
"Computed CNfA: Perceptual Fluency","cnfa.fluency.processing_load_proxy","Processing Load Proxy (Compression)","N/A (Computed)","N/A (Computed)","ImageMagick, PIL (Python)","(CV Method) 1. Take the full-res image. 2. Save it as a JPEG at 90% quality. 3. \`Load_Proxy = file_size_in_bytes / total_pixels\`. A high ratio = high-frequency detail (complex). A low ratio = large simple surfaces (fluent).","This is a 'pure' measure of visual information density. 'High Load' (e.g., a patterned wallpaper) literally requires more data to store, just as it requires more cognitive effort to process. 'Low Load' (a plain painted wall) is 'fluent'."
`;
        
        // 1. Fetch and Parse the CSV data
        // 1. Fetch feature data from the API (v1/features)
        window.onload = () => {
            fetch('/api/v1/features/')
                .then(resp => {
                    if (!resp.ok) throw new Error('Failed to load features');
                    return resp.json();
                })
                .then(apiFeatures => {
                    // Adapt API schema to the CSV-like schema used by the UI
                    allFeatures = apiFeatures.map(f => ({
                        Feature_Category: f.category,
                        Feature_Name: f.key,
                        Human_Readable_Label: f.label,
                        CNfA_Relevance: f.cfa_relevance || "",
                    }));
                    initializeApp(allFeatures);
                })
                .catch(err => {
                    console.error('Error fetching features:', err);
                });
        };
        };

        // 2. Initialize the dashboard with parsed data
        function initializeApp(data) {
            renderSummary(data);
            const categories = getCategoryCounts(data);
            renderCategoryChart(categories);
            renderFilterOptions(categories);
            renderFeatureList(data);
        }

        // 3. Render Summary Stats
        function renderSummary(data) {
            const categories = new Set(data.map(item => item.Feature_Category));
            document.getElementById('total-features').textContent = data.length;
            document.getElementById('total-categories').textContent = categories.size;
        }

        // 4. Get Category Counts
        function getCategoryCounts(data) {
            const counts = {};
            data.forEach(item => {
                const category = item.Feature_Category || 'Uncategorized';
                counts[category] = (counts[category] || 0) + 1;
            });
            return counts;
        }

        // 5. Render Bar Chart
        function renderCategoryChart(categories) {
            const container = document.getElementById('chart-container');
            container.innerHTML = ''; // Clear previous chart
            
            const maxCount = Math.max(...Object.values(categories));
            let colorIndex = 0;

            const sortedCategories = Object.entries(categories).sort(([,a],[,b]) => b-a);

            for (const [category, count] of sortedCategories) {
                const widthPercentage = (count / maxCount) * 100;
                const color = categoryColors[colorIndex % categoryColors.length];
                
                const barHtml = `
                    <div class="flex items-center">
                        <div class="w-48 text-sm font-medium text-gray-600 truncate pr-2">${category}</div>
                        <div class="flex-1 bg-gray-200 rounded-full h-6">
                            <div class="chart-bar ${color} h-6 rounded-full text-xs font-bold text-white text-right pr-2 leading-6" style="width: ${widthPercentage > 0 ? widthPercentage : 1}%" title="${count} features">
                                ${count}
                            </div>
                        </div>
                    </div>
                `;
                container.innerHTML += barHtml;
                colorIndex++;
            }
        }

        // 6. Render Filter Checkboxes
        function renderFilterOptions(categories) {
            const container = document.getElementById('filter-container');
            const sortedKeys = Object.keys(categories).sort();
            
            sortedKeys.forEach(category => {
                const filterId = `filter-${category.replace(/\W/g, '')}`;
                const checkboxHtml = `
                    <div class="flex items-center">
                        <input id="${filterId}" data-category="${category}" type="checkbox" class="h-4 w-4 rounded border-gray-300 text-blue-600 focus:ring-blue-500 category-filter" onchange="filterData()">
                        <label for="${filterId}" class="ml-3 text-sm text-gray-700">${category} (${categories[category]})</label>
                    </div>
                `;
                container.innerHTML += checkboxHtml;
            });

            // Add logic to 'All Categories' checkbox
            document.getElementById('filter-all').onchange = () => {
                const isChecked = document.getElementById('filter-all').checked;
                document.querySelectorAll('.category-filter').forEach(cb => cb.checked = false);
                filterData();
            };
        }

        // 7. Filter and Re-render List
        function filterData() {
            const allCheckbox = document.getElementById('filter-all');
            const categoryCheckboxes = document.querySelectorAll('.category-filter:checked');
            
            let selectedCategories = [];
            categoryCheckboxes.forEach(cb => selectedCategories.push(cb.dataset.category));

            if (selectedCategories.length > 0) {
                allCheckbox.checked = false;
                const filteredFeatures = allFeatures.filter(item => selectedCategories.includes(item.Feature_Category));
                renderFeatureList(filteredFeatures);
            } else {
                allCheckbox.checked = true;
                renderFeatureList(allFeatures);
            }
        }

        // 8. Render the actual feature list (table)
        function renderFeatureList(data) {
            const tbody = document.getElementById('feature-list');
            tbody.innerHTML = ''; // Clear list

            if (data.length === 0) {
                tbody.innerHTML = '<tr><td colspan="3" class="px-6 py-4 text-center text-gray-500">No features match the selected filters.</td></tr>';
                return;
            }

            data.forEach(item => {
                const row = `
                    <tr>
                        <td class="px-6 py-4 whitespace-nowrap">
                            <div class="text-sm font-medium text-gray-900">${item.Feature_Name || 'N/A'}</div>
                            <div class="text-sm text-gray-500">${item.Human_Readable_Label || 'N/A'}</div>
                        </td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">${item.Feature_Category || 'N/A'}</td>
                        <td class="px-6 py-4">
                            <div class="text-sm text-gray-900" style="white-space: normal; min-width: 300px;">${item.CNfA_Relevance || 'N/A'}</div>
                        </td>
                    </tr>
                `;
                tbody.innerHTML += row;
            });
        }

    </script>
</body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/public/feature_onboarding.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Feature Onboarding Wizard ‚Äî CNfA</title>
  <style>
    body { max-width: 800px; margin: 2rem auto; font-family: system-ui, sans-serif; line-height: 1.5; }
    .card { background: #fff; border: 1px solid #eee; border-radius: 8px; padding: 24px; box-shadow: 0 2px 8px rgba(0,0,0,0.05); }
    .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; }
    label { font-weight: 600; display: block; margin-bottom: 4px; font-size: 0.9rem; }
    input, textarea, select { width: 100%; padding: 8px; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; }
    textarea { min-height: 60px; }
    h3 { margin-top: 20px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
    .btn { padding: 10px 16px; border-radius: 6px; border: none; cursor: pointer; font-weight: 600; background: #2563eb; color: #fff; }
    .btn:hover { opacity: 0.9; }
  </style>
</head>
<body>
  <div class="card">
    <h1>üß™ Feature Onboarding Wizard</h1>
    <p>Define new architectural attributes. Export to JSON to add to the system.</p>

    <div class="grid">
      <div>
        <label>Feature Category</label>
        <input id="cat" placeholder="e.g. Spatial, Cognitive">
      </div>
      <div>
        <label>Feature Key</label>
        <input id="key" placeholder="e.g. spatial.prospect">
      </div>
      <div>
        <label>Human Label</label>
        <input id="label" placeholder="e.g. Prospect Depth">
      </div>
      <div>
        <label>Method</label>
        <select id="method">
            <option value="vlm">VLM (AI Reasoning)</option>
            <option value="cv">Computer Vision (Math)</option>
        </select>
      </div>
    </div>

    <h3>Goldilocks Parameters</h3>
    <div class="grid">
      <div><label>Low End (Boring)</label><textarea id="low"></textarea></div>
      <div><label>High End (Chaotic)</label><textarea id="high"></textarea></div>
      <div><label>Optimal (Just Right)</label><textarea id="opt"></textarea></div>
    </div>

    <br>
    <button class="btn" onclick="exportJSON()">Download Definition</button>
  </div>

<script>
  function exportJSON(){
    const d = (id) => document.getElementById(id).value;
    const data = {
      key: d('key'), label: d('label'), category: d('cat'), method: d('method'),
      goldilocks: { low: d('low'), high: d('high'), optimal: d('opt') }
    };
    const b = new Blob([JSON.stringify(data,null,2)], {type:"application/json"});
    const u = URL.createObjectURL(b);
    const a = document.createElement('a'); a.href=u; a.download=`feature_${data.key}.json`;
    document.body.appendChild(a); a.click(); a.remove();
  }
</script>
</body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/src/App.jsx
----- CONTENT START -----
import React, { useState, useEffect } from 'react';
import { Header, Button, Toggle, ApiClient, useToast } from '@shared';
import { ShieldAlert, DollarSign, Server, Power, Info, RefreshCcw, Download, Activity } from 'lucide-react';

const api = new ApiClient('/api/v1/admin', { 'X-User-Role': 'admin' });
const vlmHealthApi = new ApiClient('/api/v1/vlm-health', { 'X-User-Role': 'admin' });


const VLMConfigPanel = () => {
  const toast = useToast();
  const [provider, setProvider] = React.useState('auto');
  const [engineName, setEngineName] = React.useState(null);
  const [available, setAvailable] = React.useState({});
  const [status, setStatus] = React.useState(null);
  const [error, setError] = React.useState(null);
  const [imageId, setImageId] = React.useState('');
  const [busy, setBusy] = React.useState(false);
  const [loading, setLoading] = React.useState(true);
  const [promptOverride, setPromptOverride] = React.useState('');
  const [maxBatchSize, setMaxBatchSize] = React.useState('');
  const [costPer1k, setCostPer1k] = React.useState('');

  const loadConfig = async () => {
    setLoading(true);
    setError(null);
    try {
      const resp = await api.get('/vlm/config');
      if (resp) {
        setProvider(resp.provider || 'auto');
        setEngineName(resp.engine || null);
        setAvailable(resp.available_backends || {});
        setPromptOverride(resp.cognitive_prompt_override || '');
        setMaxBatchSize(
          typeof resp.max_batch_size === 'number' ? String(resp.max_batch_size) : ''
        );
        setCostPer1k(
          typeof resp.cost_per_1k_images_usd === 'number'
            ? String(resp.cost_per_1k_images_usd)
            : ''
        );
      }
    } catch (err) {
      console.error('Failed to load VLM config', err);
      setError(err.message || 'Failed to load VLM configuration');
    } finally {
      setLoading(false);
    }
  };

  React.useEffect(() => {
        loadAll();
        loadUploadJobs();

  const handleSave = async () => {
    setBusy(true);
    setStatus(null);
    setError(null);
    try {
      await api.post('/vlm/config', {
        provider,
        cognitive_prompt_override: promptOverride || null,
        max_batch_size: maxBatchSize ? parseInt(maxBatchSize, 10) : null,
        cost_per_1k_images_usd: costPer1k ? parseFloat(costPer1k) : null,
      });
      setStatus('Saved VLM configuration.');
      toast.success('Saved VLM configuration.', { title: 'VLM Config' });
      await loadConfig();
    } catch (err) {
      console.error('Failed to save VLM config', err);
      setError(err.message || 'Failed to save VLM configuration');
      toast.error('Failed to save VLM configuration', { message: err.message || String(err) });
    } finally {
      setBusy(false);
    }
  };

  const handleTest = async () => {
    if (!imageId) {
      setStatus('Enter an image ID to test.');
      return;
    }
    setBusy(true);
    setStatus(null);
    setError(null);
    try {
      const payload = { image_id: Number(imageId), prompt: 'Quick VLM sanity check for this architectural image.' };
      const resp = await api.post('/vlm/test', payload);
      if (resp) {
        const label = resp.is_stub ? 'STUB (no API keys visible)' : 'LIVE';
        setStatus(`Test OK with ${resp.engine}: ${label}.`);
      } else {
        setStatus('No response from VLM test.');
      }
    } catch (err) {
      console.error('VLM test failed', err);
      setError(err.message || 'VLM test failed');
      toast.error('VLM test failed', { message: err.message || String(err) });
    } finally {
      setBusy(false);
    }
  };

  const providerLabel = (key) => {
    switch (key) {
      case 'auto':
        return 'Auto (prefer Gemini, then OpenAI, then Anthropic)';
      case 'gemini':
        return 'Gemini (Google, e.g. 1.5 Flash/Pro)';
      case 'openai':
        return 'OpenAI (e.g. GPT-4o / 4.1)';
      case 'anthropic':
        return 'Anthropic (e.g. Claude 3.5)';
      case 'stub':
        return 'Stub (no network calls, neutral outputs)';
      default:
        return key;
    }
  };

  return (
    <div>
      <div className="flex items-center justify-between mb-2">
        <div className="flex flex-col">
          <h2 className="font-semibold text-sm text-gray-900">VLM Engine</h2>
          <p className="text-[11px] text-gray-500">
            Choose which Visual Language Model to use for cognitive analysis.
          </p>
        </div>
        {loading && (
          <span className="text-[11px] text-gray-400">Loading‚Ä¶</span>
        )}
      </div>


<div className="mt-2 p-2 rounded-md bg-blue-50 border border-blue-100 text-[11px] text-blue-900 space-y-1">
  <p className="font-semibold">VLM configuration: handle with care</p>
  <ul className="list-disc ml-4 space-y-0.5">
    <li>Changes here affect which provider (Gemini, OpenAI, Anthropic, stub) is used for all VLM-assisted analyses.</li>
    <li>Always verify that API keys are detected and that the effective engine looks correct before enabling real runs.</li>
    <li>After editing settings, run a small test on a known image ID and confirm the response looks plausible.</li>
    <li>If tests fail or output looks broken, revert to a safe configuration and notify an engineer.</li>
  </ul>
</div>

      <div className="space-y-3">
        <div className="space-y-1">
          <label className="text-[11px] uppercase tracking-wide text-gray-500 font-semibold">
            Provider
          </label>
          <select
            className="w-full text-xs border border-gray-200 rounded px-2 py-1 focus:outline-none focus:ring-1 focus:ring-blue-400"
            value={provider}
            onChange={(e) => setProvider(e.target.value)}
            disabled={busy}
          >
            <option value="auto">Auto (prefer Gemini)</option>
            <option value="gemini">Gemini</option>
            <option value="openai">OpenAI</option>
            <option value="anthropic">Anthropic</option>
            <option value="stub">Stub</option>
          </select>
          <p className="text-[11px] text-gray-400">
            {providerLabel(provider)}
          </p>
        </div>

        <div className="grid grid-cols-2 gap-2 text-[11px] text-gray-500">
          <div>
            <span className="font-semibold text-gray-700">Detected keys:</span>
            <ul className="mt-1 space-y-0.5">
              <li>Gemini: {available.gemini ? 'yes' : 'no'}</li>
              <li>OpenAI: {available.openai ? 'yes' : 'no'}</li>
              <li>Anthropic: {available.anthropic ? 'yes' : 'no'}</li>
            </ul>
          </div>
          <div>
            <span className="font-semibold text-gray-700">Effective engine:</span>
            <p className="mt-1 text-gray-800 text-xs">
              {engineName || 'Stub / none configured'}
            </p>
          </div>
        </div>

        <div className="space-y-1">
          <label className="text-[11px] uppercase tracking-wide text-gray-500 font-semibold">
            Test on Image ID
          </label>
          <div className="flex items-center gap-2">
            <input
              type="number"
              className="w-24 text-xs border border-gray-200 rounded px-2 py-1 focus:outline-none focus:ring-1 focus:ring-blue-400"
              placeholder="e.g. 1"
              value={imageId}
              onChange={(e) => setImageId(e.target.value)}
            />
            <Button
              size="xs"
              variant="secondary"
              onClick={handleTest}
              disabled={busy}
            >
              Test VLM
            </Button>
          </div>
        </div>

        <div className="flex items-center justify-between pt-1">
          <div className="flex items-center gap-2">
            <Button
              size="xs"
              variant="primary"
              onClick={handleSave}
              disabled={busy}
            >
              Save
            </Button>
          </div>
          <div className="flex-1 text-right">
            {status && (
              <p className="text-[11px] text-gray-600">{status}</p>
            )}
            {error && (
              <p className="text-[11px] text-red-600">{error}</p>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};


const BulkUploadPanel = ({ onUploadCompleted }) => {
  const toast = useToast();
  const [files, setFiles] = React.useState(null);
  const [status, setStatus] = React.useState(null);
  const [isUploading, setIsUploading] = React.useState(false);

  const handleChange = (e) => {
    setFiles(e.target.files);
  };

  const handleUpload = async () => {
    if (!files || files.length === 0) {
      setStatus("Please choose one or more image files first.");
      return;
    }
    setIsUploading(true);
    setStatus("Uploading...");
    try {
      const form = new FormData();
      Array.from(files).forEach((file) => form.append("files", file));
      const res = await fetch("/api/v1/admin/upload", {
        method: "POST",
        headers: {
          "X-User-Role": "admin",
        },
        body: form,
      });
      if (!res.ok) {
        const text = await res.text();
        setStatus(`Upload failed: ${res.status} ${text}`);
        toast.error(`Upload failed: ${res.status}`, { title: 'Bulk Upload' });
        setIsUploading(false);
        return;
      }
      const data = await res.json();
      setStatus(
        `Uploaded ${data.created_count} images. New IDs: ${data.image_ids.join(", ")}`
      );
      toast.success(`Uploaded ${data.created_count} images.`, { title: 'Bulk Upload' });

      if (onUploadCompleted) {
        try {
          onUploadCompleted(data);
        } catch (callbackErr) {
          console.error('BulkUploadPanel onUploadCompleted callback failed', callbackErr);
        }
      }
    } catch (err) {
      setStatus(`Upload error: ${String(err)}`);
      toast.error('Upload error', { message: String(err) });
    } finally {
      setIsUploading(false);
    }
  };

  return (
    <div className="mt-6 rounded-xl border border-gray-200 bg-white p-4 shadow-sm">
      <h2 className="text-lg font-semibold text-gray-900">Bulk image upload</h2>
      <p className="mt-1 text-sm text-gray-600">
        Use this when you want to add new images to the dataset without touching the
        command line. Files will be stored under IMAGE_STORAGE_ROOT and made
        available to the science pipeline and Explorer.
      </p>
      <div className="mt-3 flex flex-col gap-2 md:flex-row md:items-center">
        <input
          type="file"
          multiple
          accept="image/*"
          onChange={handleChange}
          className="text-sm"
        />
        <button
          type="button"
          onClick={handleUpload}
          disabled={isUploading}
          className="inline-flex items-center rounded-md border border-transparent bg-blue-600 px-3 py-1.5 text-sm font-medium text-white shadow-sm hover:bg-blue-700 disabled:opacity-50"
        >
          {isUploading ? "Uploading..." : "Upload"}
        </button>
      </div>
      {status && (
        <p className="mt-2 text-xs text-gray-700 whitespace-pre-wrap">{status}</p>
      )}
    </div>
  );
};



const UploadJobsPanel = ({ jobs, loading, onRefresh, lastJobId }) => {
    const hasJobs = Array.isArray(jobs) && jobs.length > 0;

    return (
        <div>
            <div className="flex items-center justify-between mb-2">
                <div className="flex items-center gap-2">
                    <Server size={16} className="text-slate-500" />
                    <h2 className="font-semibold text-sm text-gray-900">
                        Upload jobs
                    </h2>
                </div>
                <Button
                    size="xs"
                    variant="ghost"
                    onClick={() => onRefresh && onRefresh()}
                    disabled={loading}
                >
                    <RefreshCcw
                        size={14}
                        className={loading ? 'animate-spin mr-1' : 'mr-1'}
                    />
                    <span className="text-[11px]">Refresh</span>
                </Button>
            </div>

            {lastJobId && (
                <p className="text-[11px] text-gray-500 mb-1">
                    Last upload job:{' '}
                    <span className="font-mono text-gray-800">#{lastJobId}</span>
                </p>
            )}

            {!hasJobs && (
                <p className="text-[11px] text-gray-500">
                    No upload jobs recorded yet. Upload a batch to see progress here.
                </p>
            )}

            {hasJobs && (
                <div className="mt-2 max-h-56 overflow-y-auto text-xs">
                    <table className="w-full text-left">
                        <thead className="text-[11px] text-gray-500 border-b border-gray-200">
                            <tr>
                                <th className="py-1 pr-2">Job</th>
                                <th className="py-1 pr-2">Status</th>
                                <th className="py-1 pr-2">Progress</th>
                                <th className="py-1">Errors</th>
                            </tr>
                        </thead>
                        <tbody>
                            {jobs.map(job => (
                                <tr key={job.id} className="border-b border-gray-100">
                                    <td className="py-1 pr-2 font-mono text-[11px]">
                                        #{job.id}
                                    </td>
                                    <td className="py-1 pr-2">
                                        <span className="inline-flex items-center rounded-full px-2 py-0.5 text-[10px] font-medium bg-slate-100 text-slate-700">
                                            {job.status}
                                        </span>
                                    </td>
                                    <td className="py-1 pr-2">
                                        {job.completed_items}/{job.total_items}
                                    </td>
                                    <td
                                        className="py-1 text-[10px] text-gray-500 truncate max-w-[120px]"
                                        title={job.error_summary || ''}
                                    >
                                        {job.failed_items > 0
                                            ? `${job.failed_items} failed`
                                            : '‚Äî'}
                                    </td>
                                </tr>
                            ))}
                        </tbody>
                    </table>
                </div>
            )}
        </div>
    );
}

function CostHistoryCard({ dailyCosts, totalSpent, hardLimit }) {
    if (!dailyCosts || dailyCosts.length === 0) {
        return (
            <div className="bg-white rounded-xl border border-gray-200 p-4">
                <div className="flex items-center justify-between mb-2">
                    <div className="flex items-center gap-2">
                        <DollarSign className="text-emerald-500" size={18} />
                        <h2 className="font-semibold text-sm text-gray-900">
                            Cost history
                        </h2>
                    </div>
                </div>
                <p className="text-[11px] text-gray-500">
                    No VLM usage has been recorded yet. Once the science pipeline
                    calls an external model, a daily cost history will appear here.
                </p>
            </div>
        );
    }

    const maxCost = Math.max(
        ...dailyCosts.map(p => (typeof p.total_cost === 'number' ? p.total_cost : 0)),
        0.01
    );
    const lastSeven = dailyCosts.slice(-7);
    const prevSeven = dailyCosts.slice(-14, -7);

    const sum = arr =>
        arr.reduce(
            (acc, p) => acc + (typeof p.total_cost === 'number' ? p.total_cost : 0),
            0
        );

    const lastSevenTotal = sum(lastSeven);
    const prevSevenTotal = sum(prevSeven);
    const delta = lastSevenTotal - prevSevenTotal;

    return (
        <div className="bg-white rounded-xl border border-gray-200 p-4">
            <div className="flex items-center justify-between mb-2">
                <div className="flex items-center gap-2">
                    <DollarSign className="text-emerald-500" size={18} />
                    <h2 className="font-semibold text-sm text-gray-900">
                        Cost history
                    </h2>
                </div>
                <span className="text-[11px] text-gray-500">
                    Last {dailyCosts.length} days
                </span>
            </div>

            <div className="flex items-baseline justify-between text-xs mb-3">
                <div>
                    <div className="text-gray-500">Last 7 days</div>
                    <div className="font-semibold text-gray-900">
                        ${lastSevenTotal.toFixed(2)}
                    </div>
                </div>
                <div className="text-right">
                    <div className="text-gray-500">Change vs. prior 7 days</div>
                    <div
                        className={
                            delta >= 0 ? 'text-red-600 font-semibold' : 'text-emerald-600 font-semibold'
                        }
                    >
                        {delta >= 0 ? '+' : ''}
                        {delta.toFixed(2)}
                    </div>
                </div>
            </div>

            <div className="flex items-end gap-0.5 h-16 mb-2">
                {dailyCosts.map((point, idx) => {
                    const value =
                        typeof point.total_cost === 'number' ? point.total_cost : 0;
                    const heightPct = Math.max(8, (value / maxCost) * 100);
                    return (
                        <div
                            key={idx}
                            className="flex-1 bg-emerald-100"
                            style={{ height: `${heightPct}%` }}
                            title={`${point.day}: $${value.toFixed(2)}`}
                        />
                    );
                })}
            </div>

            <div className="text-[11px] text-gray-500 flex justify-between mt-1">
                <span>Total spent: ${totalSpent.toFixed(2)}</span>
                <span>Budget: ${hardLimit.toFixed(2)}</span>
            </div>
        </div>
    );
}



function VLMHealthCard() {
    const [runs, setRuns] = useState([]);
    const [loading, setLoading] = useState(false);
    const [error, setError] = useState(null);

    useEffect(() => {
        let cancelled = false;

        async function fetchRuns() {
            setLoading(true);
            setError(null);
            try {
                const data = await vlmHealthApi.get('/runs');
                if (!cancelled) {
                    setRuns(Array.isArray(data) ? data : []);
                }
            } catch (err) {
                console.error('Failed to load VLM health runs', err);
                if (!cancelled) {
                    setError('Unable to load VLM health runs.');
                }
            } finally {
                if (!cancelled) {
                    setLoading(false);
                }
            }
        }

        fetchRuns();
        return () => {
            cancelled = true;
        };
    }, []);

    return (
        <div className="bg-white rounded-xl border border-gray-200 p-4">
            <div className="flex items-center justify-between mb-2">
                <div className="flex items-center gap-2">
                    <Activity className="text-indigo-500" size={18} />
                    <h2 className="font-semibold text-sm text-gray-900">
                        VLM health runs
                    </h2>
                </div>
                {loading && (
                    <span className="text-[11px] text-gray-400">Loading‚Ä¶</span>
                )}
            </div>

            {error && (
                <p className="text-[11px] text-red-600 mb-1">
                    {error}
                </p>
            )}

            {!loading && (!runs || runs.length === 0) && (
                <p className="text-[11px] text-gray-500">
                    No VLM health runs found yet. After you run{' '}
                    <code className="bg-gray-100 px-1 py-[1px] rounded text-[10px]">
                        make vlm-health-init
                    </code>{' '}
                    and the follow-up steps from the VLM Health Quickstart,
                    runs will appear here.
                </p>
            )}

            {runs && runs.length > 0 && (
                <div className="space-y-2 mt-1">
                    {runs.map((run) => (
                        <div
                            key={run.run_id}
                            className="flex items-center justify-between text-[11px]"
                        >
                            <div className="flex flex-col">
                                <span className="font-mono text-xs text-gray-900">
                                    {run.run_id}
                                </span>
                                {run.created_at && (
                                    <span className="text-[10px] text-gray-500">
                                        {new Date(run.created_at).toLocaleString()}
                                    </span>
                                )}
                            </div>
                            <div className="flex flex-col items-end gap-1">
                                <div className="flex gap-2">
                                    <span
                                        className={
                                            'px-1.5 py-[1px] rounded-full text-[10px] ' +
                                            (run.has_variance_audit
                                                ? 'bg-emerald-50 text-emerald-700 border border-emerald-100'
                                                : 'bg-gray-50 text-gray-400 border border-gray-100')
                                        }
                                    >
                                        variance
                                    </span>
                                    <span
                                        className={
                                            'px-1.5 py-[1px] rounded-full text-[10px] ' +
                                            (run.has_turing_summary
                                                ? 'bg-blue-50 text-blue-700 border border-blue-100'
                                                : 'bg-gray-50 text-gray-400 border border-gray-100')
                                        }
                                    >
                                        turing
                                    </span>
                                </div>
                                <div className="flex gap-2">
                                    {run.has_variance_audit && (
                                        <a
                                            href={`/api/v1/vlm-health/runs/${encodeURIComponent(
                                                run.run_id
                                            )}/variance-audit`}
                                            className="text-[10px] text-blue-600 hover:underline"
                                        >
                                            CSV
                                        </a>
                                    )}
                                    {run.has_turing_summary && (
                                        <a
                                            href={`/api/v1/vlm-health/runs/${encodeURIComponent(
                                                run.run_id
                                            )}/turing-summary`}
                                            className="text-[10px] text-blue-600 hover:underline"
                                        >
                                            Summary
                                        </a>
                                    )}
                                </div>
                            </div>
                        </div>
                    ))}
                </div>
            )}

            <p className="mt-3 text-[10px] text-gray-400">
                This view is read-only. Edit or regenerate runs from the CLI
                using the VLM Health SOP.
            </p>
        </div>
    );
}

export default function AdminApp() {
    const [models, setModels] = useState([]);
    const [budget, setBudget] = useState(null);
    const [killSwitchActive, setKillSwitchActive] = useState(false);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [savingModelId, setSavingModelId] = useState(null);
    const [killBusy, setKillBusy] = useState(false);
    const [exportIds, setExportIds] = useState('');
    const [exportBusy, setExportBusy] = useState(false);
    const [exportMessage, setExportMessage] = useState(null);
    const [imageExportBusy, setImageExportBusy] = useState(false);
    const [imageExportMessage, setImageExportMessage] = useState(null);

    const [dailyCosts, setDailyCosts] = useState([]);
    const [costWindowDays, setCostWindowDays] = useState(30);
    const [uploadJobs, setUploadJobs] = useState([]);
    const [uploadJobsLoading, setUploadJobsLoading] = useState(false);
    const [lastUploadJobId, setLastUploadJobId] = useState(null);
    useEffect(() => {
        loadAll();
        loadUploadJobs();
    }, []);

    async function loadAll() {
        setLoading(true);
        setError(null);
        try {
            const [modelsResp, budgetResp, costsResp] = await Promise.all([
                api.get('/models'),
                api.get('/budget'),
                api.get(`/costs/daily?days=${costWindowDays}`),
            ]);
            setModels(Array.isArray(modelsResp) ? modelsResp : []);
            if (budgetResp) {
                setBudget(budgetResp);
                setKillSwitchActive(!!budgetResp.is_kill_switched);
                setDailyCosts(Array.isArray(costsResp) ? costsResp : []);
            }
        } catch (err) {
            console.error('Failed to load admin data', err);
            setError(err.message || 'Failed to load admin cockpit');
        } finally {
            setLoading(false);
        }
    }

async function loadUploadJobs(limit = 20) {
    setUploadJobsLoading(true);
    try {
        const jobs = await api.get(`/upload/jobs?limit=${limit}`);
        setUploadJobs(Array.isArray(jobs) ? jobs : []);
    } catch (err) {
        console.error('Failed to load upload jobs', err);
        // We treat this as a soft failure to avoid breaking the main admin load.
    } finally {
        setUploadJobsLoading(false);
    }
}

function handleUploadCompleted(data) {
    if (data && typeof data.job_id === 'number') {
        setLastUploadJobId(data.job_id);
    }
    // Refresh the job list so progress is visible shortly after upload.
    loadUploadJobs();
}

    async function handleToggle(id) {
        const current = models.find(m => m.id === id);
        if (!current) return;
        setSavingModelId(id);
        setError(null);
        try {
            const updated = await api.patch(`/models/${id}`, {
                is_enabled: !current.is_enabled,
            });
            setModels(models.map(m => (m.id === id ? updated : m)));
        } catch (err) {
            console.error('Failed to update model', err);
            setError(err.message || 'Failed to update model');
        } finally {
            setSavingModelId(null);
        }
    }

    async function handleCostBlur(id, rawValue) {
        const value = parseFloat(rawValue);
        if (Number.isNaN(value)) {
            return;
        }
        const current = models.find(m => m.id === id);
        if (!current || current.cost_per_1k_tokens === value) {
            return;
        }
        setSavingModelId(id);
        setError(null);
        try {
            const updated = await api.patch(`/models/${id}`, {
                cost_per_1k_tokens: value,
            });
            setModels(models.map(m => (m.id === id ? updated : m)));
        } catch (err) {
            console.error('Failed to update cost', err);
            setError(err.message || 'Failed to update model cost');
        } finally {
            setSavingModelId(null);
        }
    }

    
    async function handleKillSwitch(nextActive) {
        setKillBusy(true);
        setError(null);
        try {
            const resp = await api.post(`/kill-switch?active=${nextActive}`);
            if (resp) {
                // Backend returns BudgetStatus; keep UI in sync.
                setBudget(resp);
                setKillSwitchActive(!!resp.is_kill_switched);
            }
        } catch (err) {
            console.error('Failed to update kill switch', err);
            setError(
                err && err.message
                    ? err.message
                    : 'Failed to update kill switch'
            );
        } finally {
            setKillBusy(false);
        }
    }

    async function handleTrainingExport() {
        setExportBusy(true);
        setExportMessage(null);
        try {
            const ids = exportIds
                .split(',')
                .map((s) => s.trim())
                .filter(Boolean)
                .map((s) => Number(s))
                .filter((n) => !Number.isNaN(n));

            const payload = { image_ids: ids };

            const resp = await fetch('/api/v1/admin/training/export', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'X-User-Role': 'admin',
                },
                body: JSON.stringify(payload),
            });

            if (!resp.ok) {
                const text = await resp.text();
                setExportMessage(`Export failed: ${resp.status} ${text}`);
                return;
            }

            const data = await resp.json();
            const blob = new Blob([JSON.stringify(data, null, 2)], {
                type: 'application/json',
            });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'training_export.json';
            a.click();
            window.URL.revokeObjectURL(url);

            setExportMessage(
                `Exported ${Array.isArray(data) ? data.length : 0} training examples.`
            );
        } catch (err) {
            console.error('Training export failed', err);
            setExportMessage(
                `Training export failed: ${err && err.message ? err.message : String(err)}`
            );
        } finally {
            setExportBusy(false);
        }
    }

    async function handleImageExport() {
        setImageExportBusy(true);
        setImageExportMessage(null);
        try {
            const resp = await fetch('/api/v1/admin/export/images', {
                method: 'GET',
                headers: {
                    'X-User-Role': 'admin',
                },
            });

            if (!resp.ok) {
                const text = await resp.text();
                setImageExportMessage(`Export failed: ${resp.status} ${text}`);
                return;
            }

            const blob = await resp.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'image_tagger_images_export.zip';
            a.click();
            window.URL.revokeObjectURL(url);

            setImageExportMessage('Image export started. Check your downloads.');
        } catch (err) {
            console.error('Image export failed', err);
            setImageExportMessage(
                `Image export failed: ${err && err.message ? err.message : String(err)}`
            );
        } finally {
            setImageExportBusy(false);
        }
    }
            if (resp) {
                setBudget(resp);
                setKillSwitchActive(!!resp.is_kill_switched);
            }
        } catch (err) {
            console.error('Failed to toggle kill switch', err);
            setError(err.message || 'Failed to toggle kill switch');
        } finally {
            setKillBusy(false);
        }
    }

const totalModels = models.length;
    const enabledModels = models.filter(m => m.is_enabled).length;

    const totalSpent = budget ? budget.total_spent : 0;
    const hardLimit = budget ? budget.hard_limit : 1;
    const usagePct = hardLimit > 0 ? Math.min(100, (totalSpent / hardLimit) * 100) : 0;

    return (
        <div className="min-h-screen bg-gray-100 pb-10">
            <Header appName="Admin" title="Cost & Governance Cockpit" />

            <div className="p-8 max-w-6xl mx-auto space-y-8">
                <div className="flex items-center justify-between gap-4">
                    <div>
                        <p className="text-sm text-gray-500">
                            Configure which models and tools are allowed to run, and monitor budget risk.
                        </p>
                        {error && (
                            <p className="text-xs text-red-600 mt-1">
                                {error}
                            </p>
                        )}
                    </div>
                    <div className="flex items-center gap-3">
                        {loading && (
                            <span className="text-xs text-gray-500 flex items-center gap-2">
                                <RefreshCcw className="animate-spin" size={14} /> Loading‚Ä¶
                            </span>
                        )}
                        <Button variant="secondary" onClick={loadAll}>
                            <RefreshCcw size={16} className="mr-2" /> Refresh
                        </Button>
                    </div>
                </div>

                <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
                    {/* Models & Tools */}
                    <section className="lg:col-span-2 bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                        <div className="p-4 border-b border-gray-100 flex items-center justify-between">
                            <div className="flex items-center gap-2">
                                <Server className="text-blue-500" size={18} />
                                <h2 className="font-semibold text-gray-900 text-sm">
                                    AI Models
                                </h2>
                            </div>
                            <span className="text-xs text-gray-400">
                                {enabledModels}/{totalModels} models enabled
                            </span>
                        </div>
                        <div className="divide-y divide-gray-100">
                            {models.map(model => (
                                <div
                                    key={model.id}
                                    className="flex items-center justify-between px-4 py-3 hover:bg-gray-50 transition-colors"
                                >
                                    <div>
                                        <p className="text-sm font-medium text-gray-900">
                                            {model.name}
                                        </p>
                                        <p className="text-xs text-gray-500">
                                            Provider: {model.provider || '‚Äî'}
                                        </p>
                                    </div>
                                    <div className="flex items-center gap-4">
                                        <div className="text-right">
                                            <label className="block text-[11px] uppercase tracking-wide text-gray-500 font-semibold mb-1">
                                                Cost / 1K tokens
                                            </label>
                                            <div className="flex items-center gap-1">
                                                <DollarSign size={12} className="text-gray-400" />
                                                <input
                                                    type="number"
                                                    step="0.0001"
                                                    defaultValue={model.cost_per_1k_tokens}
                                                    onBlur={e => handleCostBlur(model.id, e.target.value)}
                                                    className="w-20 px-2 py-1 border border-gray-200 rounded text-xs text-right focus:outline-none focus:ring-1 focus:ring-blue-400"
                                                />
                                            </div>
                                        </div>
                                        <div className="flex flex-col items-end">
                                            <span className="text-[11px] text-gray-500 mb-1">
                                                {model.is_enabled ? 'Enabled' : 'Disabled'}
                                            </span>
                                            <Toggle
                                                checked={model.is_enabled}
                                                disabled={savingModelId === model.id}
                                                onChange={() => handleToggle(model.id)}
                                            />
                                        </div>
                                    </div>
                                </div>
                            ))}
                            {!models.length && !loading && (
                                <div className="p-4 text-xs text-gray-400">
                                    No ToolConfigs found. Run the seed_tool_configs script or insert rows into
                                    the tool_configs table to populate this view.
                                </div>
                            )}
                        </div>
                    </section>

                    {/* Kill Switch & Budget */}
                    <section className="space-y-4">
                        <div className="bg-red-50 border border-red-100 rounded-xl p-4 flex flex-col gap-3">
                            <div className="flex items-center gap-2">
                                <ShieldAlert className="text-red-500" size={18} />
                                <h2 className="font-semibold text-sm text-red-800">
                                    Kill Switch
                                </h2>
                            </div>
                            <p className="text-xs text-red-700">
                                When activated, all paid models (cost_per_1k_tokens &gt; 0) are disabled. This is
                                enforced server-side via the ToolConfig table and checked before tools are used.
                            </p>
                            <div className="flex items-center justify-between mt-2">
                                <div>
                                    <p className="text-[11px] text-red-600 font-semibold uppercase tracking-wide">
                                        Status
                                    </p>
                                    <p className="text-sm font-medium text-red-900">
                                        {killSwitchActive ? 'ACTIVE' : 'Inactive'}
                                    </p>
                                </div>
                                <Button
                                    variant={killSwitchActive ? 'outline' : 'primary'}
                                    size="sm"
                                    disabled={killBusy}
                                    onClick={() => handleKillSwitch(!killSwitchActive)}
                                >
                                    <Power size={14} className="mr-1" />
                                    {killSwitchActive ? 'Disable Kill Switch' : 'Activate Kill Switch'}
                                </Button>
                            </div>
                            <p className="text-[11px] text-red-500 flex items-center gap-1 mt-1">
                                <Info size={12} /> Use this if cost monitoring shows you are approaching budget.
                            </p>
                        </div>

                        <div className="bg-slate-900 rounded-xl p-4 text-slate-50 shadow-sm">
<div className="bg-white rounded-xl border border-gray-200 p-4">
    <div className="flex items-center justify-between mb-2">
        <div className="flex items-center gap-2">
            <Download className="text-gray-600" size={18} />
            <h2 className="font-semibold text-sm text-gray-900">
                Training Export
            </h2>
        </div>
    </div>
    <p className="text-xs text-gray-500 mb-2">
        Export validated tags as JSON for fine-tuning or active learning. Provide a comma-separated
        list of image IDs (or leave blank to export nothing).
    </p>

<div className="mt-1 mb-2 p-2 rounded-md bg-blue-50 border border-blue-100 text-[11px] text-blue-900 space-y-1">
    <p className="font-semibold">What this export is for</p>
    <ul className="list-disc ml-4 space-y-0.5">
        <li>Each row in the exported JSON corresponds to an image/case with its validated tags.</li>
        <li>Use this for training downstream models (BNs, regressions, or fine-tuned vision models).</li>
        <li>Keep track of which canon version and schema were active when you generated the export.</li>
        <li>For reproducible studies, store export parameters (image IDs, filters) alongside your analysis code.</li>
    </ul>
</div>
    <textarea
        className="w-full text-xs border border-gray-200 rounded p-2 mb-2 focus:outline-none focus:ring-1 focus:ring-blue-400"
        rows={2}
        placeholder="e.g. 101, 102, 103"
        value={exportIds}
        onChange={e => setExportIds(e.target.value)}
    />
    <div className="flex items-center justify-between">
        <Button
            size="sm"
            variant="secondary"
            disabled={exportBusy}
            onClick={handleTrainingExport}
        >
            <Download size={14} className="mr-1" />
            Download JSON
        </Button>
        {exportMessage && (
            <span className="text-[11px] text-gray-500">
                {exportMessage}
            </span>
        )}
    
</div>

    <div className="bg-white rounded-xl border border-gray-200 p-4 mt-4">
        <div className="flex items-center justify-between mb-2">
            <div className="flex items-center gap-2">
                <Download className="text-gray-600" size={18} />
                <h2 className="font-semibold text-sm text-gray-900">
                    Image Archive Export
                </h2>
            </div>
        </div>
        <p className="text-xs text-gray-500 mb-2">
            Download a zip of all stored image files for offline analysis or backup.
            This uses the same storage paths as the science pipeline.
        </p>
        <div className="flex items-center justify-between">
            <Button
                size="sm"
                variant="secondary"
                disabled={imageExportBusy}
                onClick={handleImageExport}
            >
                <Download size={14} className="mr-1" />
                Download .zip
            </Button>
            {imageExportMessage && (
                <span className="text-[11px] text-gray-500">
                    {imageExportMessage}
                </span>
            )}
        </div>
    </div>

    {/* Admin Tools: Bulk Upload & VLM */}
    <div className="mt-4 grid grid-cols-1 gap-4">
        <div className="bg-white rounded-xl border border-gray-200 p-4">
            <BulkUploadPanel onUploadCompleted={handleUploadCompleted} />
        </div>
        <div className="bg-white rounded-xl border border-gray-200 p-4">
            <VLMConfigPanel />
        </div>
        <div className="bg-white rounded-xl border border-gray-200 p-4">
            <UploadJobsPanel
                jobs={uploadJobs}
                loading={uploadJobsLoading}
                onRefresh={loadUploadJobs}
                lastJobId={lastUploadJobId}
            />
        </div>
    </div>

                            <div className="flex items-center justify-between">
                                <div className="flex items-center gap-2">
                                    <DollarSign className="text-emerald-300" size={18} />
                                    <h2 className="font-semibold text-sm">
                                        Cost Overview
                                    </h2>
                                </div>
                                <span className="text-[11px] text-slate-400">
                                    Prototype estimator
                                </span>
                            </div>
                            <div className="mt-4 space-y-1 text-xs">
                                <p>
                                    <span className="text-slate-400">Estimated spend:</span>{' '}
                                    <span className="font-semibold">${totalSpent.toFixed(2)}</span>
                                </p>
                                <p>
                                    <span className="text-slate-400">Hard limit:</span>{' '}
                                    <span className="font-semibold">${hardLimit.toFixed(2)}</span>
                                </p>
                            </div>
                            <div className="w-full bg-slate-800 h-2 rounded-full mt-4 overflow-hidden">
                                <div
                                    className="bg-emerald-300 h-full"
                                    style={{ width: `${usagePct}%` }}
                                />
                            </div>
                            <p className="text-[11px] text-slate-400 mt-2">
                                {usagePct.toFixed(0)}% of hard limit used.
                            </p>
                        </div>
                        <div className="mt-4">
                            <CostHistoryCard
                                dailyCosts={dailyCosts}
                                totalSpent={totalSpent}
                                hardLimit={hardLimit}
                            />
                        </div>
                    </section>
                </div>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: frontend/apps/admin/src/main.jsx
----- CONTENT START -----
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.jsx'
import { ToastProvider, MaintenanceOverlay } from '@shared';
import '../../../../index.css'
ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <ToastProvider>
      <MaintenanceOverlay />
      <App />
    </ToastProvider>
  </React.StrictMode>,
)----- CONTENT END -----
----- FILE PATH: frontend/apps/explorer/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Explorer | Image Tagger v3</title>
  </head>
  <body class="bg-white h-screen overflow-hidden">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/explorer/package.json
----- CONTENT START -----
{
  "name": "explorer",
  "private": true,
  "version": "3.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build"
  }
}----- CONTENT END -----
----- FILE PATH: frontend/apps/explorer/vite.config.js
----- CONTENT START -----
import { defineConfig } from 'vite';
import getBaseConfig from '../../vite.config.base';
import path from 'path';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));
export default defineConfig(getBaseConfig(__dirname));----- CONTENT END -----
----- FILE PATH: frontend/apps/explorer/src/App.jsx
----- CONTENT START -----
import React, { useState, useEffect, useMemo } from 'react';
import { Header, Button, ApiClient } from '@shared';
import { Search, Filter, Download, Image as ImageIcon, CheckSquare, SlidersHorizontal, HelpCircle } from 'lucide-react';

// Fallback demo data (used only if backend returns nothing)
const SAMPLE_IMAGES = Array.from({ length: 12 }).map((_, i) => ({
    id: i,
    url: `https://picsum.photos/seed/${i + 100}/400/${300 + (i % 3) * 50}`,
    tags: i % 2 === 0 ? ["Modern", "Kitchen", "High-Res"] : ["Traditional", "Living Room", "Low-Light"],
}));

const api = new ApiClient('/api/v1/explorer');

export default function ExplorerApp() {
    const [cart, setCart] = useState([]);
    const [debugMode, setDebugMode] = useState('none'); // 'none' | 'edges' | 'overlay' | 'depth'
    const [overlayOpacity, setOverlayOpacity] = useState(0.5);
    const [edgeThresholds, setEdgeThresholds] = useState({ low: 50, high: 150 });
    const [query, setQuery] = useState("");
    const [filtersOpen, setFiltersOpen] = useState(true);

    const [images, setImages] = useState(SAMPLE_IMAGES);
    const [attributes, setAttributes] = useState([]);
    const [selectedTags, setSelectedTags] = useState([]);
    const [loading, setLoading] = useState(false);
    const [attrLoading, setAttrLoading] = useState(false);
    const [error, setError] = useState(null);

    useEffect(() => {
        loadAttributes();
        // Initial search with empty filters/query
        runSearch([]);
    }, []);

    const groupedAttributes = useMemo(() => {
        if (!attributes || !attributes.length) return [];
        const groups = {};
        for (const attr of attributes) {
            const key = attr.key || "";
            const prefix = key.split('.')[0] || 'Other';
            if (!groups[prefix]) groups[prefix] = [];
            groups[prefix].push(attr);
        }
        return Object.entries(groups)
            .map(([name, items]) => ({
                name,
                items: items.slice(0, 8),
            }))
            .slice(0, 4);
    }, [attributes]);

    async function loadAttributes() {
        setAttrLoading(true);
        try {
            const data = await api.get('/attributes');
            if (Array.isArray(data)) {
                setAttributes(data);
            }
        } catch (err) {
            console.error('Failed to load attributes', err);
        } finally {
            setAttrLoading(false);
        }
    }

    async function runSearch(tagsOverride) {
        setLoading(true);
        setError(null);
        try {
            const filters = {};
            const tags = tagsOverride ?? selectedTags;
            if (tags && tags.length) {
                filters.tags = tags;
            }
            const data = await api.post('/search', {
                query_string: query,
                filters,
                page: 1,
                page_size: 48,
            });
            if (Array.isArray(data) && data.length) {
                setImages(data);
            } else {
                // If backend returns no results, keep UI consistent but empty
                setImages([]);
            }
        } catch (err) {
            console.error('Search failed', err);
            if (err && err.isMaintenance) {
                setError('__MAINTENANCE__:' + (err.message || 'System temporarily unavailable (503).'));
            } else {
                setError(err && err.message ? err.message : 'Search failed');
            }
        } finally {
            setLoading(false);
        }
    }

    const handleQueryKeyDown = (e) => {
        if (e.key === 'Enter') {
            runSearch();
        }
    };

    const handleAttributeToggle = (key) => {
        setSelectedTags(prev => {
            const next = prev.includes(key)
                ? prev.filter(k => k !== key)
                : [...prev, key];
            // Run search with the next selection
            runSearch(next);
            return next;
        });
    };

    const toggleCart = (id) => {
        setCart(prev => (
            prev.includes(id) ? prev.filter(x => x !== id) : [...prev, id]
        ));
    };

    const selectAll = () => {
        setCart(images.map(img => img.id));
    };

    const handleExport = async () => {
        if (!cart.length) return;
        try {
            const payload = { image_ids: cart, format: 'json' };
            const data = await api.post('/export', payload);
            const blob = new Blob([JSON.stringify(data, null, 2)], { type: 'application/json' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'dataset_export.json';
            a.click();
            URL.revokeObjectURL(url);
        } catch (err) {
            console.error('Export failed', err);
            if (err && err.isMaintenance) {
                setError('__MAINTENANCE__:' + (err.message || 'System temporarily unavailable (503).'));
            } else {
                setError('Export failed: ' + ((err && err.message) ? err.message : 'Unknown error'));
            }
        }
    };

    const isMaintenance = typeof error === 'string' && error.startsWith('__MAINTENANCE__:');
    const maintenanceMessage = isMaintenance ? error.replace('__MAINTENANCE__:', '') : null;

    return (
        <div className="relative flex flex-col h-screen bg-white">
            <Header appName="Explorer" title="Research Discovery" />

            {isMaintenance && (
                <div className="absolute inset-0 z-50 flex items-center justify-center bg-black/60 text-gray-100">
                    <div className="bg-gray-900/95 px-6 py-4 rounded-lg shadow-lg max-w-md text-center">
                        <div className="font-semibold mb-2 text-sm">System temporarily unavailable</div>
                        <div className="text-xs mb-3">{maintenanceMessage || "The backend is reporting a maintenance / outage condition (503). Searches and exports are temporarily paused."}</div>
                        <div className="text-[10px] text-gray-400">If this persists for more than a few minutes, contact your TA or lab lead.</div>
                    </div>
                </div>
            )}

{/* Quick Help */}
<div className="border-b border-blue-100 bg-blue-50 px-4 py-3 text-xs text-blue-900 flex gap-3 items-start">
    <HelpCircle className="mt-0.5 flex-shrink-0" size={18} />
    <div>
        <div className="font-semibold text-sm mb-1">How to use Explorer</div>
        <ol className="list-decimal ml-4 space-y-0.5">
            <li>Start with a search query, or leave it blank to browse recent images.</li>
            <li>Use <span className="font-semibold">Filters</span> to narrow by attributes, tags, or source.</li>
            <li>Click image thumbnails to inspect their tags and attributes.</li>
            <li>Use the checkbox on each card to add images to your export cart.</li>
            <li>Click <span className="font-semibold">Export Dataset</span> to download a JSON file for training or analysis.</li>
        </ol>
    </div>
</div>

            {/* Toolbar */}
            <div className="border-b border-gray-200 p-4 flex gap-4 bg-gray-50 items-center z-20 shadow-sm">
                <div className="relative flex-1 max-w-2xl">
                    <Search className="absolute left-3 top-1/2 transform -translate-y-1/2 text-gray-400" size={20} />
                    <input
                        type="text"
                        placeholder='Query (e.g. "High prospect windows in modern style")'
                        className="w-full pl-10 pr-4 py-2.5 border border-gray-200 rounded-lg focus:ring-2 focus:ring-blue-500 outline-none shadow-sm"
                        value={query}
                        onChange={e => setQuery(e.target.value)}
                        onKeyDown={handleQueryKeyDown}
                    />
                </div>
                <Button variant="secondary" onClick={() => setFiltersOpen(!filtersOpen)}>
                    <SlidersHorizontal size={18} className="mr-2" /> Filters
                </Button>
                <Button onClick={() => runSearch()} disabled={loading}>
                    <Search size={18} className="mr-2" /> Search
                </Button>
                <Button
                    onClick={() => {
                        const next = debugMode === 'none'
                            ? 'edges'
                            : debugMode === 'edges'
                                ? 'overlay'
                                : debugMode === 'overlay'
                                    ? 'depth'
                                    : 'none';
                        setDebugMode(next);
                    }}
                >
                    <ImageIcon size={18} className="mr-2" />
                    {debugMode === 'none'
                        ? 'Debug: Off'
                        : debugMode === 'edges'
                            ? 'Debug: Edges'
                            : debugMode === 'overlay'
                                ? 'Debug: Overlay'
                                : 'Debug: Depth'}
                </Button>
                {(debugMode === 'edges' || debugMode === 'overlay' || debugMode === 'depth') && (
                    <div className="flex items-center gap-2 ml-3">
                        {(debugMode === 'edges' || debugMode === 'overlay') && (
                            <>
                                <span className="text-xs text-gray-600 hidden md:inline">Edges</span>
                                <input
                                    type="range"
                                    min={10}
                                    max={200}
                                    value={edgeThresholds.low}
                                    onChange={(e) =>
                                        setEdgeThresholds(prev => ({ ...prev, low: Number(e.target.value) }))
                                    }
                                />
                                <input
                                    type="range"
                                    min={50}
                                    max={300}
                                    value={edgeThresholds.high}
                                    onChange={(e) =>
                                        setEdgeThresholds(prev => ({ ...prev, high: Number(e.target.value) }))
                                    }
                                />
                            </>
                        )}
                        {debugMode === 'overlay' && (
                            <>
                                <span className="text-xs text-gray-600 hidden md:inline">Overlay</span>
                                <input
                                    type="range"
                                    min={0}
                                    max={100}
                                    value={Math.round(overlayOpacity * 100)}
                                    onChange={(e) => setOverlayOpacity(Number(e.target.value) / 100)}
                                />
                            </>
                        )}
                        {debugMode === 'depth' && (
                            <span className="text-xs text-gray-600 hidden md:inline">
                                Depth debug uses model defaults (no sliders).
                            </span>
                        )}
                    </div>
                )}
                <div className="flex-1"></div>

                {/* Cart Summary */}
                <div className="flex items-center gap-4 pl-6 border-l border-gray-300">
                    <div className="text-sm text-gray-600 hidden md:block">
                        <span className="font-bold text-gray-900 text-lg">{cart.length}</span> items
                    </div>
                    <Button onClick={handleExport} disabled={cart.length === 0}>
                        <Download size={18} className="mr-2" /> Export Dataset
                    </Button>
                </div>
            </div>

            <div className="flex flex-1 overflow-hidden">
                {/* Filters Sidebar */}
                {filtersOpen && (
                    <aside className="w-72 border-r border-gray-200 bg-white overflow-y-auto flex-shrink-0 transition-all duration-300">
                        <div className="p-4 border-b border-gray-100 flex justify-between items-center">
                            <h3 className="font-bold text-gray-900 flex items-center gap-2">
                                <Filter size={18} /> Refine
                            </h3>
                            <button onClick={() => setFiltersOpen(false)} className="text-gray-400 hover:text-gray-600 text-sm">
                                Hide
                            </button>
                        </div>

                        <div className="p-4 space-y-8">
                            {attrLoading && (
                                <p className="text-xs text-gray-500">Loading attribute taxonomy‚Ä¶</p>
                            )}

                            {!attrLoading && !attributes.length && (
                                <p className="text-xs text-gray-500">
                                    Attribute registry is empty. Run install & seeding to load attributes.yml.
                                </p>
                            )}

                            {groupedAttributes.map(group => (
                                <div key={group.name}>
                                    <h4 className="font-semibold text-xs text-gray-500 uppercase tracking-wider mb-3">
                                        {group.name}
                                    </h4>
                                    <div className="space-y-2">
                                        {group.items.map(attr => {
                                            const checked = selectedTags.includes(attr.key);
                                            return (
                                                <label
                                                    key={attr.id}
                                                    className="flex items-center gap-2 text-sm text-gray-700 cursor-pointer hover:text-blue-600 group"
                                                >
                                                    <input
                                                        type="checkbox"
                                                        className="rounded border-gray-300 text-blue-600 focus:ring-blue-500"
                                                        checked={checked}
                                                        onChange={() => handleAttributeToggle(attr.key)}
                                                    />
                                                    <span className="group-hover:translate-x-1 transition-transform">
                                                        <span title={attr.description || attr.name || attr.key}>{attr.name || attr.key}</span>
                                                    </span>
                                                </label>
                                            );
                                        })}
                                    </div>
                                </div>
                            ))}
                        </div>
                    </aside>
                )}

                {/* Masonry Grid */}
                <main className="flex-1 p-6 overflow-y-auto bg-gray-100">
                    <div className="flex justify-between items-center mb-4">
                        <p className="text-sm text-gray-500">
                            {loading
                                ? "Loading results‚Ä¶"
                                : `Showing ${images.length} result${images.length === 1 ? "" : "s"}`}
                        </p>
                        <button onClick={selectAll} className="text-sm text-blue-600 font-medium hover:underline">
                            Select All
                        </button>
                    </div>

                    {error && (
                        <div className="mb-4 text-xs text-red-600">
                            {error}
                        </div>
                    )}

                    {!error && !loading && images.length === 0 && (
                        <div className="mb-4 text-xs text-gray-500">
                            No data yet ‚Äì run the seeding scripts and science pipeline
                            (see README_v3.md ‚ÄúQuickstart & Seeding‚Äù) and then re-run this search.
                        </div>
                    )}

                    {(!images.length && !loading) && (
                        <div className="mt-10 flex flex-col items-center text-gray-400 text-sm">
                            <ImageIcon size={40} className="mb-3" />
                            <p>No results yet. Adjust your query or filters.</p>
                        </div>
                    )}

                    <div className="columns-1 sm:columns-2 md:columns-3 xl:columns-4 gap-6 space-y-6 pb-20">
                        {images.map(img => {
                            const tags = Array.isArray(img.tags) ? img.tags : [];
                            return (
                                <div
                                    key={img.id}
                                    className={`break-inside-avoid group relative rounded-xl overflow-hidden border border-gray-200 shadow-sm cursor-pointer bg-white transition-all duration-200 ${
                                        cart.includes(img.id)
                                            ? 'ring-2 ring-blue-500 transform scale-[1.02]'
                                            : 'hover:shadow-xl hover:-translate-y-1'
                                    }`}
                                    onClick={() => toggleCart(img.id)}
                                >
                                    {/* Image */}
                                    {debugMode === 'overlay' ? (
                                        <div className="relative w-full">
                                            <img
                                                src={img.url}
                                                alt={img.meta_data && img.meta_data.filename ? img.meta_data.filename : `Image ${img.id}`}
                                                className="w-full h-auto block"
                                                loading="lazy"
                                            />
                                            <img
                                                src={`/api/v1/debug/images/${img.id}/edges?t1=${edgeThresholds.low}&t2=${edgeThresholds.high}`}
                                                alt={`Edges for image ${img.id}`}
                                                className="w-full h-auto block absolute inset-0 pointer-events-none mix-blend-screen"
                                                style={{ opacity: overlayOpacity }}
                                            />
                                        </div>
                                    ) : (
                                        <img
                                            src={debugMode === 'edges'
                                                ? `/api/v1/debug/images/${img.id}/edges?t1=${edgeThresholds.low}&t2=${edgeThresholds.high}`
                                                : debugMode === 'depth'
                                                    ? `/api/v1/debug/images/${img.id}/depth`
                                                    : img.url}
                                        alt={img.meta_data && img.meta_data.filename ? img.meta_data.filename : `Image ${img.id}`}
                                        className="w-full h-auto block"
                                        loading="lazy"
                                    />

                                    {/* Overlay Info */}
                                    <div className="absolute top-2 right-2 bg-black/60 backdrop-blur text-white text-xs font-semibold px-2 py-1 rounded">
                                        {tags.length} tags
                                    </div>

                                    {/* Selection State */}
                                    {cart.includes(img.id) && (
                                        <div className="absolute inset-0 bg-blue-900/10 flex items-center justify-center backdrop-contrast-125">
                                            <div className="bg-blue-600 text-white p-3 rounded-full shadow-lg">
                                                <CheckSquare size={24} />
                                            </div>
                                        </div>
                                    )}

                                    {/* Tags */}
                                    <div className="p-3">
                                        <div className="flex flex-wrap gap-1">
                                            {tags.map(t => (
                                                <span
                                                    key={t}
                                                    className="bg-gray-100 text-gray-600 text-[10px] uppercase font-bold px-2 py-1 rounded"
                                                >
                                                    {t}
                                                </span>
                                            ))}
                                        </div>
                                    </div>
                                </div>
                            );
                        })}
                    </div>
                </main>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: frontend/apps/explorer/src/main.jsx
----- CONTENT START -----
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.jsx'
import { ToastProvider, MaintenanceOverlay } from '@shared';
import '../../../../index.css'
ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <ToastProvider>
      <MaintenanceOverlay />
      <App />
    </ToastProvider>
  </React.StrictMode>,
)----- CONTENT END -----
----- FILE PATH: frontend/apps/monitor/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Supervisor | Image Tagger v3</title>
  </head>
  <body class="bg-gray-100 h-screen">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/monitor/package.json
----- CONTENT START -----
{
  "name": "monitor",
  "private": true,
  "version": "3.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build"
  }
}----- CONTENT END -----
----- FILE PATH: frontend/apps/monitor/vite.config.js
----- CONTENT START -----
import { defineConfig } from 'vite';
import getBaseConfig from '../../vite.config.base';
import path from 'path';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));
export default defineConfig(getBaseConfig(__dirname));----- CONTENT END -----
----- FILE PATH: frontend/apps/monitor/src/App.jsx
----- CONTENT START -----
import React, { useEffect, useState, useMemo } from 'react';
import { Header, ApiClient, Button } from '@shared';
import { AlertTriangle, TrendingUp, Users, Activity, BarChart2, RefreshCcw, Eye, X } from 'lucide-react';

const api = new ApiClient('/api/v1/monitor', { 'X-User-Role': 'admin' });
const debugApi = new ApiClient('/api/v1/debug', { 'X-User-Role': 'admin' });

export default function MonitorApp() {
    const [teamStats, setTeamStats] = useState([]);
    const [irrStats, setIrrStats] = useState([]);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [inspectorImage, setInspectorImage] = useState(null);
    const [inspectorRecords, setInspectorRecords] = useState([]);
    const [inspectorLoading, setInspectorLoading] = useState(false);
    const [inspectorError, setInspectorError] = useState(null);
    const [pipelineHealth, setPipelineHealth] = useState(null);
    const [pipelineLoading, setPipelineLoading] = useState(false);
    const [pipelineError, setPipelineError] = useState(null);

    useEffect(() => {
        loadData();
    }, []);

    async function loadData() {
        setLoading(true);
        setError(null);
        try {
            const [velocity, irr, pipeline] = await Promise.all([
                api.get('/velocity'),
                api.get('/irr'),
                debugApi.get('/pipeline_health'),
            ]);
            setTeamStats(Array.isArray(velocity) ? velocity : []);
            setIrrStats(Array.isArray(irr) ? irr : []);
            setPipelineHealth(pipeline || null);
        } catch (err) {
            console.error('Failed to load supervisor data', err);
            setError(err.message || 'Failed to load supervisor dashboard');
        } finally {
            setLoading(false);
            setPipelineLoading(false);
        }
    }

    const totalValidations = useMemo(
        () => teamStats.reduce((sum, u) => sum + (u.images_validated || 0), 0),
        [teamStats]
    );

    const flaggedCount = useMemo(
        () => teamStats.filter(u => u.status === 'flagged').length,
        [teamStats]
    );

    const globalIRR = useMemo(() => {
        if (!irrStats.length) return null;
        const total = irrStats.reduce((sum, r) => sum + (r.agreement_score || 0), 0);
        return total / irrStats.length;
    }, [irrStats]);

    const activeTaggers = useMemo(
        () => teamStats.length,
        [teamStats]
    );


async function loadPipelineHealth() {
    setPipelineLoading(true);
    setPipelineError(null);
    try {
        const data = await debugApi.get('/pipeline_health');
        setPipelineHealth(data || null);
    } catch (err) {
        console.error('Failed to load pipeline health', err);
        setPipelineError(err && err.message ? err.message : 'Failed to load pipeline health');
    } finally {
        setPipelineLoading(false);
    }
}
    async function openInspector(row) {
        setInspectorImage({ image_id: row.image_id, filename: row.filename });
        setInspectorRecords([]);
        setInspectorError(null);
        setInspectorLoading(true);
        try {
            const data = await api.get(`/image/${row.image_id}/validations`);
            setInspectorRecords(Array.isArray(data) ? data : []);
        } catch (err) {
            console.error('Failed to load inspection data', err);
            setInspectorError(err.message || 'Failed to load inspection data');
        } finally {
            setInspectorLoading(false);
        }
    }

    function closeInspector() {
        setInspectorImage(null);
        setInspectorRecords([]);
        setInspectorError(null);
    }

    return (
        <div className="min-h-screen bg-gray-100 pb-10">
            <Header appName="Supervisor" title="Quality Control Dashboard" />

            <div className="p-8 max-w-7xl mx-auto space-y-8">
                {/* Controls / Status */}
                <div className="flex items-center justify-between gap-4">
                    <div>
                        <p className="text-sm text-gray-500">
                            Monitor team velocity, agreement, and potential quality issues.
                        </p>
                        {error && (
                            <p className="text-xs text-red-600 mt-1">
                                {error}
                            </p>
                        )}
                    </div>
                    <div className="flex items-center gap-3">
                        {loading && (
                            <span className="text-xs text-gray-500 flex items-center gap-2">
                                <Activity className="animate-spin" size={14} /> Updating‚Ä¶
                            </span>
                        )}
                        <Button variant="secondary" onClick={loadData}>
                            <RefreshCcw size={16} className="mr-2" /> Refresh
                        </Button>
                    </div>
                </div>


<div className="mt-2 p-2 rounded-md bg-blue-50 border border-blue-100 text-[11px] text-blue-900 space-y-1">
    <p className="font-semibold">How to use the Supervisor dashboard</p>
    <ul className="list-disc ml-4 space-y-0.5">
        <li>This view is for supervisors and PIs to track team throughput, agreement, and possible quality issues.</li>
        <li>Use the metrics above to spot low volume, low IRR, or spikes in errors that may need investigation.</li>
        <li>Drill into specific images or annotators using Tag Inspector when a pattern looks suspicious.</li>
        <li>If metric tiles fail to load or look wrong, treat that as a system issue and notify an engineer.</li>
    </ul>
</div>

                {/* Top Metrics */}
                <div className="grid grid-cols-1 md:grid-cols-4 gap-4">
                    <MetricCard
                        icon={<Activity />}
                        label="Total Validations"
                        value={totalValidations}
                        unit="decisions logged"
                    />
                    <MetricCard
                        icon={<TrendingUp />}
                        label="Global IRR"
                        value={globalIRR !== null ? globalIRR.toFixed(2) : '‚Äî'}
                        unit="agreement"
                    />
                    <MetricCard
                        icon={<Users />}
                        label="Active Taggers"
                        value={activeTaggers}
                        unit="users"
                    />
                    <MetricCard
                        icon={<AlertTriangle />}
                        label="Flagged Taggers"
                        value={flaggedCount}
                        unit="needs review"
                        danger={flaggedCount > 0}
                    />
                </div>


{/* Science Pipeline Health */}
<section className="mt-4">
    <div className="flex items-center justify-between mb-2">
        <div className="flex items-center gap-2">
            <BarChart2 size={16} className="text-blue-600" />
            <h2 className="text-sm font-semibold text-gray-900">
                Science pipeline health
            </h2>
        </div>
        <div className="flex items-center gap-2">
            {pipelineHealth && (
                <span className="text-[11px] text-gray-500">
                    Last check: contracts by tier for configured analyzers.
                </span>
            )}
            <Button
                variant="ghost"
                size="xs"
                onClick={loadPipelineHealth}
                disabled={pipelineLoading}
            >
                <RefreshCcw size={14} className="mr-1" />
                {pipelineLoading ? 'Checking‚Ä¶' : 'Re-check'}
            </Button>
        </div>
    </div>

    <div className="p-3 rounded-lg border border-gray-200 bg-white text-xs text-gray-800 space-y-2">
        <p className="text-[11px] text-blue-900">
            This check instantiates each analyzer once and reports its tier and contracts.
            It does <span className="font-semibold">not</span> process real images, but it will
            catch most import/contract breakage before a full science run fails.
        </p>

        <div className="flex flex-wrap gap-4 items-center">
            <div>
                <p className="text-[10px] uppercase tracking-wide text-gray-500 font-semibold">
                    Import status
                </p>
                <p className="text-sm font-medium">
                    {pipelineHealth
                        ? (pipelineHealth.import_ok ? 'OK' : 'FAILED')
                        : 'Not checked yet'}
                </p>
            </div>
            <div>
                <p className="text-[10px] uppercase tracking-wide text-gray-500 font-semibold">
                    OpenCV
                </p>
                <p className="text-sm font-medium">
                    {pipelineHealth
                        ? (pipelineHealth.cv2_available ? 'available' : 'missing')
                        : '‚Äî'}
                </p>
            </div>
            {pipelineError && (
                <div className="flex-1 text-[11px] text-red-600">
                    {pipelineError}
                </div>
            )}
        </div>

        {pipelineHealth && pipelineHealth.analyzers_by_tier && (
            <div className="mt-2">
                <p className="text-[11px] font-semibold text-gray-700 mb-1">
                    Analyzers by tier
                </p>
                <div className="grid grid-cols-1 md:grid-cols-3 gap-3">
                    {Object.entries(pipelineHealth.analyzers_by_tier).map(([tier, analyzers]) => (
                        <div key={tier} className="border border-gray-100 rounded-md p-2 bg-gray-50">
                            <p className="text-[11px] font-semibold text-gray-800 mb-1">
                                Tier {tier} ({Array.isArray(analyzers) ? analyzers.length : 0})
                            </p>
                            <ul className="space-y-0.5">
                                {Array.isArray(analyzers) && analyzers.map((a) => (
                                    <li key={a.name} className="flex flex-col">
                                        <span className="text-[11px] font-medium text-gray-900">
                                            {a.name}
                                        </span>
                                        <span className="text-[10px] text-gray-500">
                                            requires: {Array.isArray(a.requires) && a.requires.length
                                                ? a.requires.join(', ')
                                                : '‚Äî'}
                                        </span>
                                        <span className="text-[10px] text-gray-500">
                                            provides: {Array.isArray(a.provides) && a.provides.length
                                                ? a.provides.join(', ')
                                                : '‚Äî'}
                                        </span>
                                    </li>
                                ))}
                            </ul>
                        </div>
                    ))}
                </div>
            </div>
        )}

        {pipelineHealth && pipelineHealth.warnings && pipelineHealth.warnings.length > 0 && (
            <div className="mt-2">
                <p className="text-[11px] font-semibold text-amber-700 mb-1">
                    Warnings
                </p>
                <ul className="list-disc ml-4 space-y-0.5 text-[11px] text-amber-800">
                    {pipelineHealth.warnings.map((w, idx) => (
                        <li key={idx}>{w}</li>
                    ))}
                </ul>
            </div>
        )}

        {pipelineHealth && pipelineHealth.analyzer_errors && pipelineHealth.analyzer_errors.length > 0 && (
            <div className="mt-2">
                <p className="text-[11px] font-semibold text-red-700 mb-1">
                    Analyzer errors
                </p>
                <ul className="list-disc ml-4 space-y-0.5 text-[11px] text-red-800">
                    {pipelineHealth.analyzer_errors.map((e, idx) => (
                        <li key={idx}>
                            {e.analyzer}: {e.error}
                        </li>
                    ))}
                </ul>
            </div>
        )}
    </div>
</section>

                <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
                    {/* Team Velocity Table */}
                    <div className="lg:col-span-2 bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                        <div className="p-4 flex items-center justify-between border-b border-gray-100">
                            <div className="flex items-center gap-2">
                                <BarChart2 className="text-blue-500" size={18} />
                                <h2 className="font-semibold text-gray-900 text-sm">
                                    Tagger Velocity & Quality
                                </h2>
                            </div>
                            <span className="text-xs text-gray-400">
                                Based on Validation.duration_ms and per-user totals
                            </span>
                        </div>

                        <div className="overflow-x-auto">
                            <table className="w-full text-left">
                                <thead>
                                    <tr className="text-xs font-bold text-gray-500 uppercase tracking-wider border-b">
                                        <th className="pb-3 pl-2">User</th>
                                        <th className="pb-3">Validations</th>
                                        <th className="pb-3">Avg. Dwell</th>
                                        <th className="pb-3 text-right pr-2">Status</th>
                                    </tr>
                                </thead>
                                <tbody className="divide-y divide-gray-100">
                                    {teamStats.map(user => (
                                        <TaggerRow key={user.user_id} user={user} />
                                    ))}
                                    {!teamStats.length && !loading && (
                                        <tr>
                                            <td colSpan={4} className="py-6 text-center text-xs text-gray-400">
                                                No validations found yet. Run a tagging session in Workbench.
                                            </td>
                                        </tr>
                                    )}
                                </tbody>
                            </table>
                        </div>
                    </div>

                    {/* IRR / Agreement Heatmap */}
                    <div className="bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                        <div className="p-4 flex items-center justify-between border-b border-gray-100">
                            <div className="flex items-center gap-2">
                                <BarChart2 className="text-purple-500" size={18} />
                                <h2 className="font-semibold text-gray-900 text-sm">
                                    Inter-Rater Agreement
                                </h2>
                            </div>
                            <span className="text-xs text-gray-400">
                                Simple majority-based agreement per image
                            </span>
                        </div>

                        <div className="p-4">
                            <p className="text-xs text-gray-500 mb-3">
                                Rows show images with multiple raters. Agreement is the proportion
                                of raters that match the majority decision (value &gt; 0.5 vs ‚â§ 0.5).
                            </p>

                            <div className="overflow-x-auto">
                                <table className="w-full text-left">
                                    <thead>
                                        <tr className="text-xs font-bold text-gray-500 uppercase tracking-wider border-b">
                                            <th className="pb-3">Image</th>
                                            <th className="pb-3">Agreement</th>
                                            <th className="pb-3">Conflicts</th>
                                            <th className="pb-3">Raters</th>
                                            <th className="pb-3 text-right">Actions</th>
                                        </tr>
                                    </thead>
                                    <tbody className="divide-y divide-gray-100">
                                        {irrStats.map(row => (
                                            <IRRRow key={row.image_id} row={row} onInspect={() => openInspector(row)} />
                                        ))}
                                        {!irrStats.length && !loading && (
                                            <tr>
                                                <td colSpan={4} className="py-6 text-center text-xs text-gray-400">
                                                    No overlapping ratings yet. Encourage multiple taggers to rate the same images.
                                                </td>
                                            </tr>
                                        )}
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                                {inspectorImage && (
                    <InspectorDrawer
                        image={inspectorImage}
                        records={inspectorRecords}
                        loading={inspectorLoading}
                        error={inspectorError}
                        onClose={closeInspector}
                    />
                )}
            </div>
        </div>
    );
}

function MetricCard({ icon, label, value, unit, danger = false }) {
    return (
        <div className={`bg-white p-6 rounded-xl shadow border-l-4 ${
            danger ? 'border-red-500' : 'border-blue-500'
        } flex items-center justify-between`}>
            <div>
                <p className="text-gray-500 text-xs font-bold uppercase tracking-wider">{label}</p>
                <h3 className="text-2xl font-bold text-gray-900 mt-1">{value}</h3>
                <p className="text-xs text-gray-400 font-medium">{unit}</p>
            </div>
            <div className={`p-3 rounded-lg ${danger ? 'bg-red-50 text-red-500' : 'bg-blue-50 text-blue-600'}`}>
                {icon}
            </div>
        </div>
    );
}

function TaggerRow({ user }) {
    const avgSec = (user.avg_duration_ms || 0) / 1000.0;
    const isFlagged = user.status === 'flagged';

    return (
        <tr className={`hover:bg-gray-50 transition-colors ${isFlagged ? 'bg-red-50/40' : ''}`}>
            <td className="py-3 pl-2 font-medium text-gray-900">
                {user.username || `user-${user.user_id}`}
            </td>
            <td className="py-3 text-gray-600">
                {user.images_validated} validations
            </td>
            <td className="py-3 text-gray-600">
                {avgSec.toFixed(2)}s / decision
            </td>
            <td className="py-3 pr-2 text-right">
                <span className={`inline-flex items-center px-2 py-1 rounded-full text-[11px] font-semibold ${
                    isFlagged
                        ? 'bg-red-100 text-red-700'
                        : 'bg-emerald-100 text-emerald-700'
                }`}>
                    {isFlagged ? 'Flagged' : 'Healthy'}
                </span>
            </td>
        </tr>
    );
}

function IRRRow({ row, onInspect }) {
    const agreePct = (row.agreement_score || 0) * 100;
    const conflict = row.conflict_count || 0;
    const raters = Array.isArray(row.raters) ? row.raters : [];

    const severity =
        agreePct >= 90 ? 'high' :
        agreePct >= 70 ? 'medium' :
        'low';

    let badgeClass = 'bg-emerald-100 text-emerald-700';
    if (severity === 'medium') {
        badgeClass = 'bg-amber-100 text-amber-700';
    } else if (severity === 'low') {
        badgeClass = 'bg-red-100 text-red-700';
    }

    return (
        <tr className="hover:bg-gray-50 transition-colors">
            <td className="py-3 text-gray-900 text-sm">
                {row.filename}
            </td>
            <td className="py-3">
                <span className={`inline-flex items-center px-2 py-1 rounded-full text-[11px] font-semibold ${badgeClass}`}>
                    {agreePct.toFixed(0)}% agreement
                </span>
            </td>
            <td className="py-3 text-gray-600 text-sm">
                {conflict} conflict{conflict === 1 ? '' : 's'}
            </td>
            <td className="py-3 text-right text-xs text-gray-500">
                {raters.join(', ')}
            </td>
        </tr>
    );
}


function InspectorDrawer({ image, records, loading, error, onClose }) {
    const [detail, setDetail] = useState(null);
    const [detailLoading, setDetailLoading] = useState(true);
    const [detailError, setDetailError] = useState(null);
    const [showHelp, setShowHelp] = useState(false);

    useEffect(() => {
        if (!image || !image.image_id) {
            return;
        }

        let cancelled = false;

        async function fetchInspector() {
            setDetailLoading(true);
            setDetailError(null);
            try {
                const resp = await fetch(`/api/v1/monitor/image/${image.image_id}/inspector`);
                if (!resp.ok) {
                    throw new Error(`Inspector HTTP ${resp.status}`);
                }
                const data = await resp.json();
                if (!cancelled) {
                    setDetail(data);
                }
            } catch (err) {
                console.error('Failed to load inspector detail', err);
                if (!cancelled) {
                    setDetailError(err.message || 'Failed to load inspector detail');
                }
            } finally {
                if (!cancelled) {
                    setDetailLoading(false);
                }
            }
        }

        fetchInspector();

        return () => {
            cancelled = true;
        };
    }, [image?.image_id]);

    const isBusy = loading || detailLoading;
    const effectiveError = error || detailError;

    const tags = detail?.tags || [];
    const features = detail?.features || [];
    const inspectorValidations = detail?.validations || records || [];

    return (
        <div className="fixed inset-0 bg-black/30 flex justify-end z-40">
            <div className="w-full max-w-5xl bg-white h-full shadow-xl flex flex-col">
                <div className="px-4 py-3 border-b flex items-center justify-between">
                    <div>
                        <h3 className="text-sm font-semibold text-gray-900">
                            Tag Inspector
                        </h3>
                        <p className="text-xs text-gray-500">
                            Image {image.image_id} ¬∑ {image.filename}
                        </p>
                        {detail?.image?.url && (
                            <p className="text-[11px] text-gray-400">
                                Source: {detail.image.url}
                            </p>
                        )}
                    </div>

<div className="flex items-center gap-1">
    <button
        type="button"
        onClick={() => setShowHelp(prev => !prev)}
        className="p-1 rounded-full hover:bg-gray-100 text-gray-500"
    >
        <HelpCircle size={16} />
    </button>
    <button
        type="button"
        onClick={onClose}
        className="p-1 rounded-full hover:bg-gray-100 text-gray-500"
    >
        <X size={16} />
    </button>
</div>
                </div>


{showHelp && (
    <div className="mx-4 my-2 p-2 rounded-md bg-blue-50 border border-blue-100 text-[11px] text-blue-900 space-y-1">
        <p className="font-semibold">How to read Tag Inspector</p>
        <ul className="list-disc ml-4 space-y-0.5">
            <li>This view is read-only and for supervisors/admins.</li>
            <li>The top snapshot shows how many science attributes, indices, and the IRR value (if available) were computed for this image.</li>
            <li>The High-level indices table lists the composite indices that can feed BN models (e.g., restorativeness, clutter, fluency).</li>
            <li>The Science attributes table shows the raw numeric features computed by the pipeline for this particular room image.</li>
            <li>The Human validations table shows who rated which attributes, with values, dwell times, and timestamps.</li>
        </ul>
    </div>
)
                <div className="flex-1 overflow-y-auto p-4 space-y-4 text-xs">
                    {isBusy && (
                        <div className="text-gray-500 flex items-center gap-2">
                            <Activity className="animate-spin" size={14} />
                            Loading inspector data‚Ä¶
                        </div>
                    )}

                    {effectiveError && (
                        <div className="text-red-600 text-xs">
                            {effectiveError}
                        </div>
                    )}

                    {!isBusy && !effectiveError && (
                        <>
                            <div className="grid grid-cols-1 md:grid-cols-3 gap-4 items-start">
                                {/* Image preview + basic meta */}
                                <div className="md:col-span-1 space-y-3">
                                    <div className="border rounded-lg overflow-hidden bg-gray-50">
                                        {detail?.image?.url ? (
                                            <img
                                                src={detail.image.url}
                                                alt={image.filename}
                                                className="w-full h-40 object-contain bg-black/5"
                                            />
                                        ) : (
                                            <div className="h-40 flex items-center justify-center text-gray-400 text-[11px]">
                                                No preview available
                                            </div>
                                        )}
                                    </div>
                                    <div className="space-y-1">
                                        <div className="font-semibold text-[11px] text-gray-700">
                                            Science snapshot
                                        </div>
                                        <div className="text-[11px] text-gray-500 space-y-0.5">
                                            <div>
                                                <span className="font-medium">Science attrs:</span>{" "}
                                                {features.length}
                                            </div>
                                            <div>
                                                <span className="font-medium">Indices:</span>{" "}
                                                {tags.length}
                                            </div>
                                            {detail?.bn?.irr !== undefined && detail.bn.irr !== null && (
                                                <div>
                                                    <span className="font-medium">IRR:</span>{" "}
                                                    {detail.bn.irr.toFixed
                                                        ? detail.bn.irr.toFixed(3)
                                                        : detail.bn.irr}
                                                </div>
                                            )}
                                        </div>
                                    </div>
                                </div>

                                {/* High-level indices / tags */}
                                <div className="md:col-span-2">
                                    <div className="flex items-center justify-between mb-2">
                                        <div className="font-semibold text-[11px] text-gray-700">
                                            High-level indices (BN inputs)
                                        </div>
                                        <div className="text-[11px] text-gray-400">
                                            {tags.length ? `${tags.length} indices` : "No indices yet"}
                                        </div>
                                    </div>
                                    <div className="border rounded-lg max-h-56 overflow-y-auto">
                                        {tags.length ? (
                                            <table className="w-full text-left text-[11px]">
                                                <thead className="bg-gray-50 border-b">
                                                    <tr className="text-gray-500 uppercase tracking-wide">
                                                        <th className="py-2 px-3">Index</th>
                                                        <th className="py-2 px-3">Bin / Value</th>
                                                        <th className="py-2 px-3">Label</th>
                                                    </tr>
                                                </thead>
                                                <tbody className="divide-y divide-gray-100">
                                                    {tags.map(tag => (
                                                        <tr key={tag.key} className="hover:bg-gray-50">
                                                            <td className="py-1.5 px-3 text-gray-700">
                                                                {tag.key}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-800">
                                                                {tag.bin || tag.value || "‚Äî"}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-500">
                                                                {tag.label || tag.description || "‚Äî"}
                                                            </td>
                                                        </tr>
                                                    ))}
                                                </tbody>
                                            </table>
                                        ) : (
                                            <div className="p-3 text-[11px] text-gray-400">
                                                No composite indices have been stored yet for this image.
                                            </div>
                                        )}
                                    </div>
                                </div>
                            </div>

                            {/* Features + validations */}
                            <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
                                <div>
                                    <div className="flex items-center justify-between mb-2">
                                        <div className="font-semibold text-[11px] text-gray-700">
                                            Science attributes
                                        </div>
                                        <div className="text-[11px] text-gray-400">
                                            {features.length ? `${features.length} attributes` : "‚Äî"}
                                        </div>
                                    </div>
                                    <div className="border rounded-lg max-h-64 overflow-y-auto">
                                        {features.length ? (
                                            <table className="w-full text-left text-[11px]">
                                                <thead className="bg-gray-50 border-b">
                                                    <tr className="text-gray-500 uppercase tracking-wide">
                                                        <th className="py-2 px-3">Key</th>
                                                        <th className="py-2 px-3">Value</th>
                                                    </tr>
                                                </thead>
                                                <tbody className="divide-y divide-gray-100">
                                                    {features.map(feat => (
                                                        <tr key={feat.key} className="hover:bg-gray-50">
                                                            <td className="py-1.5 px-3 text-gray-700">
                                                                {feat.key}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-800">
                                                                {feat.value === null || feat.value === undefined
                                                                    ? "‚Äî"
                                                                    : feat.value.toFixed
                                                                    ? feat.value.toFixed(3)
                                                                    : feat.value}
                                                            </td>
                                                        </tr>
                                                    ))}
                                                </tbody>
                                            </table>
                                        ) : (
                                            <div className="p-3 text-[11px] text-gray-400">
                                                No science attributes recorded yet for this image.
                                            </div>
                                        )}
                                    </div>
                                </div>

                                <div>
                                    <div className="flex items-center justify-between mb-2">
                                        <div className="font-semibold text-[11px] text-gray-700">
                                            Human validations
                                        </div>
                                        <div className="text-[11px] text-gray-400">
                                            {inspectorValidations.length
                                                ? `${inspectorValidations.length} validations`
                                                : "‚Äî"}
                                        </div>
                                    </div>
                                    <div className="border rounded-lg max-h-64 overflow-y-auto">
                                        {inspectorValidations.length ? (
                                            <table className="w-full text-left text-[11px]">
                                                <thead className="bg-gray-50 border-b">
                                                    <tr className="text-gray-500 uppercase tracking-wide">
                                                        <th className="py-2 px-3">User</th>
                                                        <th className="py-2 px-3">Attribute</th>
                                                        <th className="py-2 px-3">Value</th>
                                                        <th className="py-2 px-3">Dwell (ms)</th>
                                                        <th className="py-2 px-3">When</th>
                                                    </tr>
                                                </thead>
                                                <tbody className="divide-y divide-gray-100">
                                                    {inspectorValidations.map(r => (
                                                        <tr key={r.id} className="hover:bg-gray-50">
                                                            <td className="py-1.5 px-3 text-gray-800">
                                                                {r.username || `user-${r.user_id}`}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-600">
                                                                {r.attribute_key}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-600">
                                                                {r.value && r.value.toFixed
                                                                    ? r.value.toFixed(2)
                                                                    : r.value}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-600">
                                                                {r.duration_ms ?? "‚Äî"}
                                                            </td>
                                                            <td className="py-1.5 px-3 text-gray-400">
                                                                {r.created_at
                                                                    ? new Date(r.created_at).toLocaleString()
                                                                    : "‚Äî"}
                                                            </td>
                                                        </tr>
                                                    ))}
                                                </tbody>
                                            </table>
                                        ) : (
                                            <div className="p-3 text-[11px] text-gray-400">
                                                No human validations recorded yet for this image.
                                            </div>
                                        )}
                                    </div>
                                </div>
                            </div>
                        </>
                    )}
                </div>
            </div>
        </div>
    );
}----- CONTENT END -----
----- FILE PATH: frontend/apps/monitor/src/main.jsx
----- CONTENT START -----
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.jsx'
import { ToastProvider, MaintenanceOverlay } from '@shared';
import '../../../../index.css'
ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <ToastProvider>
      <MaintenanceOverlay />
      <App />
    </ToastProvider>
  </React.StrictMode>,
)----- CONTENT END -----
----- FILE PATH: frontend/apps/workbench/index.html
----- CONTENT START -----
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Workbench | Image Tagger v3</title>
  </head>
  <body class="bg-gray-100 h-screen overflow-hidden">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>----- CONTENT END -----
----- FILE PATH: frontend/apps/workbench/package.json
----- CONTENT START -----
{
  "name": "workbench",
  "private": true,
  "version": "3.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build"
  }
}----- CONTENT END -----
----- FILE PATH: frontend/apps/workbench/vite.config.js
----- CONTENT START -----
import { defineConfig } from 'vite';
import getBaseConfig from '../../vite.config.base';
import path from 'path';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));
export default defineConfig(getBaseConfig(__dirname));----- CONTENT END -----
----- FILE PATH: frontend/apps/workbench/src/App.jsx
----- CONTENT START -----
import React, { useEffect, useState, useCallback } from 'react';
import { ApiClient, Button, Header } from '@shared';
import { AlertCircle, Zap, Keyboard, CheckCircle2, XCircle, HelpCircle, Menu, ChevronLeft, ChevronRight } from 'lucide-react';

const api = new ApiClient('/api/v1/workbench');

export default function WorkbenchApp() {
    const [image, setImage] = useState(null);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);
    const [streak, setStreak] = useState(0);
    const [lastAction, setLastAction] = useState(null);
    
    // UX: Collapsible Sidebar State
    const [isSidebarOpen, setSidebarOpen] = useState(true);
    const [isMobile, setIsMobile] = useState(window.innerWidth < 1024);

    // Handle Window Resize
    useEffect(() => {
        const handleResize = () => {
            const mobile = window.innerWidth < 1024;
            setIsMobile(mobile);
            if (mobile) setSidebarOpen(false); // Auto-close on mobile
            else setSidebarOpen(true); // Auto-open on desktop
        };
        window.addEventListener('resize', handleResize);
        return () => window.removeEventListener('resize', handleResize);
    }, []);

    useEffect(() => {
        loadNextImage();
    }, []);

    const loadNextImage = async () => {
        setLoading(true);
        setError(null);
        try {
            const data = await api.get('/next');
            setImage(data);
        } catch (err) {
            if (err && err.isMaintenance) {
                setError('__MAINTENANCE__:' + (err.message || 'System temporarily unavailable (503).'));
            } else {
                setError((err && err.message) ? err.message : 'Failed to load next image');
            }
        } finally {
            setLoading(false);
        }
    };

    const handleDecision = async (value) => {
        if (!image) return;
        const currentId = image.id;
        const action = value === 1.0 ? 'accept' : 'reject';
        
        // 1. Optimistic UI Update
        setLastAction(action);
        setImage(null); 
        setLoading(true);
        setStreak(s => s + 1);
        
        setTimeout(() => setLastAction(null), 500);

        try {
            await api.post('/validate', {
                image_id: currentId,
                attribute_key: "global.relevance", 
                value: value,
                duration_ms: 1200 
            });
            loadNextImage();
        } catch (err) {
            if (err && err.isMaintenance) {
                setError('__MAINTENANCE__:' + (err.message || 'System temporarily unavailable (503).'));
            } else {
                setError("Failed to save decision: " + ((err && err.message) ? err.message : 'Unknown error'));
            }
            setLoading(false);
            setStreak(0);
        }
    };

    const handleKeyDown = useCallback((event) => {
        if (event.key === '1') handleDecision(0.0);
        if (event.key === '2') handleDecision(1.0);
        if (event.key === 'b') setSidebarOpen(prev => !prev); // 'b' for sidebar
    }, [image]);

    useEffect(() => {
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [handleKeyDown]);

    const isMaintenance = typeof error === 'string' && error.startsWith('__MAINTENANCE__:');
    const maintenanceMessage = isMaintenance ? error.replace('__MAINTENANCE__:', '') : null;

    return (
        <div className="relative flex flex-col h-screen bg-gray-50 overflow-hidden">
            <Header appName="Workbench" title="Tagger Station" />

            {isMaintenance && (
                <div className="absolute inset-0 z-50 flex items-center justify-center bg-black/70 text-gray-100">
                    <div className="bg-gray-900/90 px-6 py-4 rounded-lg shadow-lg max-w-md text-center">
                        <div className="font-semibold mb-2 text-sm">System temporarily unavailable</div>
                        <div className="text-xs mb-3">{maintenanceMessage || "The backend is reporting a maintenance / outage condition (503). Please pause tagging and try again shortly."}</div>
                        <div className="text-[10px] text-gray-400">If this persists for more than a few minutes, contact your TA or lab lead.</div>
                    </div>
                </div>
            )}

            {/* Quick Help & Toolbar */}
            <div className="border-b border-blue-100 bg-blue-50 px-4 py-2 text-xs text-blue-900 flex items-center justify-between">
                <div className="flex items-center gap-2">
                    <HelpCircle size={16} className="flex-shrink-0" />
                    <span className="hidden sm:inline">
                        Press <span className="font-semibold">1</span> (NO) or <span className="font-semibold">2</span> (YES). 
                        <span className="font-semibold"> 'b'</span> toggles sidebar.
                    </span>
                    <span className="sm:hidden">1=NO, 2=YES</span>
                </div>
                {/* Mobile Menu Toggle */}
                <button 
                    onClick={() => setSidebarOpen(!isSidebarOpen)}
                    className="p-1 hover:bg-blue-100 rounded text-blue-700 lg:hidden"
                >
                    <Menu size={18} />
                </button>
            </div>

<div className="mx-4 my-2 p-2 rounded-md bg-blue-50 border border-blue-100 text-[11px] text-blue-900 space-y-1">
    <p className="font-semibold">Workbench: what you do here</p>
    <ul className="list-disc ml-4 space-y-0.5">
        <li>This screen is for taggers: make fast, careful YES/NO decisions for each image.</li>
        <li>Use the keyboard shortcuts (1 = NO, 2 = YES) to stay on the canvas and minimize mouse movement.</li>
        <li>The large image area shows the current target; the right sidebar shows context, queue progress, and any extra guidance.</li>
        <li>Each decision is logged as a validation so supervisors can monitor consistency and quality later.</li>
    </ul>
</div>
            
            <div className="flex-1 flex relative overflow-hidden">
                {/* Main Canvas */}
                <main className={`flex-1 relative bg-black flex items-center justify-center group transition-all duration-300 ease-in-out ${isSidebarOpen && !isMobile ? 'mr-0' : ''}`}>
                    {loading && !image && (
                        <div className="text-white/50 animate-pulse flex flex-col items-center gap-2">
                            <Zap size={32} />
                            <span>Fetching Task...</span>
                        </div>
                    )}
                    
                    {error && (
                        <div className="absolute inset-0 bg-gray-900 z-50 flex flex-col items-center justify-center text-red-400">
                            <AlertCircle size={48} />
                            <p className="mt-4 font-bold text-xl">{error}</p>
                            <Button onClick={loadNextImage} variant="secondary" className="mt-6">Retry</Button>
                        </div>
                    )}

                    {image && !loading && (
                        <img 
                            src={image.url} 
                            alt="Tagging Target" 
                            className="max-w-full max-h-full object-contain shadow-2xl"
                        />
                    )}

                    {/* Feedback Animation */}
                    {lastAction === 'accept' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-green-500/20 pointer-events-none animate-ping">
                            <CheckCircle2 size={128} className="text-green-400" />
                        </div>
                    )}
                    {lastAction === 'reject' && (
                        <div className="absolute inset-0 flex items-center justify-center bg-red-500/20 pointer-events-none animate-ping">
                            <XCircle size={128} className="text-red-400" />
                        </div>
                    )}

                    <div className="absolute top-4 right-4 bg-black/50 backdrop-blur px-4 py-2 rounded-full text-white font-mono text-sm border border-white/20">
                        üî• Streak: {streak}
                    </div>
                    
                    {/* Desktop Sidebar Toggle (Floating on Canvas edge) */}
                    <button 
                        onClick={() => setSidebarOpen(!isSidebarOpen)}
                        className="absolute top-1/2 right-0 transform -translate-y-1/2 translate-x-1/2 bg-white border border-gray-200 rounded-full p-1 shadow-md hover:bg-gray-50 z-20 hidden lg:flex"
                    >
                        {isSidebarOpen ? <ChevronRight size={16} /> : <ChevronLeft size={16} />}
                    </button>
                </main>

                {/* Tool Sidebar - Responsive Drawer */}
                <aside 
                    className={`
                        fixed lg:relative inset-y-0 right-0 z-30
                        w-80 bg-white border-l border-gray-200 flex flex-col shadow-2xl lg:shadow-none
                        transition-transform duration-300 ease-in-out
                        ${isSidebarOpen ? 'translate-x-0' : 'translate-x-full lg:hidden'}
                    `}
                >
                    <div className="p-6 border-b border-gray-100 flex justify-between items-center">
                        <h2 className="font-bold text-gray-800 text-sm uppercase tracking-wider">Current Task</h2>
                        {isMobile && (
                            <button onClick={() => setSidebarOpen(false)}>
                                <XCircle size={20} className="text-gray-400" />
                            </button>
                        )}
                    </div>
                    
                    <div className="p-6 border-b border-gray-100">
                        <p className="text-lg font-medium text-gray-900">Is this image "Modern"?</p>
                    </div>
                    
                    <div className="flex-1 p-6 flex flex-col gap-4 overflow-y-auto">
                        <div className="bg-blue-50 p-4 rounded-lg border border-blue-100 text-blue-900 text-sm leading-relaxed">
                            <strong>Instructions:</strong> Look for clean lines, glass curtains, lack of ornament, and industrial materials.
                        </div>
                        {/* Future: Region List can go here */}
                    </div>

                    <div className="p-6 border-t border-gray-200 bg-gray-50">
                        <div className="grid grid-cols-2 gap-4">
                            <Button onClick={() => handleDecision(0.0)} variant="danger" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">NO</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-black/10 px-2 py-1 rounded">Key: 1</span>
                            </Button>
                            <Button onClick={() => handleDecision(1.0)} variant="primary" className="h-20 flex flex-col">
                                <span className="text-2xl font-bold">YES</span>
                                <span className="text-xs uppercase opacity-75 font-mono bg-white/20 px-2 py-1 rounded">Key: 2</span>
                            </Button>
                        </div>
                        <div className="mt-4 flex items-center justify-center text-gray-400 text-xs gap-2">
                            <Keyboard size={14} />
                            <span>Shortcuts Active</span>
                        </div>
                    </div>
                </aside>
                
                {/* Mobile Backdrop */}
                {isMobile && isSidebarOpen && (
                    <div 
                        className="fixed inset-0 bg-black/50 z-20"
                        onClick={() => setSidebarOpen(false)}
                    />
                )}
            </div>
        </div>
    );
}
----- CONTENT END -----
----- FILE PATH: frontend/apps/workbench/src/main.jsx
----- CONTENT START -----
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.jsx'
import { ToastProvider, MaintenanceOverlay } from '@shared';
import '../../../../index.css'
ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <ToastProvider>
      <MaintenanceOverlay />
      <App />
    </ToastProvider>
  </React.StrictMode>,
)----- CONTENT END -----
----- FILE PATH: frontend/shared/package.json
----- CONTENT START -----
{
  "name": "@image-tagger/shared",
  "version": "1.0.0",
  "main": "src/index.js",
  "type": "module"
}----- CONTENT END -----
----- FILE PATH: frontend/shared/src/api-client.js
----- CONTENT START -----
/**
 * Centralized API Client.
 * Handles JWT injection and standardized error parsing.
 */
export class ApiClient {
  constructor(baseUrl = '/api', defaultHeaders = {}) {
    this.baseUrl = baseUrl;
    this.defaultHeaders = defaultHeaders;
    }

    async get(endpoint) {
        return this._request(endpoint, { method: 'GET' });
    }

    async post(endpoint, body) {
        return this._request(endpoint, {
            method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        ...this.defaultHeaders,
        ...(options.headers || {}),
      },
            body: JSON.stringify(body)
        });
    }

    async delete(endpoint) {
        return this._request(endpoint, { method: 'DELETE' });
    }

    async put(endpoint, body) {
        return this._request(endpoint, {
            method: 'PUT',
      headers: {
        'Content-Type': 'application/json',
        ...this.defaultHeaders,
        ...(options.headers || {}),
      },
            body: JSON.stringify(body)
        });
    }

    async _request(endpoint, options) {
        // Mock Auth Header - In production this comes from AuthContext
        const headers = { 
            'X-User-ID': '1', 
            ...options.headers 
        };
        
        try {
            const response = await fetch(`${this.baseUrl}${endpoint}`, { ...options, headers });
            
if (!response.ok) {
    let payload = {};
    try {
        payload = await response.json();
    } catch (e) {
        payload = {};
    }
    const message = payload.detail || payload.message || `API Request Failed: ${response.status}`;
    const error = new Error(message);
    error.status = response.status;
    if (response.status === 503) {
        // Hint to the UI that this is a maintenance / outage condition.
        error.isMaintenance = true;
        // Broadcast a global maintenance event so that top-level shells
        // (Explorer / Workbench / Admin / Monitor) can show a full-screen
        // overlay rather than a cryptic 'fetch failed' toast.
        if (typeof window !== 'undefined' && typeof window.dispatchEvent === 'function') {
            const detail = {
                status: response.status,
                endpoint: `${this.baseUrl}${endpoint}`,
                message,
            };
            window.dispatchEvent(new CustomEvent('image-tagger:maintenance', { detail }));
        }
    }
    throw error;
}
            
            // Handle 204 No Content
            if (response.status === 204) return null;

            return await response.json();
        } catch (err) {
            console.error(`API Error [${endpoint}]:`, err);
            throw err;
        }
    }
}----- CONTENT END -----
----- FILE PATH: frontend/shared/src/index.js
----- CONTENT START -----
export { ApiClient } from './api-client';
export { Button } from './components/Button';
export { Header } from './components/Header';
export { Toggle } from './components/Toggle';
export { ToastProvider, useToast } from './components/Toast';

export { MaintenanceOverlay } from './components/MaintenanceOverlay';
----- CONTENT END -----
----- FILE PATH: frontend/shared/src/components/Button.jsx
----- CONTENT START -----
import React from 'react';

export const Button = ({ children, onClick, variant = 'primary', disabled = false, className = '' }) => {
    const baseStyle = "px-4 py-2 rounded font-semibold transition-colors duration-200 disabled:opacity-50 flex items-center justify-center gap-2";
    const variants = {
        primary: "bg-blue-600 hover:bg-blue-700 text-white shadow-sm",
        secondary: "bg-white hover:bg-gray-50 text-gray-700 border border-gray-300 shadow-sm",
        danger: "bg-red-600 hover:bg-red-700 text-white shadow-sm",
        ghost: "bg-transparent hover:bg-gray-100 text-gray-600"
    };

    return (
        <button 
            onClick={onClick} 
            disabled={disabled}
            className={`${baseStyle} ${variants[variant]} ${className}`}
        >
            {children}
        </button>
    );
};----- CONTENT END -----
----- FILE PATH: frontend/shared/src/components/Header.jsx
----- CONTENT START -----
import React from 'react';
import { LayoutGrid } from 'lucide-react';

export const Header = ({ title, appName }) => {
    return (
        <header className="bg-enterprise-blue text-white p-4 shadow-md flex justify-between items-center z-50">
            <div className="flex items-center gap-3">
                <div className="w-8 h-8 bg-blue-600 rounded flex items-center justify-center font-bold shadow-inner">
                    <LayoutGrid size={18} />
                </div>
                <div>
                    <h1 className="text-lg font-bold leading-none tracking-tight">{appName}</h1>
                    <span className="text-xs text-gray-400 font-mono">v3.0 Enterprise</span>
                </div>
            </div>
            <div className="flex items-center gap-4">
                <div className="text-sm font-medium bg-surface-dark px-3 py-1 rounded border border-gray-700">
                    {title}
                </div>
                <div className="w-8 h-8 rounded-full bg-gray-600 border-2 border-gray-500"></div>
            </div>
        </header>
    );
};----- CONTENT END -----
----- FILE PATH: frontend/shared/src/components/MaintenanceOverlay.jsx
----- CONTENT START -----
import React, { useEffect, useState } from 'react';

/**
 * Global full-screen maintenance overlay.
 *
 * This component listens for the custom "image-tagger:maintenance"
 * event emitted by the shared ApiClient whenever a 503 response is
 * returned from the backend. When triggered, it displays a clear,
 * student-friendly explanation that the system is temporarily
 * unavailable (e.g. DB down, migrations running, or heavy science
 * batch in progress).
 *
 * We intentionally centralise this behaviour in @shared so that all
 * frontends (Explorer, Workbench, Admin, Monitor) get the same UX
 * without duplicating code.
 */
export function MaintenanceOverlay() {
  const [active, setActive] = useState(false);
  const [lastMessage, setLastMessage] = useState('');
  const [lastEndpoint, setLastEndpoint] = useState('');

  useEffect(() => {
    function handler(event) {
      const detail = event.detail || {};
      setActive(true);
      setLastMessage(detail.message || 'The Image Tagger backend is temporarily unavailable.');
      setLastEndpoint(detail.endpoint || '');
    }

    if (typeof window !== 'undefined') {
      window.addEventListener('image-tagger:maintenance', handler);
    }
    return () => {
      if (typeof window !== 'undefined') {
        window.removeEventListener('image-tagger:maintenance', handler);
      }
    };
  }, []);

  if (!active) return null;

  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black bg-opacity-60 backdrop-blur-sm">
      <div className="max-w-lg w-full mx-4 rounded-2xl bg-white shadow-2xl border border-red-100 p-6">
        <div className="flex items-start gap-3">
          <div className="mt-1 flex h-8 w-8 items-center justify-center rounded-full bg-red-50 border border-red-200">
            <span className="text-red-600 text-lg font-bold">!</span>
          </div>
          <div className="flex-1">
            <h2 className="text-lg font-semibold text-gray-900">
              Maintenance in progress
            </h2>
            <p className="mt-2 text-sm text-gray-700">
              One of the backend services is currently returning a{' '}
              <code className="px-1 py-0.5 text-xs bg-gray-100 rounded border border-gray-200">
                503 Service Unavailable
              </code>{' '}
              response. This usually means the database, science worker, or depth
              model is restarting or running a migration.
            </p>
            {lastEndpoint && (
              <p className="mt-2 text-xs text-gray-500 break-all">
                Last failing endpoint: <code>{lastEndpoint}</code>
              </p>
            )}
            {lastMessage && (
              <p className="mt-1 text-xs text-gray-500">
                Detail: {lastMessage}
              </p>
            )}
            <p className="mt-3 text-xs text-gray-600">
              You can safely leave this tab open. Once the backend is healthy
              again, try refreshing the page or re-running your last action.
            </p>
          </div>
        </div>
        <div className="mt-4 flex justify-end gap-3">
          <button
            type="button"
            onClick={() => setActive(false)}
            className="inline-flex items-center rounded-md border border-gray-300 bg-white px-3 py-1.5 text-xs font-medium text-gray-700 shadow-sm hover:bg-gray-50"
          >
            Dismiss
          </button>
        </div>
      </div>
    </div>
  );
}
----- CONTENT END -----
----- FILE PATH: frontend/shared/src/components/Toast.jsx
----- CONTENT START -----
import React, { createContext, useContext, useState, useCallback } from 'react';

/**
 * Simple global toast / notification system.
 *
 * Usage:
 *   import { ToastProvider, useToast } from '@shared';
 *
 *   // In main.jsx:
 *   ReactDOM.createRoot(...).render(
 *     <React.StrictMode>
 *       <ToastProvider>
 *         <App />
 *       </ToastProvider>
 *     </React.StrictMode>
 *   );
 *
 *   // In any child component:
 *   const toast = useToast();
 *   toast.success('Saved configuration.');
 *   toast.error('Upload failed.');
 */

const ToastContext = createContext(null);

export const ToastProvider = ({ children }) => {
  const [toasts, setToasts] = useState([]);

  const removeToast = useCallback((id) => {
    setToasts((prev) => prev.filter((t) => t.id !== id));
  }, []);

  const addToast = useCallback((toast) => {
    const id = Math.random().toString(36).slice(2);
    const duration = toast.duration ?? 4000;

    const next = {
      id,
      type: toast.type || 'info',
      title: toast.title || null,
      message: toast.message || '',
    };

    setToasts((prev) => [...prev, next]);

    if (duration > 0) {
      setTimeout(() => {
        removeToast(id);
      }, duration);
    }
  }, [removeToast]);

  const api = {
    show: (opts) => addToast(opts),
    success: (message, opts = {}) => addToast({ type: 'success', message, ...opts }),
    error: (message, opts = {}) => addToast({ type: 'error', message, ...opts }),
    info: (message, opts = {}) => addToast({ type: 'info', message, ...opts }),
  };

  return (
    <ToastContext.Provider value={api}>
      {children}
      <div className="fixed top-4 right-4 z-50 flex flex-col space-y-2 max-w-sm">
        {toasts.map((t) => (
          <div
            key={t.id}
            className={
              'rounded-lg shadow-lg px-4 py-3 text-sm text-white flex items-start justify-between gap-3 ' +
              (t.type === 'success'
                ? 'bg-emerald-600'
                : t.type === 'error'
                  ? 'bg-red-600'
                  : 'bg-slate-800')
            }
          >
            <div className="flex-1">
              {t.title && <div className="font-semibold mb-0.5">{t.title}</div>}
              <div className="text-xs whitespace-pre-line">{t.message}</div>
            </div>
            <button
              type="button"
              onClick={() => removeToast(t.id)}
              className="ml-2 text-xs opacity-75 hover:opacity-100 focus:outline-none"
            >
              √ó
            </button>
          </div>
        ))}
      </div>
    </ToastContext.Provider>
  );
};

export const useToast = () => {
  const ctx = useContext(ToastContext);
  if (!ctx) {
    throw new Error('useToast must be used within a ToastProvider');
  }
  return ctx;
};
----- CONTENT END -----
----- FILE PATH: frontend/shared/src/components/Toggle.jsx
----- CONTENT START -----
import React from 'react';

export const Toggle = ({ checked, onChange, label, danger = false }) => {
    return (
        <label className="flex items-center cursor-pointer">
            <div className="relative">
                <input 
                    type="checkbox" 
                    className="sr-only" 
                    checked={checked} 
                    onChange={(e) => onChange(e.target.checked)} 
                />
                <div className={`block w-12 h-7 rounded-full transition-colors ${
                    checked 
                        ? (danger ? 'bg-red-600' : 'bg-green-500') 
                        : 'bg-gray-300'
                }`}></div>
                <div className={`absolute left-1 top-1 bg-white w-5 h-5 rounded-full transition-transform shadow-sm ${
                    checked ? 'transform translate-x-5' : ''
                }`}></div>
            </div>
            {label && <span className="ml-3 text-sm font-medium text-gray-700">{label}</span>}
        </label>
    );
};----- CONTENT END -----
----- FILE PATH: governance/TODO_running_list.md
----- CONTENT START -----
# Running TODO / Sprint Backlog (Governance-Tracked)

This file is the canonical, running TODO list for Image Tagger.
It is maintained across sprints and only **appended to**, never pruned.
Each item is written so that a future maintainer (human or AI) can implement it
without needing the original conversation for context.

Items are grouped by subsystem. When a task is completed in a later version,
it should be **struck through** and annotated with the version number
(e.g., ~~Done in v3.4.27~~).

---

## 1. Tag Inspector Stack (Supervisor / Monitor)

1.1 **Inspector endpoint tests**
- **Goal:** Protect `/api/v1/monitor/image/{image_id}/inspector` from regressions.
- **What to implement:**
  - Pytest-based tests that:
    - Seed an image with a few `science_pipeline_*` `Validation` rows and a couple of human validations.
    - Call the inspector endpoint and assert the presence and shape of:
      - `image`, `pipeline`, `features`, `tags`, `bn`, `validations`.
    - Check that feature keys and validation keys include known seeded values.
    - Assert 404 behavior for non-existent `image_id`.
- **Hints:** Use the same factories/fixtures used by BN export tests so canon and inspector evolve together.

1.2 **Decouple inspector from private BN helpers**
- **Goal:** Avoid hard-coding inspector against `_collect_*` private helpers in `v1_bn_export`.
- **What to implement:**
  - Create a small helper module (e.g., `backend/science/bn_helpers.py`) that:
    - Exposes public functions:
      - `collect_indices_for_image(db, image_id, index_keys) -> Dict[str, float]`
      - `collect_bins_for_image(db, image_id) -> Dict[str, str]`
      - `compute_irr_for_image(db, image_id) -> Optional[float]`
    - Internally reuses the existing BN export logic.
  - Refactor both BN export endpoint(s) and inspector endpoint to call these helpers.
- **Hints:** Keep helper signatures minimal; avoid leaking Response objects or HTTP concerns into science helpers.

1.3 **Pipeline status fields**
- **Goal:** Make `pipeline.overall_status` and `pipeline.analyzers_run` in the inspector payload informative.
- **What to implement:**
  - Define a per-image pipeline run marker (e.g., a `PipelineRun` table or a status field attached to `Image`).
  - Populate it whenever the science pipeline runs (batch or single).
  - In the inspector endpoint, set:
    - `overall_status` to something like `"not_run" | "ok" | "partial" | "error"`.
    - `analyzers_run` to a list of analyzer IDs or names that completed.
- **Hints:** Even a coarse first pass (e.g., `"ok"` vs `"not_run"`) is useful and can be refined later.

1.4 **Rule-level explanations (first family)**
- **Goal:** Start surfacing ‚Äúwhy this psych tag or index fired‚Äù in the inspector payload.
- **What to implement:**
  - For at least one small family of indices (e.g., restorativeness and clutter), add:
    - A data structure describing which science attributes feed which index and via what thresholds/rules.
    - A helper that, given an image‚Äôs features, computes:
      - which rules fired,
      - their confidence, and
      - a short natural-language explanation string.
  - Extend the inspector payload with a `rules` block per index.
- **Hints:** Start small and explicit; this is about interpretability, not speed.

---

## 2. GUI Role Help & Onboarding

2.1 **Monitor dashboard help strip**
- **Goal:** Explain the Monitor dashboard as the ‚Äúcontrol tower‚Äù for supervisors.
- **What to implement:**
  - A small inline help strip (similar to Workbench and Explorer) that:
    - States that this view is for supervisors / PIs.
    - Explains what the key metrics mean (queue length, throughput, IRR, error counts).
    - Suggests when to drill into Tag Inspector vs escalate pipeline issues.
- **Hints:** Reuse the HelpCircle pattern and keep text under ~5 bullet points.

2.2 **Admin: provider / API configuration help**
- **Goal:** Avoid accidental misconfiguration of OpenAI/Anthropic/Gemini providers.
- **What to implement:**
  - Inline help near provider/API config that:
    - Clarifies that this is a sensitive, system-wide configuration surface.
    - Explains the recommended defaults and how to run a smoke test after changing keys/models.
    - Notes that VLM/materials paths may be disabled or degraded if configuration is incomplete.
- **Hints:** Include a short ‚ÄúAfter editing, run: [X]‚Äù note pointing to a concrete test (e.g., small VLM micro-loop).

2.3 **Admin: schema / index catalog help**
- **Goal:** Make it explicit that index/catalog edits affect BN inputs and inspector views.
- **What to implement:**
  - A help strip or tooltip cluster near index/catalog editing UI that:
    - States that index definitions determine which features are exported to BN and how Tag Inspector labels indices.
    - Encourages small, versioned changes (e.g., canon v1.0, v1.1).
    - Suggests review by at least one other scientist before changing canonical indices.
- **Hints:** Consider a ‚Äúproposed vs active‚Äù index state to allow staged rollouts.

2.4 **BN export screen help**
- **Goal:** Help users understand what they are exporting and how to use it downstream.
- **What to implement:**
  - A help strip on the BN export / dataset export page that:
    - Explains that each row is an image/case.
    - Describes what columns correspond to (indices, features, tags, provenance).
    - Notes how to use the export for:
      - BN learning,
      - regression models,
      - cross-cultural comparisons.
- **Hints:** Point to a small example notebook path (if present) that demonstrates loading the export.

2.5 **Pipeline health / debug panel help**
- **Goal:** Explain how to interpret the `/v1/debug/pipeline_health` UI.
- **What to implement:**
  - Brief inline help describing:
    - What each tier means (e.g., L0 color, L1 texture, L2 fractal, etc.).
    - How to interpret pass/fail/partial indicators.
    - When a supervisor should escalate to an engineer.
- **Hints:** Use conservative language; this is a diagnostic panel, not a guarantee of scientific validity.

---

## 3. Scientific Validation & Norming

3.1 **Minimal validation harness**
- **Goal:** Establish a concrete path from computed features to human ratings/behavior.
- **What to implement:**
  - A small, well-documented script or module that:
    - Takes a curated set of images with human ratings for a few target constructs (e.g., restorativeness, stress, coziness).
    - Computes correlations or simple regressions between tiered features/indices and those ratings.
    - Writes out a report (e.g., CSV/Markdown) with effect sizes and directions.
- **Hints:** Start with a single scale (e.g., restoration) and one or two feature families to avoid overfitting.

3.2 **Population baselines by room type / culture**
- **Goal:** Support Goldilocks-style reasoning (mid-level optimality) and cultural modulation.
- **What to implement:**
  - A mechanism to:
    - Group images by room type (e.g., bedroom, office, caf√©) and culture (e.g., India vs US).
    - Compute baseline distributions for key features (e.g., fractal metrics, clutter entropy, color saturation).
    - Store these baselines in a way that BN rules and Tag Inspector can reference.
- **Hints:** Treat baselines as versioned, with clear sample sizes, so future data can refine them without breaking existing analyses.

---

## 4. Governance & Integrity Extensions

4.1 **Syntax and AST guards**
- **Goal:** Ensure no syntactically broken Python modules ship in critical paths.
- **What to implement:**
  - A guard script (or extension of `program_integrity_guard.py`) that:
    - Walks all non-archive `.py` files in `backend/`, `scripts/`, and `tests/`.
    - Tries to `ast.parse` each file and reports failures.
  - CI and `install.sh` should fail if any AST parse fails.
- **Hints:** Respect existing exclude lists for archived/experimental code, but keep them minimal and documented.

4.2 **Inspector tests integrated into CI**
- **Goal:** Make the Tag Inspector a first-class citizen in governance.
- **What to implement:**
  - Ensure the inspector tests from Section 1.1 run under the main test suite.
  - Add a short note in `governance/README.md` (or equivalent) stating that Tag Inspector is covered by governance and must remain green for a GO release.
- **Hints:** Tag these tests with a marker (e.g., `@pytest.mark.governance`) if you need to separate quick vs full test runs.

---

## Status updates ‚Äî v3.4.27_priority_help

The following high-priority GUI help items now have concrete implementations:

- **2.1 Monitor dashboard help strip** ‚Äî Implemented in `frontend/apps/monitor/src/App.jsx` as an inline "How to use the Supervisor dashboard" box beneath the controls/status row.
- **2.2 Admin: provider / API configuration help** ‚Äî Implemented in `frontend/apps/admin/src/App.jsx` inside `VLMConfigPanel` as a "VLM configuration: handle with care" help strip under the VLM Engine header.
- **2.4 BN export / training export screen help** ‚Äî Implemented in `frontend/apps/admin/src/App.jsx` inside the Training Export card as a "What this export is for" help strip explaining downstream BN/regression/ML use.

Still pending (no dedicated UI surface yet or requires new panel):

- **2.3 Admin: schema / index catalog help** ‚Äî Will be added once a schema/index management UI is exposed.
- **2.5 Pipeline health / debug panel help** ‚Äî Will be added when the `/v1/debug/pipeline_health` results are surfaced in a dedicated debug/monitoring view.

---

## Status updates ‚Äî v3.4.28_pipeline_health_view

- **2.5 Pipeline health / debug panel help** ‚Äî Implemented as a "Science pipeline health" section in
  `frontend/apps/monitor/src/App.jsx`, backed by `/api/v1/debug/pipeline_health`. The panel:
    * Loads on initial dashboard fetch (via `loadData`).
    * Shows import status, OpenCV availability, analyzers by tier (with requires/provides),
      and any warnings/analyzer_errors.
    * Includes inline guidance explaining that this is a contracts/instantiation check rather than
      a full science run, and that supervisors should escalate persistent failures to engineering.

## 4.3 Pipeline health endpoint tests
- **Goal:** Ensure `/api/v1/debug/pipeline_health` has a stable, well-tested shape and fails loudly if its contract changes.
- **What to implement:**
  - Add a pytest module (e.g., `tests/test_pipeline_health_debug.py`) that:
    - Calls the endpoint via the FastAPI test client.
    - Asserts the presence and types of top-level fields: `import_ok` (bool), `cv2_available` (bool),
      `analyzers_by_tier` (mapping), and optional lists `warnings` and `analyzer_errors`.
    - Optionally validates that at least one analyzer is present in `analyzers_by_tier` in a seeded test environment.
  - Keep tests tolerant to minor content changes (e.g., different analyzer names) but strict about overall shape.
- **Hints:** Mirror the patterns used in `test_bn_export_smoke.py` and other existing tests that hit API endpoints.

## 4.4 Governance README note for pipeline health
- **Goal:** Make the pipeline health view part of the explicit governance surface for GO/NO-GO releases.
- **What to implement:**
  - Update `governance/README.md` (or create it if missing) to:
    - Describe the role of `/api/v1/debug/pipeline_health` and the Monitor's "Science pipeline health" panel.
    - State that a GO release requires this endpoint and panel to be functioning (no errors, sensible content).
    - Reference the associated tests (Section 4.3) and indicate that they must pass in CI.
- **Hints:** Keep this note short and operational, framed as an acceptance criterion that PIs/supervisors can understand.

---

## Status updates ‚Äî v3.4.30_pipeline_health_tests

- **4.3 Pipeline health endpoint tests** ‚Äî Implemented as `tests/test_pipeline_health_debug.py`,
  which calls `/api/v1/debug/pipeline_health` via FastAPI's TestClient and asserts:
    * Response status code is 200.
    * Top-level fields `import_ok`, `cv2_available`, and `analyzers_by_tier` are present and of the
      expected types (bool, bool, dict).
    * Optional fields `warnings` and `analyzer_errors` (if present) are lists.
    * Each analyzer entry (if present) includes `name`, `tier`, `requires`, and `provides` keys.

- **4.4 Governance README note for pipeline health** ‚Äî Still pending; will be implemented as a short
  acceptance-criteria section in `governance/README.md` that ties the Monitor pipeline health panel and
  `/api/v1/debug/pipeline_health` tests to GO/NO-GO decisions.

---

## Status updates ‚Äî v3.4.31_bn_canon_sanity

- Added `tests/test_bn_canon_sanity.py` to exercise the BN-facing canon:
  * `test_index_catalog_candidate_entries_are_well_formed` checks that each
    candidate BN index in `index_catalog` has a label, description, type, and
    bins spec with non-empty `field` and `values` (and that 3-level bins use
    `{low, mid, high}`).
  * `test_bn_export_respects_index_catalog_canon` seeds synthetic Validation rows
    for all candidate indices and their bin fields, calls `export_bn_snapshot`,
    and asserts:
      - `BNRow.indices.keys()` == the candidate index key set and all values are non-None.
      - All expected bin fields appear in `BNRow.bins`, with labels drawn from
        `{low, mid, high}` where applicable.

---

## Status updates ‚Äî v3.4.32_restorativeness_H1

- Added `_build_restorativeness_heuristic_node` to `backend/api/v1_supervision.py` and wired it
  into the Tag Inspector (`/v1/monitor/image/{image_id}/inspector`):
    * The helper reads CNfA fluency + biophilia features from `features` (notably
      `cnfa.biophilic.natural_material_ratio`, `cnfa.fluency.visual_entropy_spatial`,
      `cnfa.fluency.clutter_density_count`, and `cnfa.fluency.processing_load_proxy`).
    * It computes a simple, explicitly *heuristic* restorativeness score in [0, 1] and maps
      it into a coarse 3-bin label {low, mid, high}.
    * It then appends:
        - a BN-like node dict to `bn.nodes` with `name="affect.restorative_h1"` and a
          one-hot posterior over the chosen bin; and
        - a derived tag to `tags` with `status="derived"`, `key="affect.restorative_h1"`,
          `raw_value` equal to the numeric score, and `bin` equal to the 3-level label.
    * If fewer than two of the required features are available, the helper returns no node/tag,
      so the heuristic is fail-safe and purely additive.
- This is the first explicit "restorativeness" rule family (H1) in Tag Inspector and is
  intended as a scaffold for later calibration against human rating datasets and cultural
  baselines.

---

## Status updates ‚Äî v3.4.33_admin_killswitch_vlm_depth_fix

- **Admin kill switch build break (Gemini ruthless panel)** ‚Äî Fixed in `frontend/apps/admin/src/App.jsx` by rewriting `handleKillSwitch` so it:
  - Calls `/api/v1/admin/kill-switch?active=<bool>` with a clear busy/error state.
  - Updates both `budget` and `killSwitchActive` from the returned `BudgetStatus`.
  - Handles errors via `console.error` and a guarded `setError(...)`.
  - Always clears `killBusy` in a `finally` block.
- **Admin bulk upload endpoint (`/upload`)** ‚Äî Restored the clean async implementation in `backend/api/v1_admin.py` from the archived v3.4.13 snapshot and adapted it so:
  - The DB `Image.storage_path` stores the relative `unique_name`.
  - `storage_paths` returned to the Admin UI still use full filesystem paths for operator feedback.
  - The endpoint is `async` with `await f.read()` inside the function body (removing the prior `await`-outside-async SyntaxError).
- **Depth analyzer scientific clarification** ‚Äî Updated `backend/science/spatial/depth.py`:
  - Module docstring now clearly states this module provides 2D edge/clutter heuristics as *proxies* for depth-like qualities (openness, refuge, isovist area).
  - `DepthAnalyzer` class docstring now marks the outputs as heuristic indicators, pending integration of true monocular depth maps.
- **VLM JSON robustness** ‚Äî Introduced `_safe_json_loads` in `backend/services/vlm.py` and wired it into:
  - `OpenAIEngine.analyze_image`
  - `GeminiEngine.analyze_image`
  The helper:
  - Strips Markdown fences (```json / ```).
  - Attempts `json.loads` on the cleaned string.
  - On failure, extracts the first `{...}` span before re-raising a `JSONDecodeError`.
  This covers the most common ‚ÄúJSON wrapped in commentary/markdown‚Äù cases without doing unsafe free-form repair.
- **Python syntax health (active code)** ‚Äî All active (non-`archive/`) `.py` files pass an AST/syntax check after these changes. Archive trees remain untouched as historical snapshots.

---

## Status updates ‚Äî v3.4.34_G1_admin_killswitch_vlm_tests

- **Admin kill-switch tests** ‚Äî Added `tests/test_admin_killswitch.py` to:
  - Seed a paid `ToolConfig` if none exist.
  - Exercise `/api/v1/admin/budget` and assert it returns a boolean `is_kill_switched`.
  - Call `/api/v1/admin/kill-switch?active=true` as an admin and assert:
    - The response marks `is_kill_switched == True`.
    - No paid `ToolConfig` entries remain enabled in the database.
- **VLM JSON robustness tests** ‚Äî Added `tests/test_vlm_safe_json_loads.py` to unit-test `_safe_json_loads`:
  - Plain JSON object.
  - ```json fenced blocks.
  - Generic ``` fenced blocks.
  - JSON with leading/trailing commentary.
  - Ensures a real `json.JSONDecodeError` is raised on non-JSON garbage.
- These tests codify the Gemini ruthless panel‚Äôs concerns about the Admin kill-switch build path and VLM JSON parsing, turning those into stable, regresssion-resistant contracts.


---

## Status updates ‚Äî v3.4.35_science_bn_restH1_closure

- **BN canon + export sanities** ‚Äî Confirmed that:
  - `tests/test_bn_canon_sanity.py` enforces that all `candidate_bn_input` indices in
    `backend/science/index_catalog.py` have labels, descriptions, typed bins, and that
    `export_bn_snapshot` exposes exactly those indices and bin fields for a seeded image.
  - `tests/test_bn_export_smoke.py` exercises a full BN snapshot round-trip, asserting
    that a synthetic continuous index value and its corresponding bin field map to the
    expected `low`/`mid`/`high` label.
- **Restorativeness H1 integration** ‚Äî The H1 restorativeness node (`affect.restorative_h1`)
  is now part of the Tag Inspector BN payload, and
  `tests/test_monitor_tag_inspector.py::test_restorativeness_h1_appears_in_tag_inspector`
  covers the end-to-end path from synthetic science-like `Validation` rows to:
  - a BN node with posterior `{"high": 1.0}`, and
  - a derived Tag Inspector tag with `bin == "high"`.
- Together with the governance and syntax guards, this closes the initial science/BN sprint:
  BN inputs are canon-checked and restorativeness H1 is a stable, regression-tested contract.
----- CONTENT END -----
----- FILE PATH: infra/cloud/full_stack_vm_setup.sh
----- CONTENT START -----
#!/bin/bash
# üöÄ Anti-Gravity Setup for Full Stack VM (Track A)
# Tested on Ubuntu 22.04 LTS
#
# This script is intended for instructors / TAs who want to run the
# full Image Tagger stack on a cloud VM or lab server with minimal
# manual setup.
#
# Usage:
#   1. Copy the Image_Tagger_v3.4.71_anti_gravity_cloud_full.zip artifact
#      to the VM (or clone the repo).
#   2. Run:
#         chmod +x infra/cloud/full_stack_vm_setup.sh
#         ./infra/cloud/full_stack_vm_setup.sh
#
#   3. Follow the printed instructions to access the UI.
#
set -e

echo "[Setup] üîÑ Updating system..."
sudo apt-get update -y
sudo apt-get install -y unzip curl git

# 1. Install Docker if not present
if ! command -v docker &> /dev/null; then
    echo "[Setup] üê≥ Installing Docker..."
    curl -fsSL https://get.docker.com -o get-docker.sh
    sudo sh get-docker.sh
    sudo usermod -aG docker "$USER" || true
    echo "[Setup] ‚ö†Ô∏è  You may need to log out and back in for Docker permissions to take effect."
else:
    echo "[Setup] ‚úÖ Docker already installed."
fi

# 2. Unpack repo if a ZIP is present in the current directory
if [ -f "Image_Tagger_v3.4.71_anti_gravity_cloud_full.zip" ]; then
    echo "[Setup] üì¶ Unzipping artifact..."
    rm -rf image_tagger
    mkdir -p image_tagger
    unzip -o Image_Tagger_v3.4.71_anti_gravity_cloud_full.zip -d image_tagger
    cd image_tagger
else
    echo "[Setup] ‚ÑπÔ∏è  Zip not found. Assuming we are already in the repo root."
fi

# 3. Run auto installer
if [ -x "./auto_install.sh" ]; then
    echo "[Setup] üèóÔ∏è  Running auto_install.sh..."
    ./auto_install.sh
else
    echo "[Setup] ‚ùå auto_install.sh not found or not executable."
    echo "       Please ensure you are in the Image Tagger repo root."
    exit 1
fi

echo "----------------------------------------------------------------"
echo "‚úÖ FULL STACK SHOULD NOW BE RUNNING (or ready to start via docker-compose)."
echo ""
echo "Typical ports:"
echo "   - Backend API: http://localhost:8000"
echo "   - Frontend UI: http://localhost:8080 (or as defined in docker-compose)"
echo ""
echo "‚òÅÔ∏è  To expose this to the public internet for demos, you can use ngrok:"
echo "   1. Install:  snap install ngrok   (or follow ngrok's official instructions)"
echo "   2. Run:      ngrok http 8080"
echo ""
echo "   ngrok will print a public URL like https://xxxx-8080.ngrok-free.app"
echo "   which you can share with students for short-lived demos."
echo "----------------------------------------------------------------"
----- CONTENT END -----
----- FILE PATH: infra/turnkey_installer_v1.3/installer_config.json
----- CONTENT START -----
{
  "prechecks": [
    "python --version",
    "docker --version || echo 'docker not found (ok for CI)'"
  ],
  "steps": [
    {
      "name": "Image Tagger native installer",
      "shell": "bash install.sh"
    }
  ]
}----- CONTENT END -----
----- FILE PATH: infra/turnkey_installer_v1.3/installer_config.sample.json
----- CONTENT START -----
{
  "prechecks": [
    "python --version"
  ],
  "steps": [
    {
      "name": "Sample echo",
      "shell": "echo sample step ran"
    }
  ]
}----- CONTENT END -----
----- FILE PATH: infra/turnkey_installer_v1.3/hooks/post_install_smoke.py
----- CONTENT START -----

#!/usr/bin/env python3
# Placeholder smoke: checks for README.md and prints size.
import pathlib, sys, traceback
LOG = pathlib.Path('logs')/ 'smoke_http.log'
LOG.parent.mkdir(parents=True, exist_ok=True)
try:
    readme = pathlib.Path('README.md')
    msg = f"README.md exists={readme.exists()} size={(readme.stat().st_size if readme.exists() else 0)}"
    LOG.write_text(msg, encoding='utf-8')
    print(msg)
except Exception as e:
    LOG.write_text('SMOKE ERROR: ' + ''.join(traceback.format_exception(e)), encoding='utf-8')
    print(f"SMOKE ERROR: {e}")
    sys.exit(0)
----- CONTENT END -----
----- FILE PATH: infra/turnkey_installer_v1.3/installer/install.sh
----- CONTENT START -----
#!/usr/bin/env bash
set -euo pipefail
LOG_DIR="logs"; mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/install.log"
echo "[installer] $(date) starting" | tee -a "$LOG_FILE"

# 1) Best-effort Python toolchain
echo "[installer] upgrading pip/setuptools/wheel" | tee -a "$LOG_FILE"
python -m pip install --upgrade pip setuptools wheel >> "$LOG_FILE" 2>&1 || true

# 2) Root requirements.txt (optional, non-fatal)
if [ -f "requirements.txt" ]; then
  echo "[installer] installing requirements.txt" | tee -a "$LOG_FILE"
  python -m pip install -r requirements.txt >> "$LOG_FILE" 2>&1 || true
fi

# 3) Installer config JSON (if present)
CFG_PATH="infra/turnkey_installer_v1.3/installer_config.json"
if [ -f "$CFG_PATH" ]; then
  echo "[installer] applying config from $CFG_PATH" | tee -a "$LOG_FILE"
  python - << 'PY'
import json, subprocess
from pathlib import Path

cfg_path = Path("infra/turnkey_installer_v1.3/installer_config.json")
if cfg_path.exists():
    print("[installer-config] loading", cfg_path)
    cfg = json.loads(cfg_path.read_text())
    for cmd in cfg.get("prechecks", []):
        print(f"[installer-config] PRECHECK: {cmd}")
        subprocess.run(cmd, shell=True, check=False)
    for step in cfg.get("steps", []):
        shell = step.get("shell")
        if not shell:
            continue
        name = step.get("name", shell)
        print(f"[installer-config] STEP: {name}")
        subprocess.run(shell, shell=True, check=False)
PY
fi

echo "[installer] SUCCESS" | tee -a "$LOG_FILE"

# 4) Post-install smoke (non-fatal)
if [ -f "infra/turnkey_installer_v1.3/hooks/post_install_smoke.py" ]; then
  echo "[installer] post-install smoke" | tee -a "$LOG_FILE"
  python infra/turnkey_installer_v1.3/hooks/post_install_smoke.py 2>&1 | tee -a "$LOG_FILE" || true
fi
----- CONTENT END -----
----- FILE PATH: notebooks/VLM_Health_Lab.ipynb
----- CONTENT START -----
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\uddea VLM Health Lab (Colab) \u2013 Image Tagger 3.4.74_vlm_lab_notebook_TL_runbook\n",
        "\n",
        "This notebook gives you an **anti-gravity, ephemeral lab** for the Image Tagger project.\n",
        "\n",
        "It is designed to:\n",
        "- unpack a copy of the Image Tagger repo,\n",
        "- set up a small database and a tiny synthetic image set,\n",
        "- run the **science pipeline** in stub mode (no real API keys needed),\n",
        "- and then run the **VLM Health** variance audit on the results.\n",
        "\n",
        "## Benefits vs Costs\n",
        "\n",
        "**Benefits (TRACK B \u2013 Science Notebook):**\n",
        "- Works on most machines with a browser and a Google account.\n",
        "- No Docker, no local Postgres install.\n",
        "- Good for *short, self-contained experiments* and demos.\n",
        "- Lets you inspect how the VLM health scripts behave on a toy dataset.\n",
        "\n",
        "**Costs / tradeoffs:**\n",
        "- The environment is **ephemeral**:\n",
        "  - when the Colab runtime disconnects or you close the tab,\n",
        "    everything under `/content` is **lost**.\n",
        "  - only files you explicitly **save to Google Drive** or **download**\n",
        "    will persist.\n",
        "- Performance is limited; this is not for large-scale runs.\n",
        "- If the repo layout changes, you may need to adjust a path or two.\n",
        "\n",
        "## Relationship to the Full App (TRACK A)\n",
        "\n",
        "- The **Full App (TRACK A)** runs via Docker (locally, Codespaces, or a VM).\n",
        "  - It is **persistent**: data and settings survive restarts.\n",
        "  - It is ideal for multi-week projects and full tagging workflows\n",
        "    with Workbench and Explorer.\n",
        "- This notebook is **ephemeral** and **minimal**:\n",
        "  - it focuses on the **science + VLM health** parts only.\n",
        "  - use it when students cannot run Docker or when you want\n",
        "    a light-weight lab exercise.\n",
        "\n",
        "Before using this notebook in class, instructors and TAs should read:\n",
        "\n",
        "- `docs/ops/Cloud_AntiGravity_Quickstart.md`\n",
        "- `docs/ops/Student_Quickstart_v3.4.73.md`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_env"
      },
      "source": [
        "# @title \ud83d\udce6 Step 1: Setup \"Anti-Gravity\" Environment\n",
        "# @markdown This cell installs Python dependencies and a lightweight PostgreSQL database\n",
        "# @markdown directly in this notebook. **Run it once at the start.**\n",
        "import os\n",
        "\n",
        "print(\"\u2b07\ufe0f Installing Python libraries...\")\n",
        "!pip install -q fastapi uvicorn sqlalchemy psycopg2-binary pydantic pydantic-settings\n",
        "!pip install -q openai anthropic pandas numpy scipy scikit-image requests opencv-python-headless\n",
        "\n",
        "print(\"\ud83d\udc18 Installing PostgreSQL (this may take a minute)...\")\n",
        "!sudo apt-get -y -qq update\n",
        "!sudo apt-get -y -qq install postgresql\n",
        "\n",
        "print(\"\ud83d\udd27 Starting PostgreSQL service...\")\n",
        "!sudo service postgresql start\n",
        "\n",
        "print(\"\ud83d\udee0\ufe0f Configuring database user and schema...\")\n",
        "!sudo -u postgres psql -c \"CREATE USER tagger WITH PASSWORD 'tagger_pass';\" || echo \"User may already exist.\"\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE image_tagger_v3 OWNER tagger;\" || echo \"DB may already exist.\"\n",
        "\n",
        "os.environ['DATABASE_URL'] = \"postgresql://tagger:tagger_pass@localhost:5432/image_tagger_v3\"\n",
        "os.environ['IMAGE_STORAGE_ROOT'] = \"/content/data_store\"\n",
        "\n",
        "print(\"\u2705 Environment Ready. If you see errors above, read them carefully before continuing.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upload_zip"
      },
      "source": [
        "# @title \ud83d\udcc2 Step 2: Upload the Image Tagger ZIP\n",
        "# @markdown 1. Run this cell.\n",
        "# @markdown 2. Click **\"Choose Files\"** when prompted.\n",
        "# @markdown 3. Upload the Image Tagger repo zip your instructor gave you\n",
        "# @markdown    (for example: `Image_Tagger_3.4.74_vlm_lab_TL_runbook_full.zip` or later).\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No file uploaded. Please upload the Image Tagger zip.\")\n",
        "filename = next(iter(uploaded))\n",
        "print(f\"\ud83d\udce6 Unpacking {filename}...\")\n",
        "\n",
        "os.makedirs(\"/content/repo\", exist_ok=True)\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/repo\")\n",
        "\n",
        "os.chdir(\"/content/repo\")\n",
        "print(\"\u2705 Repo unpacked in /content/repo and set as current directory.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seed_db"
      },
      "source": [
        "# @title \ud83c\udf31 Step 3: Seed Database & Generate Toy Images\n",
        "# @markdown This step:\n",
        "# @markdown - creates database tables,\n",
        "# @markdown - seeds basic configuration (if seed scripts are present),\n",
        "# @markdown - generates a few synthetic \"architectural\" images.\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure Python can find backend modules\n",
        "sys.path.append(\"/content/repo\")\n",
        "\n",
        "print(\"\ud83d\udd04 Creating database tables...\")\n",
        "from backend.database.core import engine, Base, SessionLocal\n",
        "from backend.models import *  # noqa: F401,F403\n",
        "Base.metadata.create_all(bind=engine)\n",
        "\n",
        "print(\"\ud83c\udf31 Seeding configs & attributes (if seed scripts are present)...\")\n",
        "try:\n",
        "    !python3 backend/scripts/seed_tool_configs.py\n",
        "except Exception as e:\n",
        "    print(\"   -> seed_tool_configs.py not found or failed:\", e)\n",
        "try:\n",
        "    !python3 backend/scripts/seed_attributes.py\n",
        "except Exception as e:\n",
        "    print(\"   -> seed_attributes.py not found or failed:\", e)\n",
        "\n",
        "print(\"\ud83d\uddbc\ufe0f Generating synthetic images...\")\n",
        "data_store = Path(os.environ['IMAGE_STORAGE_ROOT'])\n",
        "data_store.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "from backend.models.assets import Image\n",
        "\n",
        "with SessionLocal() as db:\n",
        "    for i in range(1, 6):\n",
        "        img = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)\n",
        "        # Draw a central \"building\" rectangle\n",
        "        cv2.rectangle(img, (100, 100), (400, 400), (200, 200, 200), -1)\n",
        "        p = data_store / f\"toy_arch_{i}.jpg\"\n",
        "        cv2.imwrite(str(p), img)\n",
        "\n",
        "        db_img = Image(filename=p.name, storage_path=str(p))\n",
        "        db.add(db_img)\n",
        "        db.commit()\n",
        "        print(f\"   -> Created {p.name} (ID: {db_img.id})\")\n",
        "\n",
        "print(\"\u2705 Toy data ready.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_science"
      },
      "source": [
        "# @title \ud83d\udd2c Step 4: Run Science Pipeline (Stub VLM)\n",
        "# @markdown This runs the Image Tagger science pipeline on the toy images.\n",
        "# @markdown If no real VLM keys are set, it should fall back to a stub/neutral engine,\n",
        "# @markdown which is fine for testing the *plumbing*.\n",
        "import asyncio\n",
        "from backend.database.core import SessionLocal\n",
        "from backend.models.assets import Image\n",
        "from backend.science.pipeline import SciencePipeline\n",
        "\n",
        "print(\"\ud83e\uddea Running science pipeline on toy images...\")\n",
        "\n",
        "async def run_pipeline():\n",
        "    db = SessionLocal()\n",
        "    try:\n",
        "        pipeline = SciencePipeline(db)\n",
        "        images = db.query(Image).all()\n",
        "        for img in images:\n",
        "            print(f\"   Analyzing Image {img.id} ({img.filename})...\")\n",
        "            await pipeline.process_image(img.id)\n",
        "    finally:\n",
        "        db.close()\n",
        "\n",
        "await run_pipeline()\n",
        "print(\"\u2705 Science pipeline complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_audit"
      },
      "source": [
        "# @title \ud83d\udcca Step 5: Run VLM Health Variance Audit\n",
        "# @markdown This uses the `scripts/audit_vlm_variance.py` helper script from the repo\n",
        "# @markdown to compute simple variance statistics over VLM outputs.\n",
        "# @markdown\n",
        "# @markdown **Note:** The exact output paths may differ slightly by version.\n",
        "# @markdown If no CSV appears, inspect the script output and adjust the glob pattern.\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\ud83d\udcc9 Running VLM variance audit...\")\n",
        "!python3 scripts/audit_vlm_variance.py\n",
        "\n",
        "candidates = []\n",
        "candidates.extend(glob.glob(\"reports/vlm_health/*/vlm_variance_audit.csv\"))\n",
        "candidates.extend(glob.glob(\"reports/vlm_health/*/variance_audit.csv\"))\n",
        "\n",
        "if not candidates:\n",
        "    print(\"\u274c No variance audit CSV found.\")\n",
        "    print(\"   Please check the output of audit_vlm_variance.py above and adjust the path if needed.\")\n",
        "else:\n",
        "    path = sorted(candidates)[-1]\n",
        "    print(f\"\ud83d\udcc4 Found variance audit CSV: {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    display(df.head())\n",
        "    print(f\"Rows: {len(df)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}----- CONTENT END -----
----- FILE PATH: reports/Ruthless_3.4.35_five_panel_summary.md
----- CONTENT START -----
# Ruthless Audit Summary ‚Äì v3.4.35 (Five-Expert Panel)

**Panel Verdict:** CONDITIONAL GO  
**Severity:** Passed, with **high-priority warnings**.

The five-expert panel reviewed `Image_Tagger_v3.4.35_science_bn_restH1_closure_full`
and concluded that the system is buildable and runnable, but requires
targeted follow-up work before being treated as a fully hardened,
classroom-ready instrument.

## Key Findings

- **Build / Syntax:**  
  Previous syntax corruption in the Admin frontend (e.g., the
  `handleKillSwitch` / `handleTrainingExport` mix-up in `App.jsx`) has
  been resolved. The Admin app now compiles, clearing the critical
  build blocker.

- **Backend integrity:**  
  Core API routes and the science pipeline are coherent, with CI /
  smoke tests able to reach the main workflows.

- **Upload security gap:**  
  The Admin bulk upload endpoint was identified as under-specified:
  extensions were only lightly filtered, there were no explicit size
  bounds, and error handling for mixed batches was ambiguous. The panel
  requested a hardened, well-documented upload pipeline.

- **Scientific naming hygiene:**  
  BN and restorativeness (H1) configuration files show inconsistent
  naming conventions. The panel recommended:
  - enforcing a clearer naming style for nodes,
  - providing a human-readable glossary,
  - and wiring a lightweight naming guard into GO checks.

## Recommended Follow-Ups (addressed in v3.4.36)

1. Harden the Admin upload endpoint with:
   - explicit allowed suffixes,
   - per-file size limits,
   - clearer error messages for invalid uploads.
2. Introduce a BN naming guard and a short BN naming guide in `docs/`.
3. Record a concise summary of this 3.4.35 panel in `reports/` and wire
   it into sprint planning for the next version.

This summary file is intentionally short; the full AI-generated report
lives in the conversation history and can be regenerated with the
Ruthless prompt suite if needed.
----- CONTENT END -----
----- FILE PATH: reports/Ruthless_3.4.50_upload_orchestrator_cost_view.md
----- CONTENT START -----
# Ruthless Audit ‚Äî Image Tagger v3.4.50 (Upload Orchestrator + Cost View)

Target build: **Image_Tagger_v3.4.50_upload_orchestrator_cost_view_full.zip**  
Scope: backend, admin frontend, governance, science pipeline as exercised by the new upload job orchestrator and cost ledger.

## 1. Executive Verdict (GO / NO-GO)

**Verdict: GO (with high-priority workflow warnings)**

*The system is buildable, installable, and scientifically usable. The new upload orchestrator and cost ledger are wired correctly and do not introduce rot. However, the upload job system is "dark" (no dedicated Admin UI panel yet), and background execution is limited to FastAPI's in-process `BackgroundTasks`.*

- **Installability:** `install.sh` + governance guards still enforce hollow-repo, syntax, critical-import, and canon checks.
- **Science:** Image-level math metrics and cognitive VLM analysis are still functional and now log costs; arch-pattern VLM also logs costs.
- **New features:** Uploads now create `UploadJob` + `UploadJobItem` rows and enqueue a background science run; cost ledger exposes a daily time-series; Admin shows a cost history chart.
- **Workflow gap:** There is no first-class UI to monitor upload jobs; admins must call `/admin/upload/jobs` manually.

## 2. Kill List (Blockers)

No **hard** technical blockers for classroom / lab usage were found. The following are **soft blockers** for large-scale, multi-user deployments:

- **K1 ‚Äî Orchestrator observability gap (soft blocker):**  
  - There is no dedicated Admin panel to monitor upload jobs, even though the backend exposes `/admin/upload/jobs` and `/admin/upload/jobs/{job_id}`.
  - Risk: Admins cannot easily see whether a big batch is still running, has failed, or is completed without manual API calls.
  - Impact: For large cohorts, this undermines trust in the orchestrator and makes triage slow.

- **K2 ‚Äî BackgroundTasks as the only worker (soft blocker):**  
  - `run_upload_job(job_id)` is only wired from FastAPI `BackgroundTasks` in `upload_images`.
  - If the app process is restarted after upload but before the background task completes, the job may remain `RUNNING` or `PENDING` without a retry mechanism.
  - Impact: For long-running, large jobs this is fragile; for classroom-scale jobs it is acceptable.

These do **not** prevent GO for lab and teaching deployments but should be addressed before "industrial" multi-process deployment.

## 3. Architecture & Code Health

**Status: Solid, no rot**

- **Job models:** `UploadJob` and `UploadJobItem` are canonical SQLAlchemy models with timestamps, status fields, and FK links to `users` and `images`. No ellipses, no `# STUB:` markers.
- **Service layer:** `backend/services/upload_jobs.py` cleanly encapsulates:
  - Job creation (`create_upload_job_for_images`).
  - A reusable `_run_upload_job_inner(session, job_id)` that can be reused by real workers.
  - A `run_upload_job(job_id)` entry point holding its own `SessionLocal`.
- **Science pipeline:** Orchestrator uses `SciencePipeline(config=SciencePipelineConfig(enable_all=True), db=session)` and `process_image(image_id)`; no short-circuits, no half-wired calls.
- **Cost ledger:** Both `CognitiveStateAnalyzer` and `ArchPatternsVLMAnalyzer` now call `describe_vlm_configuration()` and `log_vlm_usage(...)` in their real-data paths; stub paths still early-return.
- **No rot:** `compileall` passes on `backend/`; no stray `...` or uncontrolled stubs in critical modules.

## 4. Admin Frontend (UX / Workflow)

**Status: Good foundations, missing one key panel**

- **Budget & cost:**
  - Existing Budget panel shows `total_spent`, `hard_limit`, and kill switch status.
  - New `CostHistoryCard` renders a minimal daily bar chart, last-7-days cost, and delta vs previous 7 days.
  - Failure mode: When there is no usage, card shows a clear informational message instead of blank UI.
- **Upload workflow:**
  - The upload form still works as before but now receives a `job_id` from `AdminUploadResult`.
  - There is not yet a dedicated "Upload jobs" table or progress indicator tied to this `job_id`.

**UX Warnings:**

- **W1 ‚Äî Hidden job id:** The response‚Äôs `job_id` is not surfaced explicitly in the Admin UI as a clickable entry point to a job monitor.
- **W2 ‚Äî No "Recent upload jobs" panel:** Admins have no dashboard view of job statuses, progress, or error counts.

## 5. Science & Metrics

**Status: Strong math + cognitive, semantic still stubbed**

- **Math layers (L0/L1):** Fractal, texture, color, and complexity analyzers remain fully implemented and stable.
- **Cognitive VLM:** Cognitive state analyzer uses the configured VLM, logs costs, and writes attributes + evidence strings into the frame and DB.
- **Architectural patterns VLM:**
  - VLM-backed architectural pattern analyzer is live, emitting attributes like `arch.pattern.prospect_strong`, with confidence values and evidence.
  - Costs are logged per call into the ledger.
- **Semantic tags (L3/L4):**
  - Style and room-function semantics are still in the registry but marked as stubs; they are not yet wired to real VLM classification.

**Science warning:**

- **S1 ‚Äî Semantic registry still vaporware:** The new orchestrator does not change the fact that high-level semantic tags (`style.*`, `room_function.*`) remain unimplemented. For pure CNfA reasoning this is acceptable, but for ‚Äúfull pipeline semantics‚Äù it is a known TODO, not a regression.

## 6. Governance & Rot Detection

**Status: Excellent**

- `v3_governance.yml`, `governance.lock`, and `release.keep.yml` are intact and unchanged by this sprint.
- `install.sh` still runs the 5-guard gauntlet (hollow repo, program integrity, syntax, critical imports, canon).
- New modules (`backend/models/jobs.py`, `backend/services/upload_jobs.py`) contain no stubs and respect the existing patterns.

## 7. Recommended Next Sprint (from this audit)

Prioritised based on this audit:

1. **Upload Job Monitor UI (high priority):**
   - Add an Admin "Upload jobs" panel:
     - Table: job id, status, total/completed/failed, created time, error summary.
     - Link from the upload success message: ‚ÄúJob #123 created ‚Äî view in Upload Jobs.‚Äù
     - Auto-refresh or a manual ‚ÄúRefresh jobs‚Äù button.

2. **Optional worker hook (medium priority):**
   - Add a script or CLI entry point to run `run_upload_job(job_id)` from a separate worker process.
   - Document how to wire this into a real queue in `docs/UPLOAD_JOBS_README.md`.

3. **Semantic tag activation (larger science sprint):**
   - Wire one or two high-value semantic tag families (e.g., `style.modern`, `room_function.living_room`) to a real VLM classifier and feed their costs into the ledger.

This sprint (v3.4.50) is therefore **GO** for classroom and lab use, with the above steps recommended for the next iterations.
----- CONTENT END -----
----- FILE PATH: scripts/audit_vlm_variance.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
Audit VLM variance across attributes / sources.

This script is intended to be run on top of the *flat* export of the
Validation table (or any similar CSV) in order to detect attributes
for which the Vision-Language Model (VLM) is effectively "mode
collapsed" ‚Äî e.g. always emitting the same probability or bin value.

Why this matters
----------------
If a given attribute (e.g. ``cog.load.high`` or
``affect.restorative.medium``) shows:

- near-zero standard deviation across images, and
- a very narrow range of values, or
- a single value dominating almost all rows,

then we cannot trust the downstream psychological / BN analysis for
that attribute. The audit report produced here is meant to be *read*
and *acted on* by a human (e.g. Chief Scientist / Science Lead) who
can then:

- adjust prompts or model configuration,
- re-bin or re-threshold the variable,
- or temporarily drop it from science exports.

Expected input
--------------
By default the script assumes a CSV with at least the following columns:

- ``attribute_key``  ‚Äî the BN / science variable name
- ``source``         ‚Äî e.g. ``science_pipeline.vlm_v1`` or ``manual``
- ``value``          ‚Äî numeric or numeric-coded value

You can override these column names via CLI flags if your export uses
different labels.

Example usage
-------------
    python scripts/audit_vlm_variance.py \
        --input reports/bn_validations_flat.csv \
        --out reports/vlm_variance_audit.csv \
        --source-prefix science_pipeline.vlm

The output CSV will contain one row per (attribute_key, source) pair
with summary statistics and simple "flatness" flags.
"""

from __future__ import annotations

import argparse
import csv
import math
import statistics
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import Dict, Iterable, List, Tuple


@dataclass
class AttributeStats:
    attribute_key: str
    source: str
    count: int
    mean: float
    std: float
    min_value: float
    max_value: float
    range_value: float
    dominant_value: float
    dominant_fraction: float
    is_low_std: bool
    is_low_range: bool
    is_mode_collapsed: bool


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Audit VLM variance across attributes / sources.",
    )
    parser.add_argument(
        "--input",
        required=True,
        help="Path to the CSV export (e.g. bn_validations_flat.csv).",
    )
    parser.add_argument(
        "--out",
        required=True,
        help="Path to write the audit CSV report.",
    )
    parser.add_argument(
        "--attribute-column",
        default="attribute_key",
        help="Column name for the attribute / variable key (default: attribute_key).",
    )
    parser.add_argument(
        "--source-column",
        default="source",
        help="Column name for the source (e.g. science_pipeline.vlm_v1).",
    )
    parser.add_argument(
        "--value-column",
        default="value",
        help="Column name for the numeric value (default: value).",
    )
    parser.add_argument(
        "--source-prefix",
        default="science_pipeline.vlm",
        help=(
            "Only rows whose source column starts with this prefix are "
            "included in the audit (default: science_pipeline.vlm)."
        ),
    )
    parser.add_argument(
        "--min-samples",
        type=int,
        default=30,
        help="Minimum number of samples required for an attribute/source pair "
             "to be included in the report (default: 30).",
    )
    parser.add_argument(
        "--std-threshold",
        type=float,
        default=0.02,
        help="Standard deviation threshold below which a variable is flagged "
             "as low-variance (default: 0.02).",
    )
    parser.add_argument(
        "--range-threshold",
        type=float,
        default=0.05,
        help="Range (max-min) threshold below which a variable is flagged as "
             "low-range (default: 0.05).",
    )
    parser.add_argument(
        "--dominance-threshold",
        type=float,
        default=0.95,
        help="If the most frequent value accounts for >= this fraction of "
             "observations, the variable is considered 'mode collapsed' "
             "(default: 0.95).",
    )
    return parser.parse_args()


def _iter_rows(path: str) -> Iterable[Dict[str, str]]:
    with open(path, "r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            yield row


def _coerce_float(value: str) -> float:
    try:
        return float(value)
    except Exception:
        # Treat non-numeric / empty as NaN and let the stats code ignore it.
        return float("nan")


def compute_attribute_stats(
    rows: Iterable[Dict[str, str]],
    attribute_column: str,
    source_column: str,
    value_column: str,
    source_prefix: str,
    min_samples: int,
    std_threshold: float,
    range_threshold: float,
    dominance_threshold: float,
) -> List[AttributeStats]:
    by_key_source: Dict[Tuple[str, str], List[float]] = defaultdict(list)

    for row in rows:
        attr = row.get(attribute_column)
        src = row.get(source_column)
        if not attr or not src:
            continue
        if source_prefix and not src.startswith(source_prefix):
            continue

        value = _coerce_float(row.get(value_column, ""))
        if math.isnan(value):
            continue

        by_key_source[(attr, src)].append(value)

    results: List[AttributeStats] = []

    for (attr, src), values in sorted(by_key_source.items()):
        if len(values) < min_samples:
            continue

        values_sorted = sorted(values)
        count = len(values_sorted)
        mean = float(statistics.mean(values_sorted))
        std = float(statistics.pstdev(values_sorted))  # population std
        min_value = float(values_sorted[0])
        max_value = float(values_sorted[-1])
        range_value = max_value - min_value

        # Simple mode / dominance calculation
        freq: Dict[float, int] = defaultdict(int)
        for v in values_sorted:
            freq[v] += 1
        dominant_value, dominant_count = max(freq.items(), key=lambda kv: kv[1])
        dominant_fraction = dominant_count / float(count)

        is_low_std = std <= std_threshold
        is_low_range = range_value <= range_threshold
        is_mode_collapsed = dominant_fraction >= dominance_threshold

        results.append(
            AttributeStats(
                attribute_key=attr,
                source=src,
                count=count,
                mean=mean,
                std=std,
                min_value=min_value,
                max_value=max_value,
                range_value=range_value,
                dominant_value=dominant_value,
                dominant_fraction=dominant_fraction,
                is_low_std=is_low_std,
                is_low_range=is_low_range,
                is_mode_collapsed=is_mode_collapsed,
            )
        )

    return results


def write_report(path: str, stats: List[AttributeStats]) -> None:
    fieldnames = list(asdict(stats[0]).keys()) if stats else [
        "attribute_key",
        "source",
        "count",
        "mean",
        "std",
        "min_value",
        "max_value",
        "range_value",
        "dominant_value",
        "dominant_fraction",
        "is_low_std",
        "is_low_range",
        "is_mode_collapsed",
    ]
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for stat in stats:
            writer.writerow(asdict(stat))


def main() -> None:
    args = _parse_args()

    rows = _iter_rows(args.input)
    stats = compute_attribute_stats(
        rows=rows,
        attribute_column=args.attribute_column,
        source_column=args.source_column,
        value_column=args.value_column,
        source_prefix=args.source_prefix,
        min_samples=args.min_samples,
        std_threshold=args.std_threshold,
        range_threshold=args.range_threshold,
        dominance_threshold=args.dominance_threshold,
    )

    if not stats:
        print("[audit_vlm_variance] No qualifying attribute/source pairs found. "
              "Check your filters and input file.")
        return

    write_report(args.out, stats)
    print(f"[audit_vlm_variance] Wrote report with {len(stats)} rows to {args.out}")


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
----- CONTENT END -----
----- FILE PATH: scripts/bn_add_cog_affect_bins.py
----- CONTENT START -----
"""BN helper: add discrete cognitive/affective bins to a BN snapshot CSV.

Usage:
    python scripts/bn_add_cog_affect_bins.py

Assumes:
    - Input snapshot at data/bn_snapshot_raw.csv
    - Writes output to data/bn_snapshot_with_bins.csv

The script looks for continuous columns such as:
    cognitive.coherence, cognitive.complexity, cognitive.legibility,
    cognitive.mystery, cognitive.restoration,
    affect.cozy, affect.welcoming, affect.tranquil,
    affect.scary, affect.jarring

and adds 3-state discretized columns:
    COHERENCE, COMPLEXITY, LEGIBILITY, MYSTERY, RESTORATION,
    AFFECT_COSY, AFFECT_WELCOMING, AFFECT_TRANQUIL,
    AFFECT_SCARY, AFFECT_JARRING
"""

import pandas as pd
from pathlib import Path

SNAPSHOT_IN = Path("data/bn_snapshot_raw.csv")
SNAPSHOT_OUT = Path("data/bn_snapshot_with_bins.csv")

def bin_3(x):
    """Map a scalar in [0, 1] to LOW/MID/HIGH.

    Values outside [0, 1] or non-numerical entries are mapped to MID
    as a neutral fallback.
    """
    try:
        v = float(x)
    except Exception:
        return "MID"
    if v < 0.33:
        return "LOW"
    if v < 0.66:
        return "MID"
    return "HIGH"

mapping = {
    "cognitive.coherence": "COHERENCE",
    "cognitive.complexity": "COMPLEXITY",
    "cognitive.legibility": "LEGIBILITY",
    "cognitive.mystery": "MYSTERY",
    "cognitive.restoration": "RESTORATION",
    "affect.cozy": "AFFECT_COSY",
    "affect.welcoming": "AFFECT_WELCOMING",
    "affect.tranquil": "AFFECT_TRANQUIL",
    "affect.scary": "AFFECT_SCARY",
    "affect.jarring": "AFFECT_JARRING",
}

def main():
    if not SNAPSHOT_IN.exists():
        raise SystemExit(f"[bn_add_cog_affect_bins] Input snapshot not found: {SNAPSHOT_IN}")
    df = pd.read_csv(SNAPSHOT_IN)

    for col, node in mapping.items():
        if col in df.columns:
            df[node] = df[col].apply(bin_3)
        else:
            print(f"[bn_add_cog_affect_bins] WARNING: column {col} not found; skipping")

    SNAPSHOT_OUT.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(SNAPSHOT_OUT, index=False)
    print(f"[bn_add_cog_affect_bins] wrote {SNAPSHOT_OUT}")

if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/bn_merge_cog_affect_priors.py
----- CONTENT START -----
"""BN helper: merge cognitive/affective priors into a base priors CSV.

Usage:
    python scripts/bn_merge_cog_affect_priors.py

Assumes:
    - Existing priors CSV at data/BN_PRIORS_BASE.csv
    - Cognitive/affective priors example at docs/BN_PRIORS_COG_AFFECT_EXAMPLE.csv
    - Writes merged priors to data/BN_PRIORS_WITH_COG_AFFECT.csv

The script removes any nodes from the base file that are also present
in the cog/affect priors, then appends the cog/affect rows.
"""

import pandas as pd
from pathlib import Path

BASE_PRIORS = Path("data/BN_PRIORS_BASE.csv")
COG_AFFECT_PRIORS = Path("docs/BN_PRIORS_COG_AFFECT_EXAMPLE.csv")
MERGED_PRIORS = Path("data/BN_PRIORS_WITH_COG_AFFECT.csv")

def main():
    if not BASE_PRIORS.exists():
        raise SystemExit(f"[bn_merge_cog_affect_priors] Base priors not found: {BASE_PRIORS}")
    if not COG_AFFECT_PRIORS.exists():
        raise SystemExit(f"[bn_merge_cog_affect_priors] Cog/affect priors not found: {COG_AFFECT_PRIORS}")

    base = pd.read_csv(BASE_PRIORS)
    extra = pd.read_csv(COG_AFFECT_PRIORS)

    extra_nodes = set(extra["node"].unique())
    base_filtered = base[~base["node"].isin(extra_nodes)]

    merged = pd.concat([base_filtered, extra], ignore_index=True)

    # Sanity check per (node, parents)
    for (node, parents), group in merged.groupby(["node", "parents"]):
        s = group["p"].sum()
        if abs(s - 1.0) > 1e-6:
            print(f"[bn_merge_cog_affect_priors] WARN: probabilities for ({node}, {parents}) sum to {s:.3f}")

    MERGED_PRIORS.parent.mkdir(parents=True, exist_ok=True)
    merged.to_csv(MERGED_PRIORS, index=False)
    print(f"[bn_merge_cog_affect_priors] wrote {MERGED_PRIORS}")

if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/canon_guard.py
----- CONTENT START -----
"""Canonical feature registry guard.

Ensures coverage between the canonical feature registry
(features_canonical.jsonl) and the actual compute implementations.

A feature key is allowed to exist in the registry if and only if it is:
- computed somewhere in backend/science via frame.add_attribute, or
- explicitly listed in backend.science.feature_stubs.STUB_FEATURE_KEYS.

This script is designed to be robust to partial or placeholder lines in the
JSONL file (it will skip lines that fail JSON decoding), so that it fails
only on *true* coverage issues rather than formatting glitches.
"""

from __future__ import annotations

import json
import re
import sys
from pathlib import Path

from backend.science import feature_stubs

ROOT = Path(__file__).resolve().parents[1]
CANON_PATH = ROOT / "backend" / "science" / "features_canonical.jsonl"


def _load_registry_keys() -> set[str]:
    """Load canonical feature keys from the JSONL registry.

    Each non-empty line is expected to be a JSON object with at least a
    'key' field. We also tolerate escaped newlines ("\\n") in the
    'description' field, similar to the test harness.
    """
    if not CANON_PATH.exists():
        print(
            f"[canon_guard] Canonical registry file not found: {CANON_PATH}",
            file=sys.stderr,
        )
        raise SystemExit(1)

    raw = CANON_PATH.read_text(encoding="utf-8", errors="ignore")
    keys: set[str] = set()

    for lineno, line in enumerate(raw.splitlines(), start=1):
        stripped = line.strip()
        if not stripped:
            continue

        # Some export pipelines store escaped newlines in JSONL; unescape them
        # for compatibility with the test harness behaviour.
        normalized = stripped.replace("\\n", "\n")

        try:
            obj = json.loads(normalized)
        except Exception:
            # Be conservative: skip obviously placeholder or truncated lines in
            # this guard, rather than failing with a JSON error. The program
            # integrity guard is responsible for policing placeholders.
            continue

        key = obj.get("key")
        if isinstance(key, str) and key:
            keys.add(key)

    return keys


def _load_computed_keys() -> set[str]:
    """Scan backend/science for frame.add_attribute("<key>") calls.

    This mirrors the behaviour of tests/test_feature_registry_coverage.py,
    but packaged for CI/install-time use.
    """
    base = ROOT / "backend" / "science"
    keys: set[str] = set()
    for path in base.rglob("*.py"):
        text = path.read_text(encoding="utf-8", errors="ignore")
        for m in re.finditer(r'add_attribute\("([^"]+)"', text):
            keys.add(m.group(1))
    return keys


def main() -> None:
    registry_keys = _load_registry_keys()
    computed_keys = _load_computed_keys()
    stub_keys = feature_stubs.STUB_FEATURE_KEYS

    dangling = registry_keys - computed_keys - stub_keys
    if dangling:
        print("[canon_guard] Dangling registry keys with no compute or stub:", file=sys.stderr)
        for key in sorted(dangling)[:20]:
            print(f"  - {key}", file=sys.stderr)
        raise SystemExit(1)

    print("[canon_guard] OK: registry keys covered by compute or stubs.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/check_no_pycache_in_tree.py
----- CONTENT START -----
"""Guard script: assert that no __pycache__ directories are present in the repo tree.

This is used in CI to prevent Python bytecode caches from creeping into release artifacts.
"""
from pathlib import Path
import sys

def main() -> int:
    root = Path(__file__).resolve().parents[1]
    bad_paths = []
    for path in root.rglob("__pycache__"):
        if path.is_dir():
            bad_paths.append(path)

    if not bad_paths:
        print("[pycache-guard] OK: no __pycache__ directories found.")
        return 0

    print("[pycache-guard] ERROR: __pycache__ directories found in repo:")
    for p in bad_paths:
        print(f"  - {p}")
    print("[pycache-guard] Please remove these before creating a release ZIP/TXT.")
    return 1

if __name__ == "__main__":
    sys.exit(main())
----- CONTENT END -----
----- FILE PATH: scripts/critical_import_guard.py
----- CONTENT START -----
"""Critical Import Guard

Attempts to import critical local modules to ensure they can be loaded
without local-code failures. Third-party missing dependencies are treated
as warnings (install/docker will handle them), but local package failures
are hard NO-GO.
"""

from __future__ import annotations

import importlib
import sys
from typing import List, Tuple

from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

CRITICAL_MODULES = [
    "backend.science.pipeline",
    "backend.main",
]


def _is_third_party_missing(e: ModuleNotFoundError) -> bool:
    name = getattr(e, "name", None)
    return bool(name) and not name.startswith("backend") and not name.startswith("scripts")


def main() -> None:
    failures: List[Tuple[str, str]] = []
    warnings: List[Tuple[str, str]] = []

    for mod in CRITICAL_MODULES:
        try:
            importlib.import_module(mod)
        except ModuleNotFoundError as e:
            if _is_third_party_missing(e):
                warnings.append((mod, f"missing third-party dependency: {e.name}"))
            else:
                failures.append((mod, f"missing local dependency: {e}"))
        except Exception as e:
            failures.append((mod, f"import failed: {type(e).__name__}: {e}"))

    if warnings:
        print("[critical_import_guard] WARNINGS:")
        for mod, msg in warnings:
            print(f"  {mod}: {msg}")

    if failures:
        print("[critical_import_guard] NO-GO", file=sys.stderr)
        for mod, msg in failures:
            print(f"  {mod}: {msg}", file=sys.stderr)
        raise SystemExit(1)

    print("[critical_import_guard] OK: critical modules import cleanly.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/export_bn_glossary.py
----- CONTENT START -----
"""Export BN candidate input glossary.

This script reads INDEX_CATALOG and exports a machine-readable glossary of
entries tagged as "candidate_bn_input" to docs/BN_GLOSSARY_AUTO.json.
"""

from __future__ import annotations

import json
from pathlib import Path

from backend.science.index_catalog import get_candidate_bn_keys, get_index_metadata


def main() -> int:
    catalog = get_index_metadata()
    keys = get_candidate_bn_keys()
    out: dict[str, dict] = {}

    for key in keys:
        info = catalog.get(key, {})
        out[key] = {
            "label": info.get("label"),
            "description": info.get("description"),
            "type": info.get("type"),
            "bins": info.get("bins"),
            "tags": list(info.get("tags", [])),
        }

    docs = Path("docs")
    docs.mkdir(parents=True, exist_ok=True)
    target = docs / "BN_GLOSSARY_AUTO.json"
    target.write_text(json.dumps(out, indent=2, sort_keys=True), encoding="utf-8")
    print(f"[export_bn_glossary] wrote {len(out)} entries to {target}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
----- CONTENT END -----
----- FILE PATH: scripts/export_bn_ready_dataset.py
----- CONTENT START -----
"""Export a BN-ready dataset from a running Image Tagger v3 API."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Iterable

import requests


def fetch_bn_rows(api_base: str) -> Iterable[dict]:
    url = api_base.rstrip("/") + "/v1/export/bn-snapshot"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    data = resp.json()
    if not isinstance(data, list):
        raise RuntimeError("Expected a list of BNRow objects from /v1/export/bn-snapshot")
    return data


def main() -> None:
    import argparse

    parser = argparse.ArgumentParser(description="Export BN-ready dataset from Image Tagger v3 API.")
    parser.add_argument("--api-base", default="http://localhost:8000")
    parser.add_argument("--out", default="bn_dataset.jsonl")
    args = parser.parse_args()

    rows = list(fetch_bn_rows(args.api_base))
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"Wrote {len(rows)} BN rows to {out_path}")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/generate_tag_coverage.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Generate a science tag coverage snapshot for Image Tagger.

This script inspects the science code and stub list to produce a
machine-readable coverage map:

  - which feature keys are known (from registry, stubs, or analyzers);
  - which ones are computed by which analyzers (via `add_attribute`);
  - which ones are marked as stubs only;
  - a coarse-grained source_type classification.

It writes:

  - `science_tag_coverage_v1.json` at the repo root; and
  - `docs/SCIENCE_TAG_MAP.md` with a human-readable summary.

Run from the repo root:

    python scripts/generate_tag_coverage.py
"""

from __future__ import annotations

import ast
import json
import sys
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import DefaultDict, Dict, List, Sequence, Set


REPO_ROOT = Path(__file__).resolve().parents[1]
SCIENCE_DIR = REPO_ROOT / "backend" / "science"

# Ensure `backend` is importable.
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

# These imports are intentionally late-bound for robustness.
from backend.science import feature_stubs
try:
    from backend.science.features_registry import load_features
except Exception:  # pragma: no cover - defensive
    load_features = None  # type: ignore[assignment]


@dataclass
class FeatureCoverage:
    key: str
    analyzers: List[str]
    stub: bool
    source_type: str  # math_or_deterministic | vlm_cognitive | vlm_semantic | stub_only | unassigned


class _AttributeCollector(ast.NodeVisitor):
    """Collect `frame.add_attribute("key", ...)` calls."""

    def __init__(self, module_name: str) -> None:
        self.module_name = module_name
        self.current_class: str | None = None
        self.mapping: DefaultDict[str, Set[str]] = defaultdict(set)

    def visit_ClassDef(self, node: ast.ClassDef) -> None:  # type: ignore[override]
        prev = self.current_class
        self.current_class = node.name
        self.generic_visit(node)
        self.current_class = prev

    def visit_Call(self, node: ast.Call) -> None:  # type: ignore[override]
        func = node.func
        is_add = False
        if isinstance(func, ast.Attribute) and func.attr == "add_attribute":
            is_add = True
        elif isinstance(func, ast.Name) and func.id == "add_attribute":
            is_add = True

        if is_add and node.args:
            arg0 = node.args[0]
            if isinstance(arg0, ast.Constant) and isinstance(arg0.value, str):
                key = arg0.value
                owner = self.module_name
                if self.current_class:
                    owner = f"{owner}.{self.current_class}"
                self.mapping[key].add(owner)

        self.generic_visit(node)


def _collect_semantic_keys() -> Set[str]:
    """Extract canonical semantic feature keys from SemanticTagAnalyzer.

    We do not rely on `add_attribute` calls here because the analyzer uses
    an intermediate mapping (style_map / room_map) with variables.
    Instead we scan for literal strings starting with the canonical
    prefixes.
    """
    sem_file = SCIENCE_DIR / "semantics" / "semantic_tags_vlm.py"
    if not sem_file.exists():
        return set()

    try:
        source = sem_file.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        return set()

    try:
        tree = ast.parse(source, filename=str(sem_file))
    except SyntaxError:
        return set()

    semantic_keys: Set[str] = set()

    class _ConstVisitor(ast.NodeVisitor):
        def visit_Constant(self, node):  # type: ignore[override]
            if isinstance(node.value, str):
                if node.value.startswith("style.") or node.value.startswith("spatial.room_function."):
                    semantic_keys.add(node.value)
            self.generic_visit(node)

    _ConstVisitor().visit(tree)
    return semantic_keys


def _collect_computed_keys() -> Dict[str, Set[str]]:
    """Scan backend/science for add_attribute calls.

    Returns a mapping: feature_key -> set of "module[.Class]" strings.
    """
    mapping: DefaultDict[str, Set[str]] = defaultdict(set)

    for path in SCIENCE_DIR.rglob("*.py"):
        # Skip internal / guard scripts that are not science analyzers.
        if path.name.endswith("_guard.py"):
            continue

        rel = path.relative_to(REPO_ROOT).with_suffix("")
        module_name = ".".join(rel.parts)

        try:
            text = path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            continue

        try:
            tree = ast.parse(text, filename=str(path))
        except SyntaxError:
            # Guards / legacy snapshots may not parse cleanly; skip.
            continue

        collector = _AttributeCollector(module_name)
        collector.visit(tree)

        for key, owners in collector.mapping.items():
            mapping[key].update(owners)

    return dict(mapping)


def _classify_source_type(key: str, analyzers: Sequence[str], stub_keys: Set[str]) -> str:
    """Coarse classification of source type for a feature key."""
    if analyzers:
        if any("semantic_tags_vlm.SemanticTagAnalyzer" in a for a in analyzers):
            return "vlm_semantic"
        if any("context.cognitive.CognitiveStateAnalyzer" in a for a in analyzers):
            return "vlm_cognitive"
        return "math_or_deterministic"

    # No analyzers found.
    if key in stub_keys:
        return "stub_only"
    return "unassigned"


def main() -> None:
    # 1) Collect stubs and computed keys.
    stub_keys: Set[str] = set(feature_stubs.STUB_FEATURE_KEYS)
    computed = _collect_computed_keys()

    # 1b) Inject semantic keys as computed by SemanticTagAnalyzer explicitly.
    semantic_keys = _collect_semantic_keys()
    if semantic_keys:
        owner = "backend.science.semantics.semantic_tags_vlm.SemanticTagAnalyzer"
        for key in semantic_keys:
            computed.setdefault(key, set()).add(owner)

    # 2) Advisory registry from load_features(), if available and non-empty.
    registry_meta: Dict[str, dict] = {}
    registry_keys: Set[str] = set()
    feats = []
    if load_features is not None:
        try:
            feats = load_features()
        except Exception:
            feats = []

    if feats:
        for feat in feats:
            key = getattr(feat, "key", None)
            if not isinstance(key, str) or not key:
                continue
            registry_keys.add(key)
            registry_meta[key] = {
                "key": feat.key,
                "category": getattr(feat, "category", None),
                "tier": getattr(feat, "tier", None),
                "status": getattr(feat, "status", None),
                "type": getattr(feat, "type", None),
                "group": getattr(feat, "group", None),
            }
    else:
        # Fallback: take the union of stub keys and computed keys.
        registry_keys.update(stub_keys)
        registry_keys.update(computed.keys())
        for key in registry_keys:
            registry_meta[key] = {"key": key}

    coverage: Dict[str, FeatureCoverage] = {}
    counts: DefaultDict[str, int] = defaultdict(int)

    for key in sorted(registry_keys):
        analyzers = sorted(computed.get(key, []))
        source_type = _classify_source_type(key, analyzers, stub_keys)
        stub = key in stub_keys

        cov = FeatureCoverage(
            key=key,
            analyzers=analyzers,
            stub=stub,
            source_type=source_type,
        )
        coverage[key] = cov
        counts[source_type] += 1
        if stub:
            counts["stub_total"] += 1

    meta = {
        "version": 1,
        "registry_count": len(registry_keys),
        "computed_count": sum(1 for c in coverage.values() if c.analyzers),
        "stub_count": len(stub_keys),
        "counts_by_source_type": dict(counts),
    }

    # JSON snapshot
    json_payload = {
        "meta": meta,
        "coverage": {
            key: {
                "key": cov.key,
                "analyzers": cov.analyzers,
                "stub": cov.stub,
                "source_type": cov.source_type,
            }
            for key, cov in sorted(coverage.items(), key=lambda kv: kv[0])
        },
    }

    json_path = REPO_ROOT / "science_tag_coverage_v1.json"
    json_path.write_text(json.dumps(json_payload, indent=2, sort_keys=True), encoding="utf-8")

    # Markdown summary
    docs_dir = REPO_ROOT / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)
    md_path = docs_dir / "SCIENCE_TAG_MAP.md"

    lines = []
    lines.append("# Science Tag Coverage Map (v1)")
    lines.append("")
    lines.append("Autogenerated by `scripts/generate_tag_coverage.py`.")
    lines.append("")
    lines.append(f"- Total known feature keys: {meta['registry_count']}")
    lines.append(f"- Keys with at least one compute implementation: {meta['computed_count']}")
    lines.append(f"- Stub-allowed keys: {meta['stub_count']}")
    lines.append("")
    lines.append("## Breakdown by source_type")
    lines.append("")
    lines.append("| source_type | count |")
    lines.append("|------------|-------|")
    for stype, count in sorted(meta["counts_by_source_type"].items()):
        lines.append(f"| {stype} | {count} |")
    lines.append("")
    lines.append("## Notes")
    lines.append("")
    lines.append("- `math_or_deterministic`: numeric features computed by the L0/L1 engines")
    lines.append("  (e.g., color histograms, texture, fractals, depth/spatial metrics).")
    lines.append("- `vlm_cognitive`: high-level cognitive/affective dimensions estimated by")
    lines.append("  the CognitiveStateAnalyzer VLM.")
    lines.append("- `vlm_semantic`: semantic tags such as style.* and spatial.room_function.*")
    lines.append("  estimated by the SemanticTagAnalyzer VLM.")
    lines.append("- `stub_only`: keys that are intentionally present in the registry but do")
    lines.append("  not yet have a compute implementation; they are tracked by")
    lines.append("  `backend/science/feature_stubs.py`.")
    lines.append("- `unassigned`: keys that are present in the union of registry/stub/computed")
    lines.append("  keys but that currently have no detectable compute implementation and are")
    lines.append("  not listed as stubs. In a healthy repository this count should be zero.")
    lines.append("  New builds should fail if it drifts above zero.")

    md_path.write_text("\n".join(lines) + "\n", encoding="utf-8")

    print(f"[generate_tag_coverage] Wrote {json_path.relative_to(REPO_ROOT)}")
    print(f"[generate_tag_coverage] Wrote {md_path.relative_to(REPO_ROOT)}")
    print(f"[generate_tag_coverage] Meta: {json.dumps(meta, indent=2)}")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/guardian.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
THE GUARDIAN - Governance Enforcement Script (v3.2)

Usage:
  python scripts/guardian.py freeze   -> Snapshot current state to governance.lock
  python scripts/guardian.py verify   -> Check current state against governance.lock
"""

import sys
import hashlib
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml  # Requires PyYAML

REPO_ROOT = Path(__file__).resolve().parents[1]
CONFIG_FILE = REPO_ROOT / "v3_governance.yml"
LOCK_FILE = REPO_ROOT / "governance.lock"


def load_config() -> Dict[str, Any]:
    """Load the governance config from v3_governance.yml."""
    if not CONFIG_FILE.exists():
        print("[guardian] v3_governance.yml not found; using empty config.")
        return {}
    with CONFIG_FILE.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
    return data


def sha256_file(path: Path) -> str:
    """Compute SHA256 hash of a file."""
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def snapshot(conf: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a baseline snapshot:
    - Hashes of all files under protected scopes
    - List of critical files
    - Root-level files (for prevent_new_root_files)
    """
    protected_scopes: List[str] = conf.get("protected_scopes", []) or []
    critical_files: List[str] = conf.get("critical_files", []) or []
    constraints: Dict[str, Any] = conf.get("constraints", {}) or {}

    protected_files: Dict[str, Dict[str, Any]] = {}

    for scope in protected_scopes:
        scope_path = REPO_ROOT / scope
        if not scope_path.exists():
            continue
        if scope_path.is_file():
            rel = scope_path.relative_to(REPO_ROOT).as_posix()
            protected_files[rel] = {
                "hash": sha256_file(scope_path),
                "size": scope_path.stat().st_size,
            }
            continue

        for p in scope_path.rglob("*"):
            if p.is_file():
                rel = p.relative_to(REPO_ROOT).as_posix()
                protected_files[rel] = {
                    "hash": sha256_file(p),
                    "size": p.stat().st_size,
                }

    root_files = sorted([p.name for p in REPO_ROOT.iterdir() if p.is_file()])

    snapshot_obj: Dict[str, Any] = {
        "policy_version": conf.get("policy_version", "3.0.0"),
        "protected_files": protected_files,
        "critical_files": critical_files,
        "constraints": constraints,
        "root_files": root_files,
    }
    return snapshot_obj


def freeze(conf: Dict[str, Any], lock_path: Optional[Path] = None) -> None:
    """Create or refresh the governance.lock baseline.

    Parameters
    ----------
    conf:
        Parsed config from v3_governance.yml.
    lock_path:
        Optional override for the lock file location. When omitted, the
        global LOCK_FILE is used. Tests may supply a temporary path to
        avoid mutating the real governance.lock.
    """
    if lock_path is None:
        lock_path = LOCK_FILE

    baseline = snapshot(conf)
    lock_path.write_text(json.dumps(baseline, indent=2), encoding="utf-8")
    rel = lock_path.relative_to(REPO_ROOT).as_posix()
    print(f"[guardian] Baseline written to {rel}")



def _check_science_tag_coverage(constraints: Dict[str, Any], failures: List[str]) -> None:
    """Enforce that the science tag coverage has no 'unassigned' keys.

    When `constraints.enforce_science_tag_coverage` is true, this function
    expects a `science_tag_coverage_v1.json` file at the repo root and will
    fail verification if any feature keys are reported with source_type
    "unassigned".
    """
    enforce = bool(constraints.get("enforce_science_tag_coverage", False))
    if not enforce:
        return

    cov_path = REPO_ROOT / "science_tag_coverage_v1.json"
    if not cov_path.exists():
        failures.append(
            "Science tag coverage: science_tag_coverage_v1.json is missing; "
            "run `python scripts/generate_tag_coverage.py` before verifying."
        )
        return

    try:
        payload = json.loads(cov_path.read_text(encoding="utf-8"))
    except Exception as exc:  # pragma: no cover - defensive
        failures.append(f"Science tag coverage: failed to read JSON: {exc}")
        return

    meta = payload.get("meta") or {}
    counts = meta.get("counts_by_source_type") or {}
    unassigned = int(counts.get("unassigned", 0) or 0)
    if unassigned > 0:
        failures.append(
            f"Science tag coverage: {unassigned} feature key(s) are 'unassigned'. "
            "Every key must be wired, explicitly stubbed, or removed from the registry."
        )


def _check_bn_db_health(constraints: Dict[str, Any], failures: List[str]) -> None:
    """Optionally run BN / DB health checks via backend.scripts.bn_db_health.

    When `constraints.check_bn_db_health` is true, this will attempt to run
    the BN / DB health checker in-process and append a human-readable
    failure message if any violations are detected.

    This check requires a running database. If the checker cannot be
    imported or raises an exception, the error is recorded as a failure.
    """
    if not constraints.get("check_bn_db_health"):
        return

    try:
        from backend.scripts import bn_db_health
    except Exception as exc:  # pragma: no cover - import errors
        failures.append(f"BN/DB health: could not import checker: {exc}")
        return

    try:
        summary = bn_db_health.run_health_check(exit_on_failure=False)
    except Exception as exc:  # pragma: no cover - runtime errors
        failures.append(f"BN/DB health: check raised an exception: {exc}")
        return

    if not summary.get("ok", False):
        orphan = summary.get("orphan_validations", 0)
        missing = summary.get("missing_candidates", 0)
        failures.append(
            "BN/DB health: "
            f"{orphan} orphan Validation.attribute_key rows and "
            f"{missing} missing BN candidate keys in attributes; "
            "run `python -m backend.scripts.bn_db_health` for details."
        )

def verify(conf: Dict[str, Any], lock_path: Optional[Path] = None) -> int:
    """Verify the current repo state against governance.lock.

    Rules
    -----
    - All critical_files must exist and be >= min_file_size_bytes.
    - All protected_files from the baseline must still exist and not be trivially small.
    - If prevent_new_root_files is true, no new root-level files may appear
      (except governance.lock itself).
    - Hash changes in protected files are treated as failures; they should
      be followed by a manual freeze when intentionally updating the baseline.
    """
    if lock_path is None:
        lock_path = LOCK_FILE

    if not lock_path.exists():
        print("[guardian] No governance.lock baseline; nothing to verify yet.")
        # Treat as success to avoid blocking installs before first freeze.
        return 0

    try:
        baseline = json.loads(lock_path.read_text(encoding="utf-8"))
    except Exception as exc:
        print(f"[guardian] Failed to read {lock_path}: {exc}")
        return 1

    critical_files = baseline.get("critical_files") or []
    protected_files: Dict[str, Dict[str, Any]] = baseline.get("protected_files") or {}
    root_files = set(baseline.get("root_files") or [])

    constraints = conf.get("constraints", {}) or {}
    min_size = int(constraints.get("min_file_size_bytes", 0))
    prevent_new_root = bool(constraints.get("prevent_new_root_files", False))

    failures = []

    # Science tag coverage enforcement (no 'unassigned' feature keys)
    _check_science_tag_coverage(constraints, failures)

    # Optional BN / DB health check (requires a running database)
    _check_bn_db_health(constraints, failures)

    # Critical files: existence + minimum size
    for rel in critical_files:
        p = REPO_ROOT / rel
        if not p.exists():
            failures.append(f"Critical file missing: {rel}")
        else:
            size = p.stat().st_size
            if size < min_size:
                failures.append(f"Critical file too small: {rel} ({size} bytes)")

    # Protected files: existence + minimum size + hash stability
    for rel, info in protected_files.items():
        p = REPO_ROOT / rel
        if not p.exists():
            failures.append(f"Protected file missing: {rel}")
            continue

        size = p.stat().st_size
        if size < min_size:
            failures.append(f"Protected file too small: {rel} ({size} bytes)")
            continue

        old_hash = info.get("hash")
        new_hash = sha256_file(p)
        if old_hash and new_hash != old_hash:
            failures.append(f"Protected file hash changed: {rel}")

    # Root-level file drift
    if prevent_new_root:
        baseline_root_files = set(root_files)
        current_root_files = {p.name for p in REPO_ROOT.iterdir() if p.is_file()}
        extras = sorted(current_root_files - baseline_root_files)
        # governance.lock is expected and safe as a new root file
        extras = [name for name in extras if name not in {"governance.lock"}]
        if extras:
            failures.append(
                "New root-level files detected: " + ", ".join(extras)
            )

    if failures:
        print("[guardian] Verification FAILED:")
        for msg in failures:
            print(" -", msg)
        return 1

    print("[guardian] Verification PASSED.")
    return 0


def main(argv: List[str]) -> int:
    if len(argv) < 2:
        print("Usage: python scripts/guardian.py [freeze|verify]")
        return 1

    mode = argv[1]
    conf = load_config()

    if mode == "freeze":
        freeze(conf)
        return 0
    elif mode == "verify":
        return verify(conf)
    else:
        print(f"Unknown mode: {mode}")
        return 1


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))----- CONTENT END -----
----- FILE PATH: scripts/hollow_repo_guard.py
----- CONTENT START -----
"""
Hollow Repo Guard

Simple structural sanity check to prevent "shell" releases.

Checks:
- critical_paths exist
- min_counts thresholds (per release.keep.yml) are respected
"""

from __future__ import annotations

import json
import os
import sys
from pathlib import Path

import yaml  # type: ignore


ROOT = Path(__file__).resolve().parents[1]


def load_release_policy() -> dict:
    cfg_path = ROOT / "release.keep.yml"
    if not cfg_path.exists():
        print("[hollow_repo_guard] release.keep.yml not found; treating as NO-GO.", file=sys.stderr)
        raise SystemExit(1)
    with cfg_path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def check_critical_paths(policy: dict) -> list[str]:
    missing = []
    for rel in policy.get("critical_paths", []):
        target = ROOT / rel
        if not target.exists():
            missing.append(rel)
    return missing


def count_files_under(rel: str) -> int:
    base = ROOT / rel
    if not base.exists():
        return 0
    total = 0
    for root, dirs, files in os.walk(base):
        total += len(files)
    return total


def check_min_counts(policy: dict) -> dict[str, int]:
    violations: dict[str, int] = {}
    for rel, min_count in (policy.get("min_counts") or {}).items():
        actual = count_files_under(rel)
        if actual < int(min_count):
            violations[rel] = actual
    return violations


def main() -> None:
    policy = load_release_policy()
    missing = check_critical_paths(policy)
    if missing:
        print("[hollow_repo_guard] Missing critical paths:", file=sys.stderr)
        for rel in missing:
            print(f"  - {rel}", file=sys.stderr)
        raise SystemExit(1)

    violations = check_min_counts(policy)
    if violations:
        print("[hollow_repo_guard] Min-count violations:", file=sys.stderr)
        for rel, actual in violations.items():
            print(f"  - {rel}: {actual} files (below configured minimum)", file=sys.stderr)
        raise SystemExit(1)

    print("[hollow_repo_guard] OK: structure looks non-hollow.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/import_harness.py
----- CONTENT START -----
"""Lightweight import harness for critical backend modules.

Run with:
    python -m scripts.import_harness
"""

def main() -> None:
    modules = [
        "backend.science.math.color",
        "backend.science.math.glcm",
        "backend.science.math.fractals",
        "backend.science.math.complexity",
        "backend.science.spatial.depth",
        "backend.science.context.cognitive",
        "backend.science.pipeline",
        "backend.services.auth",
        "backend.services.training_export",
        "backend.api.v1_admin",
        "backend.api.v1_supervision",
        "backend.api.v1_annotation",
        "backend.api.v1_discovery",
    ]
    for name in modules:
        try:
            __import__(name)
            print(f"[OK] {name}")
        except Exception as exc:
            print(f"[FAIL] {name}: {exc}")

if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: scripts/program_integrity_guard.py
----- CONTENT START -----
"""
Program Integrity Guard

Scans for dangerous ellipsis placeholders and uncontrolled stubs in live code.

Rules:
- A bare "..." line is forbidden outside the ellipsis_allowlist.
- "# STUB:" is allowed only in files listed in stub_allowlist.
- Archive directories are ignored.
"""

from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import Iterable

import yaml  # type: ignore


ROOT = Path(__file__).resolve().parents[1]


def load_release_policy() -> dict:
    cfg_path = ROOT / "release.keep.yml"
    if not cfg_path.exists():
        print("[program_integrity_guard] release.keep.yml not found; treating as NO-GO.", file=sys.stderr)
        raise SystemExit(1)
    with cfg_path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def iter_source_files() -> Iterable[Path]:
    for root, dirs, files in os.walk(ROOT):
        rel_root = Path(root).relative_to(ROOT)
        # Skip archive and __pycache__
        if any(part in {"archive", "__pycache__"} for part in rel_root.parts):
            continue
        for name in files:
            if name.endswith(".py"):
                if name in {"program_integrity_guard.py", "syntax_guard.py", "critical_import_guard.py"}:
                    continue
                yield Path(root) / name


def main() -> None:
    policy = load_release_policy()
    stub_allow = set(policy.get("stub_allowlist") or [])
    ellipsis_allow = set(policy.get("ellipsis_allowlist") or [])

    bad_ellipsis: list[tuple[str, int, str]] = []
    bad_stubs: list[tuple[str, int, str]] = []

    for path in iter_source_files():
        rel = str(path.relative_to(ROOT))
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for lineno, line in enumerate(f, start=1):
                stripped = line.strip()
                if stripped == "...":
                    if rel not in ellipsis_allow:
                        bad_ellipsis.append((rel, lineno, line.rstrip("\n")))
                if "# STUB:" in line:
                    if rel not in stub_allow:
                        bad_stubs.append((rel, lineno, line.rstrip("\n")))

    if bad_ellipsis or bad_stubs:
        print("[program_integrity_guard] Integrity violations detected:", file=sys.stderr)
        if bad_ellipsis:
            print("  Bare ellipses (forbidden):", file=sys.stderr)
            for rel, lineno, snippet in bad_ellipsis:
                print(f"    {rel}:{lineno}: {snippet}", file=sys.stderr)
        if bad_stubs:
            print("  Unallowlisted stubs:", file=sys.stderr)
            for rel, lineno, snippet in bad_stubs:
                print(f"    {rel}:{lineno}: {snippet}", file=sys.stderr)
        raise SystemExit(1)

    print("[program_integrity_guard] OK: no dangerous ellipses or uncontrolled stubs.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/run_science_on_sample.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Run the science pipeline on a single sample image.

This is a convenience script to verify end-to-end wiring:

- Database connectivity via SQLAlchemy
- Image loading via the data_store service
- Science analyzers (complexity, texture, fractals, materials, color, context)
- Persistence of attributes into Validation rows
"""

import argparse
import asyncio
import logging

from backend.database.core import SessionLocal
from backend.science.pipeline import SciencePipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def _run(image_id: int) -> None:
    db = SessionLocal()
    try:
        pipeline = SciencePipeline(db)
        ok = await pipeline.process_image(image_id=image_id)
        if not ok:
            logger.error("Science pipeline failed for image_id=%s", image_id)
        else:
            logger.info("Science pipeline completed for image_id=%s", image_id)
    finally:
        db.close()


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--image-id", type=int, required=True, help="ID of the Image row to process.")
    args = parser.parse_args()
    asyncio.run(_run(args.image_id))


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: scripts/run_upload_job.py
----- CONTENT START -----
#!/usr/bin/env python
"""Run a single upload job's science pipeline.

Usage:
    python -m scripts.run_upload_job <job_id>
    or
    python scripts/run_upload_job.py <job_id>

This script is a thin wrapper around `backend.services.upload_jobs.run_upload_job`.
It is intended for use by a worker process in more advanced deployments.
"""

import argparse
import logging

from backend.database import SessionLocal  # ensures DB is configured
from backend.services.upload_jobs import run_upload_job


logger = logging.getLogger(__name__)


def main() -> None:
    parser = argparse.ArgumentParser(description="Run an upload job's science pipeline")
    parser.add_argument("job_id", type=int, help="ID of the UploadJob to run")
    args = parser.parse_args()

    job_id = args.job_id
    logger.info("Running upload job %s", job_id)
    # `run_upload_job` manages its own SessionLocal internally.
    run_upload_job(job_id)
    logger.info("Completed upload job %s", job_id)


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/science_harness.py
----- CONTENT START -----
"""Standalone science-only harness.

Usage (folder mode):

  python -m scripts.science_harness --input-dir path/to/images --output science_attributes.csv

This does not require a running database; it loads images directly from a folder,
runs the science analyzers, and writes a CSV with one row per image.
"""

from __future__ import annotations

import argparse
import csv
from pathlib import Path

from backend.services.vlm import describe_vlm_configuration
from typing import List

import numpy as np

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None

from backend.science.core import AnalysisFrame
from backend.science.math.color import ColorAnalyzer
from backend.science.math.complexity import ComplexityAnalyzer
from backend.science.math.glcm import TextureAnalyzer
from backend.science.math.fractals import FractalAnalyzer
from backend.science.spatial.depth import DepthAnalyzer
from backend.science.context.cognitive import CognitiveStateAnalyzer


def iter_images(input_dir: Path) -> List[Path]:
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}

    files = []
    for p in sorted(input_dir.rglob("*")):
        if p.is_file() and p.suffix.lower() in exts:
            files.append(p)
    return files


def main() -> None:
    parser = argparse.ArgumentParser(description="Science-only harness for Image Tagger v3.3")
    parser.add_argument("--input-dir", type=str, required=True, help="Folder of images to analyze")
    parser.add_argument("--output", type=str, default="science_attributes.csv", help="Output CSV path")
    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        raise SystemExit(f"Input directory does not exist: {input_dir}")

    if cv2 is None:
        raise SystemExit("OpenCV (cv2) is required to run the science harness.")

    images = iter_images(input_dir)
    if not images:
        raise SystemExit(f"No images found in {input_dir}")

    color = ColorAnalyzer()
    comp = ComplexityAnalyzer()
    tex = TextureAnalyzer()
    frac = FractalAnalyzer()
    depth = DepthAnalyzer()
    cognitive = CognitiveStateAnalyzer()  # neutral baseline

    rows = []
    all_keys = set()

    for idx, path in enumerate(images):
        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)
        if bgr is None:
            continue
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

        frame = AnalysisFrame(image_id=idx, original_image=rgb)

        color.analyze(frame)
        comp.analyze(frame)
        tex.analyze(frame)
        frac.analyze(frame)
        depth.analyze(frame)
        cognitive.analyze(frame)

        row = {"filename": path.name}
        row.update(frame.attributes)
        rows.append(row)
        all_keys.update(frame.attributes.keys())

    all_keys = sorted(all_keys)
    fieldnames = ["filename"] + all_keys

    out_path = Path(args.output)
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)

    print(f"Wrote {len(rows)} rows to {out_path}")


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: scripts/smoke_api.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Lightweight API smoke test for Image Tagger v3.2.

This script is intentionally minimal: it checks that the FastAPI
application can be imported and that a few core routes exist.
"""

from fastapi.testclient import TestClient

from backend.main import app


def main() -> None:
    client = TestClient(app)

    # Health endpoint
    resp = client.get("/health")
    resp.raise_for_status()
    data = resp.json()
    assert data.get("status") == "healthy"
    print("[smoke_api] /health OK:", data)

    # Explorer search (stubbed data is fine; we're testing wiring)
    try:
        resp2 = client.post(
            "/v1/explorer/search",
            json={"query_string": "", "filters": {}, "page": 1, "page_size": 1},
        )
        resp2.raise_for_status()
        print("[smoke_api] /v1/explorer/search OK; returned", len(resp2.json()), "items")
    except Exception as exc:
        print("[smoke_api] /v1/explorer/search check failed:", exc)

    # Attribute registry (may be empty before seed_attributes runs)
    try:
        resp3 = client.get("/v1/explorer/attributes")
        resp3.raise_for_status()
        print("[smoke_api] /v1/explorer/attributes OK; got", len(resp3.json()), "rows")
    except Exception as exc:
        print("[smoke_api] /v1/explorer/attributes check failed:", exc)


if __name__ == "__main__":
    main()----- CONTENT END -----
----- FILE PATH: scripts/smoke_frontend.py
----- CONTENT START -----
#!/usr/bin/env python3
"""Frontend smoketest.

Verifies that the Nginx-served frontend is not only serving HTML but
that the JavaScript bundle can execute in a headless browser when
Playwright is available.

Behaviour:

- If `playwright` is installed in the environment (`pip install playwright`
  and `playwright install chromium`), this script will launch a headless
  Chromium instance, load the portal page, and assert that some expected
  text is present in the rendered body.

- If `playwright` is NOT available, it falls back to a simple HTTP GET
  check and ensures that HTML is served. This keeps the smoketest usable
  in minimal environments while still allowing a deeper E2E check where
  supported.

The target URL can be overridden via FRONTEND_URL env var.
"""

import logging
import os
import time

import requests

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

FRONTEND_URL = os.getenv("FRONTEND_URL", "http://localhost:8080/")


def _check_html_only() -> None:
    logger.info("Running frontend HTML smoke check against %s", FRONTEND_URL)
    try:
        resp = requests.get(FRONTEND_URL, timeout=10)
    except Exception as exc:  # pragma: no cover - network errors
        logger.error("Failed to reach frontend at %s: %s", FRONTEND_URL, exc)
        raise SystemExit(1)
    if resp.status_code != 200:
        logger.error("Frontend returned non-200 status: %s", resp.status_code)
        raise SystemExit(1)
    text = resp.text.lower()
    if "<html" not in text or "<body" not in text:
        logger.error("Frontend response does not look like HTML")
        raise SystemExit(1)
    logger.info("Frontend HTML check passed (status=%s, length=%s)", resp.status_code, len(resp.text))


def _check_with_playwright() -> None:
    try:
        from playwright.sync_api import sync_playwright  # type: ignore
    except ImportError:
        logger.info("playwright not installed; falling back to HTML-only check")
        _check_html_only()
        return

    logger.info("Running frontend headless browser smoke check against %s", FRONTEND_URL)
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(FRONTEND_URL, wait_until="networkidle")
        time.sleep(1.0)
        body_text = (page.text_content("body") or "").lower()
        browser.close()

    expected_fragments = [
        "image tagger",
        "tagger workbench",
        "admin cockpit",
        "research explorer",
    ]
    if not any(fragment in body_text for fragment in expected_fragments):
        logger.error("Headless browser did not see expected portal text in body.")
        logger.debug("Body text was: %s", body_text[:500])
        raise SystemExit(1)

    logger.info("Frontend headless browser smoke check passed.")


def main() -> None:
    _check_with_playwright()


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/smoke_science.py
----- CONTENT START -----
"""DB-backed smoke test for the science pipeline and composite indices.

This script verifies that:

  * The database is reachable.
  * At least one Image record exists.
  * At least one science run completes without crashing.
  * Core composite attributes and their bins are present in Validation.

It is intentionally conservative and cheap to run; it is not a full benchmark.
"""

import sys
from pathlib import Path

from sqlalchemy.orm import Session

from backend.database.core import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.pipeline import SciencePipeline


REQUIRED_KEYS = [
    "science.visual_richness",
    "science.organized_complexity",
    "science.visual_richness_bin",
    "science.organized_complexity_bin",
]


def _choose_image(session: Session) -> Image:
    image = session.query(Image).first()
    if image is None:
        raise SystemExit("[smoke_science] No Image rows found; seed at least one test image.")
    return image


def _run_pipeline(session: Session, image: Image) -> None:
    pipeline = SciencePipeline(session=session)
    # Prefer process_image, with fallback to run_for_image for legacy APIs.
    if hasattr(pipeline, "process_image"):
        ok = pipeline.process_image(image.id)
    else:
        ok = pipeline.run_for_image(image.id)  # type: ignore[attr-defined]
    if not ok:
        raise SystemExit(f"[smoke_science] Science pipeline reported failure for image_id={image.id}")


def _assert_composites_present(session: Session, image: Image) -> None:
    rows = (
        session.query(Validation)
        .filter(Validation.image_id == image.id)
        .filter(Validation.attribute_key.in_(REQUIRED_KEYS))
        .all()
    )
    present = {row.attribute_key for row in rows}
    missing = [k for k in REQUIRED_KEYS if k not in present]
    if missing:
        raise SystemExit(
            "[smoke_science] Missing expected science attributes for image_id=%s: %s"
            % (image.id, ", ".join(missing))
        )
    print("[smoke_science] Composite indices OK for image_id=%s: %s" % (image.id, ", ".join(sorted(present))))


def main() -> int:
    session = SessionLocal()
    try:
        image = _choose_image(session)
        _run_pipeline(session, image)
        _assert_composites_present(session, image)
        print("[smoke_science] SUCCESS")
        return 0
    finally:
        try:
            session.close()
        except Exception:
            pass


if __name__ == "__main__":
    sys.exit(main())
----- CONTENT END -----
----- FILE PATH: scripts/syntax_guard.py
----- CONTENT START -----
"""Syntax Guard

Enterprise GO gate. Ensures that all live Python code in critical folders
parses via AST, and that no embedded truncation placeholders made it into
the tree (e.g., 'np.fl...' from truncated edits).

Critical folders:
- backend/
- scripts/
- tests/

Archive/ and other excluded directories are ignored.
"""

from __future__ import annotations

import ast
import re
import sys
from pathlib import Path
from typing import Iterable, List, Tuple


ROOT = Path(__file__).resolve().parents[1]
CRITICAL_DIRS = ("backend", "scripts", "tests")
EXCLUDE_DIR_PARTS = {"archive", "__pycache__", ".venv", "node_modules", ".git"}

# Detect embedded truncations that AST might not catch (e.g., np.fl...)
TRUNCATION_REGEXES = [
    re.compile(r"\bnp\.[A-Za-z_]+\.\.\.\b"),   # np.fl...
    re.compile(r"\b\w+\.\w+\.\.\.\b"),      # any dotted token ending with ...
    re.compile(r"astype\(\s*np\.[A-Za-z_]*\.\.\."),  # astype(np.fl...
]


def iter_py_files() -> Iterable[Path]:
    for d in CRITICAL_DIRS:
        base = ROOT / d
        if not base.exists():
            continue
        for p in base.rglob("*.py"):
            if any(part in EXCLUDE_DIR_PARTS for part in p.parts):
                continue
            yield p


def scan_truncations(text: str) -> List[Tuple[int, str]]:
    bad: List[Tuple[int, str]] = []
    for ln, line in enumerate(text.splitlines(), start=1):
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        # Only scan code-ish lines: ignore obvious docstring-only lines
        if stripped.startswith(('"""', "'''")):
            continue
        for rx in TRUNCATION_REGEXES:
            if rx.search(line):
                bad.append((ln, stripped))
                break
    return bad


def main() -> None:
    syntax_errors = []
    trunc_errors = []

    for p in iter_py_files():
        text = p.read_text(encoding="utf-8")
        try:
            ast.parse(text, filename=str(p))
        except SyntaxError as e:
            rel = p.relative_to(ROOT)
            syntax_errors.append((str(rel), e.lineno or 0, e.msg))

        bad = []
        if p.name != "syntax_guard.py":
            bad = scan_truncations(text)
        if bad:
            rel = p.relative_to(ROOT)
            for ln, snippet in bad:
                trunc_errors.append((str(rel), ln, snippet))

    if syntax_errors or trunc_errors:
        print("[syntax_guard] NO-GO", file=sys.stderr)
        if syntax_errors:
            print("  Syntax errors:", file=sys.stderr)
            for rel, ln, msg in syntax_errors:
                print(f"    {rel}:{ln}: {msg}", file=sys.stderr)
        if trunc_errors:
            print("  Truncation placeholders:", file=sys.stderr)
            for rel, ln, snippet in trunc_errors:
                print(f"    {rel}:{ln}: {snippet}", file=sys.stderr)
        raise SystemExit(1)

    print("[syntax_guard] OK: all critical python files parse cleanly and contain no truncations.")


if __name__ == "__main__":
    main()
----- CONTENT END -----
----- FILE PATH: scripts/vlm_turing_test_prep.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
Prepare a double-blind "VLM Turing Test" panel CSV.

Goal
----
Given two CSV files ‚Äì one with VLM-generated tags and one with
human-generated tags ‚Äì this script produces a *panel* CSV suitable
for double-blind rating by external judges.

For each (image_id, attribute_key) pair present in both inputs, we:

- Randomly assign which side is "A" and which is "B".
- Record label_A and label_B as the two competing values.
- Record which_is_vlm as "A" or "B" (for later scoring).
- Carry through any optional metadata columns requested.

The output panel can then be loaded into a simple rating UI
(Excel, Google Sheets, or a lightweight React table) where
judges rate which label is better, more plausible, or guess
which one came from the AI.

Expected input format
---------------------
The script is intentionally conservative and does not assume a
particular schema. By default it expects both CSV files to have
at least:

- image_id       (int / str)
- attribute_key  (str)
- value          (str or numeric)

You can override the column names via CLI flags.

Example
-------
    python scripts/vlm_turing_test_prep.py \
        --vlm reports/vlm_validations.csv \
        --human reports/human_validations.csv \
        --out reports/vlm_turing_panel.csv \
        --max-trials 400

"""

from __future__ import annotations

import argparse
import csv
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Tuple


Key = Tuple[str, str]  # (image_id, attribute_key)


@dataclass
class Record:
    image_id: str
    attribute_key: str
    value: str


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Prepare a VLM Turing Test panel CSV.")
    parser.add_argument("--vlm", required=True, help="CSV file with VLM outputs.")
    parser.add_argument("--human", required=True, help="CSV file with human annotations.")
    parser.add_argument("--out", required=True, help="Path to write the panel CSV.")
    parser.add_argument(
        "--image-column",
        default="image_id",
        help="Column name for the image id (default: image_id).",
    )
    parser.add_argument(
        "--attribute-column",
        default="attribute_key",
        help="Column name for the attribute key (default: attribute_key).",
    )
    parser.add_argument(
        "--value-column",
        default="value",
        help="Column name for the value / label (default: value).",
    )
    parser.add_argument(
        "--max-trials",
        type=int,
        default=400,
        help="Maximum number of trials to include (default: 400).",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for trial sampling / A/B assignment (default: 42).",
    )
    return parser.parse_args()


def _read_records(path: Path, image_col: str, attr_col: str, value_col: str) -> Dict[Key, str]:
    records: Dict[Key, str] = {}
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        missing = {image_col, attr_col, value_col} - set(reader.fieldnames or [])
        if missing:
            raise SystemExit(f"[vlm_turing_test_prep] Missing columns in {path}: {sorted(missing)}")

        for row in reader:
            image_id = str(row[image_col])
            attr = str(row[attr_col])
            value = str(row[value_col])
            key: Key = (image_id, attr)
            # If there are multiple rows per (image, attr), last one wins;
            # this keeps the script simple and deterministic.
            records[key] = value

    return records


def build_trials(
    vlm: Dict[Key, str],
    human: Dict[Key, str],
    max_trials: int,
    rng: random.Random,
) -> List[Dict[str, str]]:
    keys = sorted(set(vlm.keys()) & set(human.keys()))
    rng.shuffle(keys)
    keys = keys[:max_trials]

    trials: List[Dict[str, str]] = []
    for idx, key in enumerate(keys, start=1):
        image_id, attr = key
        vlm_value = vlm[key]
        human_value = human[key]

        # Skip degenerate cases where both labels are identical.
        if vlm_value == human_value:
            continue

        if rng.random() < 0.5:
            label_a = vlm_value
            label_b = human_value
            which_is_vlm = "A"
        else:
            label_a = human_value
            label_b = vlm_value
            which_is_vlm = "B"

        trials.append(
            {
                "trial_id": str(idx),
                "image_id": image_id,
                "attribute_key": attr,
                "label_A": label_a,
                "label_B": label_b,
                "which_is_vlm": which_is_vlm,
                # Optional columns to be filled in by raters:
                "rater_id": "",
                "rating_A": "",
                "rating_B": "",
                "guess_is_ai": "",
                "notes": "",
            }
        )

    return trials


def write_panel(path: Path, trials: List[Dict[str, str]]) -> None:
    if not trials:
        raise SystemExit("[vlm_turing_test_prep] No non-degenerate trials constructed.")

    fieldnames = list(trials[0].keys())
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for t in trials:
            writer.writerow(t)


def main() -> None:
    args = _parse_args()
    rng = random.Random(args.seed)

    vlm_records = _read_records(
        Path(args.vlm),
        image_col=args.image_column,
        attr_col=args.attribute_column,
        value_col=args.value_column,
    )
    human_records = _read_records(
        Path(args.human),
        image_col=args.image_column,
        attr_col=args.attribute_column,
        value_col=args.value_column,
    )

    trials = build_trials(vlm_records, human_records, max_trials=args.max_trials, rng=rng)
    write_panel(Path(args.out), trials)

    print(
        f"[vlm_turing_test_prep] Wrote {len(trials)} trials to {args.out}. "
        "You can now distribute this panel to human judges."
    )


if __name__ == "__main__":  # pragma: no cover - CLI wrapper
    main()
----- CONTENT END -----
----- FILE PATH: scripts/vlm_turing_test_score.py
----- CONTENT START -----
#!/usr/bin/env python3
"""
Score results from a VLM Turing Test panel.

This script consumes a CSV produced by ``vlm_turing_test_prep.py`` after
it has been annotated by human judges. Each row should correspond to
one rater's judgment for a given trial and include at least:

- trial_id        (str / int)
- which_is_vlm    ("A" or "B")
- guess_is_ai     ("A" or "B")
- rating_A        (numeric, optional)
- rating_B        (numeric, optional)
- rater_id        (str, optional)

The script computes:

- overall accuracy of guesses (<= 50% ~ indistinguishable from chance),
- per-rater accuracy,
- mean ratings for AI vs human labels where available.

The goal is not to enforce a particular statistical test but to provide
a quick sanity-check summary for the Science Lead.
"""

from __future__ import annotations

import argparse
import csv
import math
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple


@dataclass
class TrialStats:
    trial_id: str
    n_raters: int
    n_correct: int
    mean_rating_ai: float
    mean_rating_human: float


@dataclass
class GlobalStats:
    total_judgments: int
    total_correct: int
    overall_accuracy: float
    per_rater_accuracy: Dict[str, float]
    mean_rating_ai: float
    mean_rating_human: float


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Score VLM Turing Test results.")
    parser.add_argument("--panel", required=True, help="CSV exported after judges have filled in the panel.")
    return parser.parse_args()


def _safe_float(value: str) -> float:
    try:
        return float(value)
    except Exception:
        return float("nan")


def score_panel(path: Path) -> GlobalStats:
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        required = {"trial_id", "which_is_vlm", "guess_is_ai"}
        missing = required - set(reader.fieldnames or [])
        if missing:
            raise SystemExit(f"[vlm_turing_test_score] Missing required columns: {sorted(missing)}")

        per_rater_counts: Dict[str, int] = Counter()
        per_rater_correct: Dict[str, int] = Counter()

        ai_ratings: List[float] = []
        human_ratings: List[float] = []

        total_judgments = 0
        total_correct = 0

        for row in reader:
            total_judgments += 1
            which_is_vlm = row["which_is_vlm"].strip()
            guess = row["guess_is_ai"].strip()
            rater_id = row.get("rater_id", "").strip() or "UNKNOWN"

            if guess and guess in {"A", "B"} and which_is_vlm in {"A", "B"}:
                is_correct = int(guess == which_is_vlm)
                total_correct += is_correct
                per_rater_counts[rater_id] += 1
                per_rater_correct[rater_id] += is_correct

            # Ratings (optional)
            rating_a = _safe_float(row.get("rating_A", ""))
            rating_b = _safe_float(row.get("rating_B", ""))

            if which_is_vlm == "A":
                if not math.isnan(rating_a):
                    ai_ratings.append(rating_a)
                if not math.isnan(rating_b):
                    human_ratings.append(rating_b)
            elif which_is_vlm == "B":
                if not math.isnan(rating_b):
                    ai_ratings.append(rating_b)
                if not math.isnan(rating_a):
                    human_ratings.append(rating_a)

        overall_accuracy = (total_correct / total_judgments) if total_judgments else 0.0

        per_rater_accuracy = {
            rater: (per_rater_correct[rater] / count) if count else 0.0
            for rater, count in per_rater_counts.items()
        }

        def _mean(xs: List[float]) -> float:
            xs = [x for x in xs if not math.isnan(x)]
            return sum(xs) / len(xs) if xs else float("nan")

        mean_ai = _mean(ai_ratings)
        mean_human = _mean(human_ratings)

        return GlobalStats(
            total_judgments=total_judgments,
            total_correct=total_correct,
            overall_accuracy=overall_accuracy,
            per_rater_accuracy=per_rater_accuracy,
            mean_rating_ai=mean_ai,
            mean_rating_human=mean_human,
        )


def main() -> None:
    args = _parse_args()
    stats = score_panel(Path(args.panel))

    print("[vlm_turing_test_score] Global summary")
    print(f"  total_judgments : {stats.total_judgments}")
    print(f"  total_correct   : {stats.total_correct}")
    print(f"  overall_accuracy: {stats.overall_accuracy:.3f}")
    print(f"  mean_rating_ai  : {stats.mean_rating_ai:.3f}")
    print(f"  mean_rating_human: {stats.mean_rating_human:.3f}")
    print("  per_rater_accuracy:")
    for rater, acc in sorted(stats.per_rater_accuracy.items()):
        print(f"    - {rater}: {acc:.3f}")


if __name__ == "__main__":  # pragma: no cover - CLI wrapper
    main()
----- CONTENT END -----
----- FILE PATH: tests/test_admin_killswitch.py
----- CONTENT START -----
import json

from fastapi.testclient import TestClient

from backend.main import app
from backend.database import SessionLocal, engine
from backend import models
from backend.models.config import ToolConfig

client = TestClient(app)


def _ensure_db():
    # Create tables if they do not exist.
    models.Base.metadata.create_all(bind=engine)


def test_admin_kill_switch_disables_paid_models():
    """Ensure the kill-switch endpoint toggles the global paid-model state.

    This test seeds at least one paid ToolConfig, verifies that the
    initial budget reflects `is_kill_switched == False`, then calls the
    kill-switch with `active=true` and asserts that no paid models remain
    enabled and that the returned BudgetStatus reports the kill switch
    as active.
    """
    _ensure_db()

    # Seed a paid tool config if none exist yet.
    with SessionLocal() as db:
        existing_paid = (
            db.query(ToolConfig)
            .filter(ToolConfig.cost_per_1k_tokens > 0.0)
            .count()
        )
        if existing_paid == 0:
            cfg = ToolConfig(
                name="unit_test_paid_model",
                provider="test",
                cost_per_1k_tokens=1.0,
                cost_per_image=0.0,
                is_enabled=True,
                settings={},
            )
            db.add(cfg)
            db.commit()

    headers = {"X-User-Role": "admin"}

    # Baseline: budget should be reachable and expose a boolean flag.
    resp_budget = client.get("/api/v1/admin/budget", headers=headers)
    assert resp_budget.status_code == 200
    data_budget = resp_budget.json()
    assert "is_kill_switched" in data_budget
    assert isinstance(data_budget["is_kill_switched"], bool)
    # With at least one paid model enabled, the kill switch should be off.
    assert data_budget["is_kill_switched"] is False

    # Activate the kill switch.
    resp_kill = client.post(
        "/api/v1/admin/kill-switch",
        headers=headers,
        params={"active": True},
    )
    assert resp_kill.status_code == 200
    data_kill = resp_kill.json()
    assert "is_kill_switched" in data_kill
    assert data_kill["is_kill_switched"] is True

    # All paid models should now be disabled in the DB.
    with SessionLocal() as db:
        remaining_paid = (
            db.query(ToolConfig)
            .filter(
                ToolConfig.cost_per_1k_tokens > 0.0,
                ToolConfig.is_enabled.is_(True),
            )
            .count()
        )
        assert remaining_paid == 0
----- CONTENT END -----
----- FILE PATH: tests/test_admin_upload.py
----- CONTENT START -----
import io
from pathlib import Path

from fastapi.testclient import TestClient

from backend.main import app
from backend.database import SessionLocal, engine
from backend import models

client = TestClient(app)


def _ensure_db():
    # Create tables if they do not exist
    models.Base.metadata.create_all(bind=engine)


def test_admin_bulk_upload_creates_images(tmp_path):
    _ensure_db()
    # Minimal PNG header bytes; contents do not need to be a real image for this endpoint
    png_bytes = b"\x89PNG\r\n\x1a\n" + b"fakeimagebytes"
    files = [
        ("files", ("test.png", io.BytesIO(png_bytes), "image/png")),
    ]
    headers = {"X-User-Role": "admin"}
    resp = client.post("/api/v1/admin/upload", files=files, headers=headers)
    assert resp.status_code == 200
    data = resp.json()
    # At least one image should be created
    assert data.get("created_count", 0) >= 1
    image_ids = data.get("image_ids", [])
    assert isinstance(image_ids, list)
    # Confirm that at least one of the returned IDs exists in the database
    with SessionLocal() as db:
        found = db.query(models.Image).filter(models.Image.id.in_(image_ids)).count()
        assert found >= 1
----- CONTENT END -----
----- FILE PATH: tests/test_arch_patterns_vlm_vlm_path.py
----- CONTENT START -----
"""
Basic wiring test for the ArchPatternsVLMAnalyzer VLM path.

We stub out backend.services.vlm.get_vlm_engine to return a fake engine
so this test runs without network or API keys.
"""

from __future__ import annotations

from typing import Any, Dict, List

import numpy as np

from backend.science.core import AnalysisFrame
from backend.science.semantics.arch_patterns_vlm import ArchPatternsVLMAnalyzer
from backend.services import vlm as vlm_mod


class _FakeEngine(vlm_mod.VLMEngine):
    def analyze_image(self, image_bytes: bytes, prompt: str) -> Dict[str, Any]:
        # Minimal deterministic payload exercising two keys.
        return {
            "patterns": [
                {
                    "key": "arch.pattern.prospect_strong",
                    "present": 0.8,
                    "confidence": 0.9,
                    "evidence": "Clear outward view through large windows.",
                },
                {
                    "key": "arch.pattern.refuge_strong",
                    "present": 0.3,
                    "confidence": 0.7,
                    "evidence": "Limited alcove-like seating.",
                },
            ]
        }


def test_arch_patterns_vlm_wiring_uses_vlm_and_writes_attributes(monkeypatch=None) -> None:
    # Patch get_vlm_engine to return our fake engine.
    original_get = vlm_mod.get_vlm_engine

    def _fake_get(provider_override: str | None = None) -> vlm_mod.VLMEngine:  # type: ignore[override]
        return _FakeEngine()

    vlm_mod.get_vlm_engine = _fake_get  # type: ignore[assignment]

    try:
        # Construct a minimal frame with a valid RGB image.
        img = np.zeros((64, 64, 3), dtype=np.uint8)
        frame = AnalysisFrame(image_id=1, original_image=img)

        analyzer = ArchPatternsVLMAnalyzer()
        analyzer.analyze(frame)

        # We expect numeric attributes for at least the two active keys.
        assert "arch.pattern.prospect_strong" in frame.attributes
        assert "arch.pattern.refuge_strong" in frame.attributes

        # Metadata should contain the candidates list and engine name.
        meta = frame.metadata.get("arch.patterns.candidates", {})
        assert isinstance(meta.get("candidates"), list)
        assert meta.get("engine") == "_FakeEngine"
    finally:
        # Restore original factory.
        vlm_mod.get_vlm_engine = original_get  # type: ignore[assignment]
----- CONTENT END -----
----- FILE PATH: tests/test_bn_canon_sanity.py
----- CONTENT START -----
"""Canon sanity tests for BN export and index catalog.

These tests are higher-level than `test_bn_export_smoke` and focus on:

- Ensuring the index catalog entries for candidate BN inputs are well-formed.
- Ensuring the BN export uses exactly the candidate keys and expected bin fields.
"""

from backend.api.v1_bn_export import export_bn_snapshot
from backend.database.core import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.index_catalog import get_candidate_bn_keys, get_index_metadata


def test_index_catalog_candidate_entries_are_well_formed():
    """All candidate BN index entries should have labels, descriptions, types, and bins.

    This is a pure-catalog sanity check (no DB). It ensures that the BN-facing
    index definitions are complete and consistent enough to be used downstream.
    """
    metadata = get_index_metadata()
    candidate_keys = get_candidate_bn_keys()
    assert candidate_keys, "Index catalog returned no candidate BN keys"

    for key in candidate_keys:
        assert key in metadata, f"Candidate key {key!r} missing from index metadata"
        info = metadata[key]

        # Required fields
        assert info.get("label"), f"Index {key} missing label"
        assert info.get("description"), f"Index {key} missing description"
        assert info.get("type") in {"float", "int", "str"}, f"Index {key} has invalid type"

        # Bins are required for BN inputs in this design.
        binspec = info.get("bins")
        assert binspec is not None, f"Index {key} missing bins spec"
        assert "field" in binspec and binspec["field"], f"Index {key} bins spec missing field"
        assert "values" in binspec and isinstance(binspec["values"], list), f"Index {key} bins spec missing values"

        values = binspec["values"]
        assert values, f"Index {key} bins spec has empty values"
        # If we only have three levels, they should be ordered low/mid/high.
        if len(values) == 3:
            assert set(values) == {"low", "mid", "high"}, f"Index {key} bins values are unexpected: {values}"


def test_bn_export_respects_index_catalog_canon():
    """BN rows should expose indices and bins consistent with the index catalog.

    We seed synthetic Validation rows for *all* candidate indices and their
    bin fields, then call the export function and check that:

    - BNRow.indices contains exactly the candidate keys and all are non-None.
    - BNRow.bins contains the expected bin fields and labels.
    """
    metadata = get_index_metadata()
    candidate_keys = get_candidate_bn_keys()
    assert candidate_keys, "Index catalog returned no candidate BN keys"

    # Derive expected bin fields for candidate indices
    expected_bin_fields = set()
    for key in candidate_keys:
        binspec = metadata[key].get("bins")
        if binspec and "field" in binspec:
            expected_bin_fields.add(binspec["field"])

    session = SessionLocal()
    try:
        # Create a synthetic image
        img = Image(filename="bn_canon_test.jpg", storage_path="/tmp/bn_canon_test.jpg")
        session.add(img)
        session.commit()
        session.refresh(img)
        image_id = img.id

        # Seed continuous values for all candidate indices
        for idx_key in candidate_keys:
            session.add(
                Validation(
                    image_id=image_id,
                    attribute_key=idx_key,
                    value=0.5,
                    source="science_pipeline_test",
                )
            )

        # Seed bin codes (2 -> 'high') for all expected bin fields
        for bin_field in expected_bin_fields:
            session.add(
                Validation(
                    image_id=image_id,
                    attribute_key=bin_field,
                    value=2.0,
                    source="science_pipeline_test",
                )
            )

        session.commit()

        # Export BN snapshot
        rows = export_bn_snapshot(db=session)
        assert rows, "BN export returned no rows"

        # Find our image row
        row = next((r for r in rows if r.image_id == image_id), None)
        assert row is not None, "BN export did not include the synthetic image"

        # Indices canon: keys should match candidate index keys and be non-None
        assert set(row.indices.keys()) == set(candidate_keys)
        for key in candidate_keys:
            assert row.indices[key] is not None, f"Index {key} is unexpectedly None in BNRow.indices"

        # Bins canon: we at least expect all configured bin fields to appear
        for bin_field in expected_bin_fields:
            assert bin_field in row.bins, f"Bin field {bin_field!r} missing from BNRow.bins"
            # Where we used 2.0, we expect 'high' when low/mid/high is configured.
            # The label mapping is tested more directly in test_bn_export_smoke.
            if row.bins[bin_field] is not None:
                assert row.bins[bin_field] in {"low", "mid", "high"}
    finally:
        session.close()
----- CONTENT END -----
----- FILE PATH: tests/test_bn_export_smoke.py
----- CONTENT START -----
"""Smoketest for the BN export endpoint.

This test does *not* require the full science pipeline to run. Instead, it inserts a
synthetic image and a couple of Validation rows that mimic science output, then calls
the BN export function directly and checks that we see non-empty indices and bins.
"""

from backend.api.v1_bn_export import export_bn_snapshot
from backend.database.core import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.index_catalog import get_candidate_bn_keys, get_index_metadata


def test_bn_export_smoke():
    # This is an integration-style smoke test: it uses the real SessionLocal.
    session = SessionLocal()
    try:
        # 1. Create a synthetic image.
        img = Image(filename="bn_smoke_test.jpg", storage_path="/tmp/bn_smoke_test.jpg")
        session.add(img)
        session.commit()
        session.refresh(img)

        image_id = img.id

        # 2. Pick at least one candidate continuous index key.
        candidate_keys = get_candidate_bn_keys()
        assert candidate_keys, "Index catalog returned no candidate BN keys"
        idx_key = candidate_keys[0]

        # 3. Pick one bin field from the index metadata, if available.
        metadata = get_index_metadata()
        bin_key = None
        for _, info in metadata.items():
            binspec = info.get("bins")
            if binspec and "field" in binspec:
                bin_key = binspec["field"]
                break

        # 4. Insert synthetic Validation rows for the chosen keys.
        session.add(
            Validation(
                image_id=image_id,
                attribute_key=idx_key,
                value=0.7,
                source="science_pipeline_test",
            )
        )
        if bin_key is not None:
            session.add(
                Validation(
                    image_id=image_id,
                    attribute_key=bin_key,
                    value=2.0,
                    source="science_pipeline_test",
                )
            )
        session.commit()

        # 5. Call the BN export routine directly.
        rows = export_bn_snapshot(db=session)
        assert rows, "BN export returned no rows"

        # Find our image row.
        row = next((r for r in rows if r.image_id == image_id), None)
        assert row is not None, "BN export did not include the synthetic image"

        # 6. Continuous index should be non-None.
        assert idx_key in row.indices, "Candidate index key missing from BNRow.indices"
        assert row.indices[idx_key] is not None

        # 7. If we had a bin key, its label should be 'high' (2 -> 'high').
        if bin_key is not None:
            assert bin_key in row.bins, "Bin key missing from BNRow.bins"
            assert row.bins[bin_key] == "high"
    finally:
        session.close()
----- CONTENT END -----
----- FILE PATH: tests/test_explorer_smoke.py
----- CONTENT START -----
"""Smoketests for Research Explorer endpoints.

These tests check that the Explorer routers are mounted and return responses
with the expected shapes on a minimal database.
"""

from fastapi.testclient import TestClient

from backend.main import app

client = TestClient(app)


def _tagger_headers():
    return {
        "X-User-Id": "1",
        "X-User-Role": "tagger",
    }


def test_explorer_attributes():
    resp = client.get("/v1/explorer/attributes", headers=_tagger_headers())
    assert resp.status_code == 200, resp.text
    data = resp.json()
    assert isinstance(data, list)
    if data:
        first = data[0]
        assert "id" in first
        assert "key" in first
        assert "name" in first


def test_explorer_search_round_trip():
    payload = {
        "query_string": "",
        "filters": {},
        "page": 1,
        "page_size": 10,
    }
    resp = client.post("/v1/explorer/search", json=payload, headers=_tagger_headers())
    # We accept either success with a list or a 204/404 if no search backend is wired yet;
    # the main purpose is to ensure the router is mounted and RBAC permits taggers.
    assert resp.status_code in (200, 204, 404), resp.text
    if resp.status_code == 200:
        data = resp.json()
        assert isinstance(data, list)
----- CONTENT END -----
----- FILE PATH: tests/test_feature_registry_coverage.py
----- CONTENT START -----
"""
Tests ensuring coverage between the canonical feature registry
and the actual compute implementations.

A feature key is allowed to exist in the registry if and only if it is:
- computed somewhere in backend/science via frame.add_attribute, or
- explicitly listed in backend.science.feature_stubs.STUB_FEATURE_KEYS.
"""

from __future__ import annotations

import json
import re
from pathlib import Path

from backend.science import feature_stubs


ROOT = Path(__file__).resolve().parents[1]


def _load_registry_keys() -> set[str]:
    """Parse backend/science/features_canonical.jsonl into a set of keys.

    The file is currently stored as JSON lines with escaped newlines ("\n"),
    so we first unescape those before splitting.
    """
    reg_path = ROOT / "backend" / "science" / "features_canonical.jsonl"
    text = reg_path.read_text(encoding="utf-8")
    # Convert "\n" sequences into real newlines so each object is on its own line.
    text = text.replace("\\n", "\n")
    keys: set[str] = set()
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        obj = json.loads(s)
        key = obj.get("key")
        if key:
            keys.add(key)
    return keys


def _load_computed_keys() -> set[str]:
    """Scan backend/science for add_attribute() calls and extract keys."""
    base = ROOT / "backend" / "science"
    keys: set[str] = set()
    for path in base.rglob("*.py"):
        text = path.read_text(encoding="utf-8", errors="ignore")
        for m in re.finditer(r'add_attribute\("([^"]+)"', text):
            keys.add(m.group(1))
    return keys


def test_registry_keys_are_covered_by_compute_or_stub() -> None:
    registry_keys = _load_registry_keys()
    computed_keys = _load_computed_keys()
    stub_keys = feature_stubs.STUB_FEATURE_KEYS

    dangling = registry_keys - computed_keys - stub_keys
    assert not dangling, f"Dangling registry keys with no compute or stub: {sorted(dangling)[:20]}"
----- CONTENT END -----
----- FILE PATH: tests/test_governance_integrity.py
----- CONTENT START -----
"""Regression tests for governance guard scripts.

These tests assert that all guard scripts *execute* successfully
under the current repo tree.
"""

from __future__ import annotations

import runpy
from pathlib import Path


ROOT = Path(__file__).resolve().parents[1]


def _run_guard(script_name: str):
    ns = runpy.run_path(str(ROOT / "scripts" / script_name))
    if "main" in ns:
        ns["main"]()


def test_hollow_repo_guard_runs():
    _run_guard("hollow_repo_guard.py")


def test_program_integrity_guard_runs():
    _run_guard("program_integrity_guard.py")


def test_syntax_guard_runs():
    _run_guard("syntax_guard.py")


def test_critical_import_guard_runs():
    _run_guard("critical_import_guard.py")
def test_canon_guard_runs() -> None:
    root = Path(__file__).resolve().parents[1]
    runpy.run_path(str(root / "scripts" / "canon_guard.py"))
----- CONTENT END -----
----- FILE PATH: tests/test_guardian.py
----- CONTENT START -----
import json
from pathlib import Path

import pytest

from scripts import guardian


def test_guardian_freeze_and_verify(tmp_path: Path):
    """Guardian freeze+verify round-trip should succeed with a temp lock file."""
    conf = guardian.load_config()
    if not conf:
        pytest.skip("No governance config loaded; skipping Guardian test.")

    temp_lock = tmp_path / "governance.lock"

    # Create a baseline snapshot in a temporary lock file
    guardian.freeze(conf, lock_path=temp_lock)

    # Verify against the same baseline; should pass
    rc_ok = guardian.verify(conf, lock_path=temp_lock)
    assert rc_ok == 0


def test_guardian_detects_hash_mismatch(tmp_path: Path):
    """Guardian should fail if baseline hashes are corrupted in the temp lock file.

    This test ONLY perturbs the baseline JSON written to a temporary lock
    file; it never touches the actual repository files or governance.lock.
    """
    conf = guardian.load_config()
    if not conf:
        pytest.skip("No governance config loaded; skipping Guardian test.")

    temp_lock = tmp_path / "governance_corrupt.lock"

    # Build a clean snapshot
    baseline = guardian.snapshot(conf)
    protected_files = baseline.get("protected_files") or {}
    if not protected_files:
        pytest.skip("No protected files configured; nothing to test.")

    # Corrupt the hash of one protected file in the baseline
    key = next(iter(protected_files.keys()))
    protected_files[key]["hash"] = "0" * 64  # impossible SHA256
    temp_lock.write_text(json.dumps(baseline, indent=2), encoding="utf-8")

    rc = guardian.verify(conf, lock_path=temp_lock)
    assert rc == 1
----- CONTENT END -----
----- FILE PATH: tests/test_monitor_tag_inspector.py
----- CONTENT START -----
from fastapi.testclient import TestClient

from backend.main import app
from backend.database import SessionLocal, engine
from backend import models

client = TestClient(app)


def _ensure_db():
    models.Base.metadata.create_all(bind=engine)


def _seed_image_and_validation():
    db = SessionLocal()
    try:
        image = models.Image(source="unit_test", path="test/path.png")
        db.add(image)
        db.commit()
        db.refresh(image)

        validation = models.Validation(
            image_id=image.id,
            tagger_id="tagger-1",
            tool_config_id="default",
            raw_payload={"foo": "bar"},
        )
        db.add(validation)
        db.commit()
        db.refresh(validation)
        return image.id
    finally:
        db.close()


def test_monitor_tag_inspector_returns_validations():
    _ensure_db()
    image_id = _seed_image_and_validation()

    headers = {"X-User-Role": "admin"}
    resp = client.get(f"/api/v1/monitor/image/{image_id}/validations", headers=headers)
    assert resp.status_code == 200
    payload = resp.json()
    # We expect at least one validation in the response
    assert isinstance(payload, list)
    assert len(payload) >= 1


def test_monitor_image_inspector_shape_and_rbac():
    """Smoketest for the Tag Inspector endpoint shape and RBAC.

    We do not require real science features or BN metadata here. The goal is to
    ensure the endpoint is wired, RBAC is enforced, and the payload has the expected
    top-level structure so that the frontend can rely on it.
    """
    _ensure_db()
    image_id = _seed_image_and_validation()

    # Without admin header, access should be denied.
    resp_forbidden = client.get(f"/api/v1/monitor/image/{image_id}/inspector")
    assert resp_forbidden.status_code in (401, 403)

    headers = {"X-User-Role": "admin"}
    resp_ok = client.get(f"/api/v1/monitor/image/{image_id}/inspector", headers=headers)
    assert resp_ok.status_code == 200

    payload = resp_ok.json()
    assert isinstance(payload, dict)

    # Basic shape checks.
    for key in ("image", "pipeline", "features", "tags", "bn", "validations"):
        assert key in payload, f"Missing '{key}' in inspector payload"

    assert isinstance(payload["image"], dict)
    assert isinstance(payload["pipeline"], dict)
    assert isinstance(payload["features"], list)
    assert isinstance(payload["tags"], list)
    assert isinstance(payload["bn"], dict)
    assert isinstance(payload["validations"], list)

    # Pipeline scaffold shape.
    pipeline = payload["pipeline"]
    assert "overall_status" in pipeline
    assert "analyzers_run" in pipeline
    assert isinstance(pipeline["analyzers_run"], list)

    # BN scaffold shape.
    bn = payload["bn"]
    assert "nodes" in bn
    assert "irr" in bn
    assert isinstance(bn["nodes"], list)
----- CONTENT END -----
----- FILE PATH: tests/test_pipeline_health_debug.py
----- CONTENT START -----
from fastapi.testclient import TestClient

from backend.main import app


client = TestClient(app)


def test_pipeline_health_basic_shape():
    """Smoketest for /api/v1/debug/pipeline_health.

    We only assert the high-level response shape so the test is robust to
    future changes in the exact analyzer set or warning messages.
    """
    resp = client.get("/api/v1/debug/pipeline_health")
    assert resp.status_code == 200

    data = resp.json()
    assert isinstance(data, dict)

    # Required top-level fields
    assert "import_ok" in data
    assert isinstance(data["import_ok"], bool)

    assert "cv2_available" in data
    assert isinstance(data["cv2_available"], bool)

    assert "analyzers_by_tier" in data
    assert isinstance(data["analyzers_by_tier"], dict)

    # Optional lists
    for key in ("warnings", "analyzer_errors"):
        if key in data:
            assert isinstance(data[key], list)

    # Basic analyzer entry shape (if any analyzers are present)
    analyzers_by_tier = data["analyzers_by_tier"]
    assert isinstance(analyzers_by_tier, dict)
    for tier, analyzers in analyzers_by_tier.items():
        assert isinstance(analyzers, list)
        for a in analyzers:
            assert isinstance(a, dict)
            assert "name" in a
            assert "tier" in a
            assert "requires" in a
            assert "provides" in a
----- CONTENT END -----
----- FILE PATH: tests/test_science_pipeline_smoke.py
----- CONTENT START -----
"""Smoketest for the science pipeline.

This test runs the SciencePipeline (or equivalent entrypoint) on a synthetic image
record and asserts that at least one science attribute is written to Validation.

It is marked as 'slow' by convention; teams can skip it in tight CI loops if needed.
"""

from pathlib import Path

import numpy as np
import cv2

import pytest

from backend.database.core import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation
from backend.science.pipeline import SciencePipeline


@pytest.mark.slow
def test_science_pipeline_writes_validation(tmp_path: Path):
    session = SessionLocal()
    try:
        # 1. Create a tiny synthetic image on disk.
        img_path = tmp_path / "science_smoke.png"
        arr = np.zeros((64, 64, 3), dtype=np.uint8)
        cv2.rectangle(arr, (8, 8), (56, 56), (255, 255, 255), thickness=2)
        cv2.imwrite(str(img_path), arr)

        # 2. Insert an Image row pointing to this file.
        image = Image(filename="science_smoke.png", storage_path=str(img_path))
        session.add(image)
        session.commit()
        session.refresh(image)
        image_id = image.id

        # 3. Run the science pipeline for this image.
        pipeline = SciencePipeline()
        ok = pipeline.run_for_image(image_id)
        assert ok is True

        # 4. Confirm that at least one science attribute exists in Validation.
        rows = (
            session.query(Validation)
            .filter(Validation.image_id == image_id)
            .filter(Validation.source.like("science_pipeline%"))
            .all()
        )
        assert rows, "Expected at least one science Validation row after running the pipeline"
    finally:
        session.close()
----- CONTENT END -----
----- FILE PATH: tests/test_v3_api.py
----- CONTENT START -----
import pytest
from fastapi.testclient import TestClient

from backend.main import app


client = TestClient(app)


def _admin_headers():
    """Minimal admin identity for RBAC-protected endpoints."""
    return {
        "X-User-Id": "1",
        "X-User-Role": "admin",
    }


def _tagger_headers():
    """Minimal tagger identity for RBAC-protected endpoints."""
    return {
        "X-User-Id": "2",
        "X-User-Role": "tagger",
    }


def test_health_root():
    c = TestClient(app)
    resp = c.get("/")
    assert resp.status_code == 200


def test_health_endpoint():
    c = TestClient(app)
    resp = c.get("/health")
    assert resp.status_code in (200, 204)


def test_admin_models_rbac():
    c = TestClient(app)
    resp_forbidden = c.get("/v1/admin/models")
    assert resp_forbidden.status_code in (401, 403)

    resp_ok = c.get("/v1/admin/models", headers=_admin_headers())
    assert resp_ok.status_code in (200, 204)


def test_explorer_attributes():
    c = TestClient(app)
    resp = c.get("/v1/explorer/attributes", headers=_tagger_headers())
    assert resp.status_code in (200, 204)


def test_explorer_search_smoketest():
    c = TestClient(app)
    resp = c.post(
        "/v1/explorer/search",
        json={"filters": {}, "page": 1, "page_size": 5},
        headers=_tagger_headers(),
    )
    assert resp.status_code in (200, 204)


def test_explorer_export_empty_list():
    c = TestClient(app)
    resp = c.post(
        "/v1/explorer/export",
        json={"image_ids": []},
        headers=_tagger_headers(),
    )
    assert resp.status_code in (200, 204)


def test_monitor_velocity_rbac_and_shape():
    c = TestClient(app)

    # No headers ‚Üí forbidden
    resp_forbidden = c.get("/v1/monitor/velocity")
    assert resp_forbidden.status_code in (401, 403)

    # Admin headers ‚Üí OK, returns list (possibly empty)
    resp_ok = c.get("/v1/monitor/velocity", headers=_admin_headers())
    assert resp_ok.status_code == 200
    data = resp_ok.json()
    assert isinstance(data, list)


def test_monitor_irr_rbac_and_shape():
    c = TestClient(app)

    resp_forbidden = c.get("/v1/monitor/irr")
    assert resp_forbidden.status_code in (401, 403)

    resp_ok = c.get("/v1/monitor/irr", headers=_admin_headers())
    assert resp_ok.status_code == 200
    data = resp_ok.json()
    assert isinstance(data, list)----- CONTENT END -----
----- FILE PATH: tests/test_validation_attribute_fk_schema.py
----- CONTENT START -----
"""Schema-level sanity check for Validation.attribute_key foreign key.

This test ensures that the SQLAlchemy model for Validation declares a
foreign key from ``validations.attribute_key`` to ``attributes.key``.
It does not require a running database.
"""

from backend.models.annotation import Validation


def test_validation_attribute_key_has_fk_to_attributes() -> None:
    fk_targets = {fk.target_fullname for fk in Validation.__table__.foreign_keys}
    assert "attributes.key" in fk_targets, fk_targets
----- CONTENT END -----
----- FILE PATH: tests/test_vlm_safe_json_loads.py
----- CONTENT START -----
import json

import pytest

from backend.services import vlm


def test_safe_json_loads_plain_object():
    obj = {"a": 1, "b": "x"}
    raw = json.dumps(obj)
    parsed = vlm._safe_json_loads(raw)
    assert parsed == obj


def test_safe_json_loads_fenced_json_block():
    obj = {"foo": 123}
    raw = "```json\n" + json.dumps(obj) + "\n```"
    parsed = vlm._safe_json_loads(raw)
    assert parsed == obj


def test_safe_json_loads_generic_fence_block():
    obj = {"bar": 42}
    raw = "```" + json.dumps(obj) + "```"
    parsed = vlm._safe_json_loads(raw)
    assert parsed == obj


def test_safe_json_loads_with_leading_and_trailing_commentary():
    obj = {"baz": "ok"}
    inner = json.dumps(obj)
    raw = "model says:\n" + inner + "\nthanks"
    parsed = vlm._safe_json_loads(raw)
    assert parsed == obj


def test_safe_json_loads_raises_on_non_json():
    with pytest.raises(json.JSONDecodeError):
        vlm._safe_json_loads("this is not json at all")
----- CONTENT END -----
----- FILE PATH: tests/test_workbench_smoke.py
----- CONTENT START -----
"""Smoketest for Tagger Workbench endpoints.

This test exercises a minimal annotation flow:
- Insert a synthetic image.
- Fetch work as a tagger.
- Post a validation for that image.
- Confirm that the Validation row exists.
"""

from fastapi.testclient import TestClient

from backend.main import app
from backend.database.core import SessionLocal
from backend.models.assets import Image
from backend.models.annotation import Validation


client = TestClient(app)


def _tagger_headers():
    return {
        "X-User-Id": "1",
        "X-User-Role": "tagger",
    }


def test_workbench_annotation_flow():
    # Use a real DB session to seed a synthetic image.
    session = SessionLocal()
    try:
        img = Image(filename="workbench_smoke.jpg", storage_path="/tmp/workbench_smoke.jpg")
        session.add(img)
        session.commit()
        session.refresh(img)
        image_id = img.id
    finally:
        session.close()

    # 1. Fetch work (implementation-specific; we just assert that the endpoint is alive).
    # If the endpoint uses query params or a body, this call may need to be adapted,
    # but this smoketest ensures that the router is mounted and RBAC allows taggers.
    resp = client.get("/v1/annotation/queue", headers=_tagger_headers())
    assert resp.status_code in (200, 204, 404), resp.text

    # 2. Post a validation for the synthetic image.
    payload = {
        "image_id": image_id,
        "attribute_key": "science.visual_richness",
        "value": 0.5,
        "source": "test_workbench_smoke",
    }
    resp = client.post("/v1/annotation/validate", json=payload, headers=_tagger_headers())
    assert resp.status_code in (200, 201), resp.text

    # 3. Confirm that a Validation row exists.
    session = SessionLocal()
    try:
        exists = (
            session.query(Validation)
            .filter(Validation.image_id == image_id)
            .filter(Validation.attribute_key == "science.visual_richness")
            .filter(Validation.source == "test_workbench_smoke")
            .first()
        )
        assert exists is not None, "Expected a Validation row to be created by the annotation endpoint"
    finally:
        session.close()
----- CONTENT END -----
